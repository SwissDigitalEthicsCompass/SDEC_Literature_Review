,Unnamed: 0,Title,Publication Title,Volume,Issue,DOI,Authors,Publication Year,URL,Type,Abstract,Keywords,ISBN,ISSN,Date,Date Added,Date Modified,Pages,Series,Publisher,Key,Eligibility_Abstract_Score,Eligibility_Keywords_Score,Eligibility_Title_Score,Eligibility_Score
0,0,Implementing Artificial Intelligence Ethics In&nbsp;Trustworthy System Development - Making AI Ethics A&nbsp;Business Case,"Product-Focused Software Process Improvement: 23rd International Conference, PROFES 2022, Jyväskylä, Finland, November 21–23, 2022, Proceedings",,,10.1007/978-3-031-21388-5_52,"Agbese, Mamia",2022.0,https://doi.org/10.1007/978-3-031-21388-5_52,conferencePaper,"Software businesses struggle to implement AI ethics or ethical requirements in their development and engineering of AI. Current tools mainly focus on the technical level, with scarce resources identified for the different groups across software business organizations. This study focuses on developing a proposed solution, the ethical requirement stack, as a toolkit software businesses can leverage to implement ethical requirements. The tool aims to improve the understanding and visibility of AI ethics by serving as a go-to in interpreting AI ethics guidelines, thereby reducing the gap in transitioning AI ethics from principles to practice.",AI Ethics; AI ethics principles; Artificial Intelligence; Ethical requirement stack; Ethical requirements; Software businesses,978-3-031-21387-8,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,656–661,,Springer-Verlag,RCRK9GYA,0.2340425531914893,0.8571428571428571,0.3846153846153846,0.3424100294013188
1,1,An AI Ethics Course Highlighting Explicit Ethical Agents,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462552,"Green, Nancy",2021.0,https://doi.org/10.1145/3461702.3462552,conferencePaper,"This is an experience report describing a pilot AI Ethics course for undergraduate computer science majors. In addition to teaching students about different ethical approaches and using them to analyze ethical issues, the course covered how ethics has been incorporated into the implementation of explicit ethical agents, and required students to implement an explicit ethical agent for a simple application. This report describes the course objectives and design, the topics covered, and a qualitative evaluation with suggestions for future offerings of the courses.",AI ethics; ethics education; explicit ethical agents,978-1-4503-8473-5,,2021,2023-11-06 01:29:48,2023-11-06 01:29:48,519–524,AIES '21,Association for Computing Machinery,KXWK8WI4,0.2048192771084337,1.1428571428571428,0.75,0.3223127430356346
2,2,AI Ethics: A Call to Faculty,Commun. ACM,64.0,9,10.1145/3478516,"Nourbakhsh, Illah Reza",2021.0,https://doi.org/10.1145/3478516,journalArticle,Integrating ethics into artificial intelligence education and development.,,,0001-0782,2021-08,2023-11-06 01:29:48,2023-11-06 01:29:48,43–45,,,V53QT6FN,0.25,0.0,0.5,0.3186274509803921
4,4,Ethical Requirements Stack: A Framework for Implementing Ethical Requirements of AI in Software Engineering Practices,Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering,,,10.1145/3593434.3593489,"Agbese, Mamia; Mohanani, Rahul; Khan, Arif Ali; Abrahamsson, Pekka",2023.0,https://doi.org/10.1145/3593434.3593489,conferencePaper,"""The ethical requirements stack uses an Agile portfolio manage- ment framework based on agile scrum practices of themes, epics, features, and stories [3, 5] for a practical and hands-on approach to implementing ethical requirements of artificial intelligence (AI) into software engineering (SE) management practices. The frame- work is conceptualized as spread across actor groups of higher-level management (strategy level activities), middle-level management (product management activities), operational or development levels, and individual or team levels. The executive or strategy layer corresponds to the themes layer in the Agile portfolio management framework. This layer creates strategic, ethical requirements for the business. Ethical require- ments are determined at this stage using appropriate ethical frame- works or tools based on AI ethics guidelines and principles to create a central or strategic ethical requirements theme. """,AI ethics; AI ethics principles; Ethical requirements; AI; Agile portfolio management; Ethical requirements stack,9798400700446,,2023,2023-11-06 01:30:00,2023-11-06 01:30:00,326–328,EASE '23,Association for Computing Machinery,N49EFHB9,0.1679389312977099,0.9285714285714286,0.4666666666666667,0.2708347757488573
5,5,Implementing AI Ethics: Making Sense of the Ethical Requirements,Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering,,,10.1145/3593434.3593453,"Agbese, Mamia; Mohanani, Rahul; Khan, Arif; Abrahamsson, Pekka",2023.0,https://doi.org/10.1145/3593434.3593453,conferencePaper,"Society’s increasing dependence on Artificial Intelligence (AI) and AI-enabled systems require a more practical approach from software engineering (SE) executives in middle and higher-level management to improve their involvement in implementing AI ethics by making ethical requirements part of their management practices. However, research indicates that most work on implementing ethical requirements in SE management primarily focuses on technical development, with scarce findings for middle and higher-level management. We investigate this by interviewing ten Finnish SE executives in middle and higher-level management to examine how they consider and implement ethical requirements. We use ethical requirements from the European Union (EU) Trustworthy Ethics guidelines for Trustworthy AI as our reference for ethical requirements and an Agile portfolio management framework to analyze implementation. Our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified. The findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental well-being for implementation as sustainability requirements. We examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the Agile portfolio management framework.",Agile portfolio management; AI; AI ethics; AI ethics principles; Ethical requirements; Ethical requirements stack,9798400700446,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,62–71,EASE '23,Association for Computing Machinery,EVKFC8AL,0.2,0.9285714285714286,0.6666666666666666,0.2671024151287309
6,6,AI Ethics - Critical Reflections on Embedding Ethical Frameworks in AI Technology,"Culture and Computing. Design Thinking and Cultural Computing: 9th International Conference, C&amp;C 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II",,,10.1007/978-3-030-77431-8_20,"Salo-Pöntinen, Henrikki",2021.0,https://doi.org/10.1007/978-3-030-77431-8_20,conferencePaper,"Embedding ethical frameworks in artificial intelligence (AI) technologies has been a popular topic for academic research for the past decade [1–7]. The approaches of the studies differ in how AI technology, ethics, role of technical artefacts and socio-technical aspects of AI are perceived. In addition, most studies define insufficiently what the connection between the process of embedding ethical frameworks to AI technology and the larger framework of AI ethics is. These deficiencies have caused that the concept of AI ethics and the construct of embedding ethical parameters into AI are used in an ambiguous, rather than in a complementary manner.One reason for the ambiguity within this field of research is due to a lack of a comprehensive conceptual framework for AI ethics in general. I intend to fill this void by grounding AI ethics as a subfield of philosophy of technology and applied ethics and presenting its main issues of study by examining recognized spheres of activities through the method of levels of abstraction [8]. I put forward an initial hierarchical conceptual framework for AI ethics as an outcome. After this, I discuss the connection between the process of embedding ethical frameworks in AI and the larger AI ethics framework, leading to presenting basic requirements for the sphere of activity hereafter known as embedded ethics.",AI ethics; Applied ethics; Embedded ethics; Human-technology interaction,978-3-030-77430-1,,2021,2023-11-06 01:29:48,2023-11-06 01:29:48,311–329,,Springer-Verlag,T2JKNGZ8,0.1953488372093023,0.875,0.5833333333333334,0.2483228832916864
7,7,Recommender Systems and Their Ethical Challenges,AI Soc.,35.0,4,10.1007/s00146-020-00950-y,"Milano, Silvia; Taddeo, Mariarosaria; Floridi, Luciano",2020.0,https://doi.org/10.1007/s00146-020-00950-y,journalArticle,"This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders—as opposed to just the receivers of a recommendation—in assessing the ethical impacts of a recommender system.",Digital ethics; Artificial intelligence; Machine learning; Algorithms; Ethical trade-offs; Ethics of recommendation; Recommender systems,,0951-5666,2020-12,2023-11-06 01:30:02,2023-11-06 01:30:02,957–967,,,7PFBW3QX,0.1168831168831168,0.6428571428571429,0.5,0.2455270051836575
8,8,A Principlist Framework for Cybersecurity Ethics,Comput. Secur.,109.0,C,10.1016/j.cose.2021.102382,"Formosa, Paul; Wilson, Michael; Richards, Deborah",2021.0,https://doi.org/10.1016/j.cose.2021.102382,journalArticle,"The ethical issues raised by cybersecurity practices and technologies are of critical importance. However, there is disagreement about what is the best ethical framework for understanding those issues. In this paper we seek to address this shortcoming through the introduction of a principlist ethical framework for cybersecurity that builds on existing work in adjacent fields of applied ethics, bioethics, and AI ethics. By redeploying the AI4People framework, we develop a domain-relevant specification of five ethical principles in cybersecurity: beneficence, non-maleficence, autonomy, justice, and explicability. We then illustrate the advantages of this principlist framework by examining the ethical issues raised by four common cybersecurity contexts: penetration testing, distributed denial of service attacks (DDoS), ransomware, and system administration. These case analyses demonstrate the utility of this principlist framework as a basis for understanding cybersecurity ethics and for cultivating the ethical expertise and ethical sensitivity of cybersecurity professionals and other stakeholders.",AI ethics; Privacy; Cybersecurity ethics; DDoS attacks; Penetration testing; Principlism; Ransomware,,0167-4048,2021-10,2023-11-06 01:29:59,2023-11-06 01:29:59,,,,VXFLFHHR,0.2027027027027027,0.5454545454545454,0.3333333333333333,0.2349071940017134
10,10,"Operationalising AI Ethics: Barriers, Enablers and next Steps",AI Soc.,38.0,1,10.1007/s00146-021-01308-8,"Morley, Jessica; Kinsey, Libby; Elhalal, Anat; Garcia, Francesca; Ziosi, Marta; Floridi, Luciano",2021.0,https://doi.org/10.1007/s00146-021-01308-8,journalArticle,"By mid-2019 there were more than 80 AI ethics guides available in the public domain. Despite this, 2020 saw numerous news stories break related to ethically questionable uses of AI. In part, this is because AI ethics theory remains highly abstract, and of limited practical applicability to those actually responsible for designing algorithms and AI systems. Our previous research sought to start closing this gap between the ‘what’ and the ‘how’ of AI ethics through the creation of a searchable typology of tools and methods designed to translate between the five most common AI ethics principles and implementable design practices. Whilst a useful starting point, that research rested on the assumption that all AI practitioners are aware of the ethical implications of AI, understand their importance, and are actively seeking to respond to them. In reality, it is unclear whether this is the case. It is this limitation that we seek to overcome here by conducting a mixed-methods qualitative analysis to answer the following four questions: what do AI practitioners understand about the need to translate ethical principles into practice? What motivates AI practitioners to embed ethical principles into design practices? What barriers do AI practitioners face when attempting to translate ethical principles into practice? And finally, what assistance do AI practitioners want and need when translating ethical principles into practice?",AI ethics; Applied ethics; Business ethics; Ethical practices; Ethical principles,,0951-5666,2021-11,2023-11-06 01:29:48,2023-11-06 01:29:48,411–423,,,XQQGTU4X,0.1583710407239819,1.3,0.375,0.2250921974267562
11,11,User Perspectives on Ethical Challenges in Human-AI Co-Creativity: A Design Fiction Study,Proceedings of the 15th Conference on Creativity and Cognition,,,10.1145/3591196.3593364,"Rezwana, Jeba; Maher, Mary Lou",2023.0,https://doi.org/10.1145/3591196.3593364,conferencePaper,"In a human-AI co-creation, AI not only categorizes, evaluates and interprets data but also generates new content and interacts with humans. As co-creative AI is a form of intelligent technology that directly involves humans, it is critical to anticipate and address ethical issues during all design stages. The open-ended nature of human-AI interactions in co-creation poses many challenges for designing ethical co-creative AI systems. Researchers have been exploring ethical issues associated with autonomous AI in recent years, but ethics in human-AI co-creativity is a relatively new research area. In order to design human-centered ethical AI, it is important to understand the perspectives, expectations, and ethical concerns of potential users. In this paper, we present a study with 18 participants to explore several ethical dilemmas and challenges in human-AI co-creation from the perspective of potential users using a design fiction (DF) methodology. DF is a speculative research method that depicts a new concept or technology through stories as an intangible prototype. We present the findings from the study as potential users’ perspectives, stances, and expectations around ethical challenges in human-AI co-creativity as a basis for designing human-centered ethical AI partners for human-AI co-creation.",AI Ethics; Co-creativity; Design Fiction; Ethical AI; Ethical Issues; Human-AI Co-Creation,9798400701801,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,62–74,C&amp;C '23,Association for Computing Machinery,9N22T8PQ,0.171875,0.9090909090909092,0.25,0.2207759718600279
12,12,Artificial Intelligence Ethics Guidelines for K-12 Education: A Review of the Global Landscape,"Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Netherlands, June 14–18, 2021, Proceedings, Part II",,,10.1007/978-3-030-78270-2_4,"Adams, Cathy; Pente, Patti; Lemermeyer, Gillian; Rockwell, Geoffrey",2021.0,https://doi.org/10.1007/978-3-030-78270-2_4,conferencePaper,"To scope the global landscape of ethical issues involving the use of AI in K-12 education, we identified relevant ethics guidance documents, and then compared and contrasted concerns raised and principles applied. We found that while AIEdK-12 ethics guidelines employed many principles common to non-AIEd policy statements (e.g., transparency), new ethical principles were being engaged including pedagogical appropriateness and children’s rights.",AI literacy; AI and ethics; AI ethics guidelines; Artificial intelligence in education; Children’s rights; K-12 education; Teacher well-being,978-3-030-78269-6,,2021,2023-11-06 01:30:04,2023-11-06 01:30:04,24–28,,Springer-Verlag,EJS6LDIG,0.180327868852459,0.3888888888888889,0.1538461538461538,0.219796816967626
13,13,The Cost of Ethical AI Development for AI Startups,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534195,"Bessen, James; Impink, Stephen Michael; Seamans, Robert",2022.0,https://doi.org/10.1145/3514094.3534195,conferencePaper,"Artificial Intelligence startups use training data as direct inputs in product development. These firms must balance numerous tradeoffs between ethical issues and data access without substantive guidance from regulators or existing judicial precedence. We survey these startups to determine what actions they have taken to address these ethical issues and the consequences of those actions. We find that 58% of these startups have established a set of AI principles. Startups with data-sharing relationships with high-technology firms or that have prior experience with privacy regulations are more likely to establish ethical AI principles and are more likely to take costly steps, like dropping training data or turning down business, to adhere to their ethical AI policies. Moreover, startups with ethical AI policies are more likely to invest in unconscious bias training, hire ethnic minorities and female programmers, seek expert advice, and search for more diverse training data. Potential costs associated with data-sharing relationships and the adherence to ethical policies may create tradeoffs between increased AI product competition and more ethical AI production.",ethics; AI; data; scale barriers; startups,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,92–106,AIES '22,Association for Computing Machinery,H9LSTATR,0.1871345029239766,0.6666666666666666,0.5555555555555556,0.2175217397315195
14,14,AI and Neurotechnology: Learning from AI Ethics to Address an Expanded Ethics Landscape,Commun. ACM,66.0,3,10.1145/3529088,"Berger, Sara; Rossi, Francesca",2023.0,https://doi.org/10.1145/3529088,journalArticle,"The merging of machine, body, and psyche is on the horizon due to the technological advancements enabled by neuroscience and AI.",,,0001-0782,2023-02,2023-11-06 01:29:50,2023-11-06 01:29:50,58–68,,,A7JIC2PM,0.0476190476190476,0.0,0.4615384615384615,0.2151120197631825
15,15,From AI for People to AI for the World and the Universe,AI Soc.,38.0,2,10.1007/s00146-022-01402-5,"Baum, Seth D.; Owe, Andrea",2022.0,https://doi.org/10.1007/s00146-022-01402-5,journalArticle,"Recent work in AI ethics often calls for AI to advance human values and interests. The concept of “AI for people” is one notable example. Though commendable in some respects, this work falls short by excluding the moral significance of nonhumans. This paper calls for a shift in AI ethics to more inclusive paradigms such as “AI for the world” and “AI for the universe”. The paper outlines the case for more inclusive paradigms and presents implications for moral philosophy and computer science work on AI ethics.",AI ethics; AI for people; Environmental ethics; Nonhumans,,0951-5666,2022-02,2023-11-06 01:29:50,2023-11-06 01:29:50,679–680,,,LQS5HU5H,0.1494252873563218,0.75,0.1666666666666666,0.2056250229517829
16,16,The State of Ethical AI in Practice: A Multiple Case Study of Estonian Public Service Organizations,Int. J. Technoethics,14.0,1,10.4018/IJT.322017,"Hinton, Charlene",2023.0,https://doi.org/10.4018/IJT.322017,journalArticle,"Despite the prolific introduction of ethical frameworks, empirical research on AI ethics in the public sector is limited. This empirical research investigates how the ethics of AI is translated into practice and the challenges of its implementation by public service organizations. Using the Value Sensitive Design as a framework of inquiry, semi-structured interviews are conducted with eight public service organizations across the Estonian government that have piloted or developed an AI solution for delivering a public service. Results show that the practical application of AI ethical principles is indirectly considered and demonstrated in different ways in the design and development of the AI. However, translation of these principles varies according to the maturity of the AI and the public servant's level of awareness, knowledge, and competences in AI. Data-related challenges persist as public service organizations work on fine-tuning their AI applications.",Ethics; Artificial Intelligence; AI Ethical Principles; Estonia; Ethical AI Design and Development; Practical Application of AI Ethics; Public Sector; Value Sensitive Design,,1947-3451,2023-04,2023-11-06 01:29:59,2023-11-06 01:29:59,1–15,,,3Z9FZKRF,0.1276595744680851,0.5909090909090909,0.25,0.2018186327252825
17,17,Modeling and Guiding the Creation of Ethical Human-AI Teams,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462573,"Flathmann, Christopher; Schelble, Beau G.; Zhang, Rui; McNeese, Nathan J.",2021.0,https://doi.org/10.1145/3461702.3462573,conferencePaper,"With artificial intelligence continuing to advance, so too do the ethical concerns that can potentially negatively impact humans and the greater society. When these systems begin to interact with humans, these concerns become much more complex and much more important. The field of human-AI teaming provides a relevant example of how AI ethics can have significant and continued effects on humans. This paper reviews research in ethical artificial intelligence, as well as ethical teamwork through the lens of the rapidly advancing field of human-AI teaming, resulting in a model demonstrating the requirements and outcomes of building ethical human-AI teams. The model is created to guide the prioritization of ethics in human-AI teaming by outlining the ethical teaming process, outcomes of ethical teams, and external requirements necessary to ensure ethical human-AI teams. A final discussion is presented on how the developed model will influence the implementation of AI teammates, as well as the development of policy and regulation surrounding the domain in the coming years.",AI ethics; artificial intelligence; human-AI ethics; human-AI teamwork,978-1-4503-8473-5,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,469–479,AIES '21,Association for Computing Machinery,PMTFXUSL,0.1646341463414634,0.625,0.3333333333333333,0.1994351874211811
18,18,Ethics of AI: A Systematic Literature Review of Principles and Challenges,Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering,,,10.1145/3530019.3531329,"Khan, Arif Ali; Badshah, Sher; Liang, Peng; Waseem, Muhammad; Khan, Bilal; Ahmad, Aakash; Fahmideh, Mahdi; Niazi, Mahmood; Akbar, Muhammad Azeem",2022.0,https://doi.org/10.1145/3530019.3531329,conferencePaper,"Ethics in AI becomes a global topic of interest for both policymakers and academic researchers. In the last few years, various research organizations, lawyers, think tankers, and regulatory bodies get involved in developing AI ethics guidelines and principles. However, there is still debate about the implications of these principles. We conducted a systematic literature review (SLR) study to investigate the agreement on the significance of AI principles and identify the challenging factors that could negatively impact the adoption of AI ethics principles. The results reveal that the global convergence set consists of 22 ethical principles and 15 challenges. Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles. Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI. The findings of this study are the preliminary inputs for proposing a maturity model that assesses the ethical capabilities of AI systems and provides best practices for further improvements.",AI Ethics; Principles; Challenges; Machine Ethics; Systematic Literature Review,978-1-4503-9613-4,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,383–392,EASE '22,Association for Computing Machinery,JD9H6H5T,0.16875,0.5555555555555556,0.2727272727272727,0.1993418041683879
19,19,"Integrating AI Ethics in Wildlife Conservation AI Systems in South Africa: A Review, Challenges, and Future Research Agenda",AI Soc.,38.0,1,10.1007/s00146-021-01285-y,"Nandutu, Irene; Atemkeng, Marcellin; Okouma, Patrice",2021.0,https://doi.org/10.1007/s00146-021-01285-y,journalArticle,"With the increased use of Artificial Intelligence (AI) in wildlife conservation, issues around whether AI-based monitoring tools in wildlife conservation comply with standards regarding AI Ethics are on the rise. This review aims to summarise current debates and identify gaps as well as suggest future research by investigating (1) current AI Ethics and AI Ethics issues in wildlife conservation, (2) Initiatives Stakeholders in AI for wildlife conservation should consider integrating AI Ethics in wildlife conservation. We find that the existing literature weakly focuses on AI Ethics and AI Ethics in wildlife conservation while at the same time ignores AI Ethics integration in AI systems for wildlife conservation. This paper formulates an ethically aligned AI system framework and discusses pre-eminent on-demand AI systems in wildlife conservation. The proposed framework uses agile software life cycle methodology to implement guidelines towards the ethical upgrade of any existing AI system or the development of any new ethically aligned AI system. The guidelines enforce, among others, the minimisation of intentional harm and bias, diversity in data collection, design compliance, auditing of all activities in the framework and ease of code inspection. This framework will inform AI developers, users, conservationists, and policymakers on what to consider when integrating AI Ethics into AI-based systems for wildlife conservation.",AI Ethics; AI Ethics integration; AI in wildlife conservation; Artificial intelligence; Human–wildlife conflicts; Wildlife conservation concerns,,0951-5666,2021-09,2023-11-06 01:29:48,2023-11-06 01:29:48,245–257,,,QIMKGXJF,0.1714285714285714,0.4375,0.2222222222222222,0.1974425263391988
20,20,Utilizing User Stories To&nbsp;Bring AI Ethics Into&nbsp;Practice In&nbsp;Software Engineering,"Product-Focused Software Process Improvement: 23rd International Conference, PROFES 2022, Jyväskylä, Finland, November 21–23, 2022, Proceedings",,,10.1007/978-3-031-21388-5_41,"Kemell, Kai-Kristian; Vakkuri, Ville; Halme, Erika",2022.0,https://doi.org/10.1007/978-3-031-21388-5_41,conferencePaper,"AI ethics is a research area characterized by a prominent gap between research and practice. With most studies in the area being conceptual in nature or focused on technical ML (Machine Learning) solutions, the link between AI (Artificial Intelligence) ethics and SE (Software Engineering) practice remains thin. Establishing this link, we argue, is vital going forward. While conceptual discussion is required to define AI ethics, much progress has already been made in this regard. Similarly, though technical ML solutions are also required for practical implementation, ML systems are ultimately still software, and thus SE cannot be forgotten. In this paper, we propose one way of bringing AI ethics closer to conventional SE practice: utilizing user stories to implement AI ethics by means of Ethical User Stories (EUS). EUS can be used to formulate both functional and non-functional requirements, although an ethical framework is required produce them. By treating AI ethics as a part of the development process in this fashion, as opposed to a separate task, it can ideally become a part of SE for ML systems.",AI ethics; Artificial Intelligence; Ethical tool; Ethical user story; User story,978-3-031-21387-8,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,553–558,,Springer-Verlag,2VG4RNXX,0.135593220338983,0.8181818181818182,0.3333333333333333,0.192289315187082
21,21,The Ethics of Algorithms: Key Problems and Solutions,AI Soc.,37.0,1,10.1007/s00146-021-01154-8,"Tsamados, Andreas; Aggarwal, Nikita; Cowls, Josh; Morley, Jessica; Roberts, Huw; Taddeo, Mariarosaria; Floridi, Luciano",2022.0,https://doi.org/10.1007/s00146-021-01154-8,journalArticle,"Research on the ethics of algorithms has grown substantially over the past decade. Alongside the exponential development and application of machine learning algorithms, new ethical problems and solutions relating to their ubiquitous use in society have been proposed. This article builds on a review of the ethics of algorithms published in 2016 (Mittelstadt et al. Big Data Soc 3(2), 2016). The goals are to contribute to the debate on the identification and analysis of the ethical implications of algorithms, to provide an updated analysis of epistemic and normative concerns, and to offer actionable guidance for the governance of the design, development and deployment of algorithms.",Digital ethics; Privacy; Artificial intelligence; Explainability; Fairness; Trust; Machine learning; Responsibility; Algorithm; Autonomy; Transparency,,0951-5666,2022-03,2023-11-06 01:30:02,2023-11-06 01:30:02,215–230,,,P2TQM7ZN,0.1047619047619047,0.5714285714285714,0.25,0.1918994114499732
22,22,A Literature Review on Digital Ethics from a Humanistic and Sustainable Perspective,Proceedings of the 14th International Conference on Theory and Practice of Electronic Governance,,,10.1145/3494193.3494295,"Teran, Luis; Pincay, Jhonny; Wallimann-Helmer, Ivo; Portmann, Edy",2022.0,https://doi.org/10.1145/3494193.3494295,conferencePaper,"The rapid technological transition requires the adoptive approach to the digital conduct of public and private institutions. Countries and companies strive to integrate a balanced understanding of digital ethics and sustainability concepts from various standpoints, which results in a dispersed and uncategorized knowledge base. This work presents a literature review on digital ethics published from 2010 to 2020 in three technical libraries and one library maintained by the community of philosophers. The investigation process integrates a thorough review of digital ethics concepts in the leading academic libraries using keywords representing various concept applications. This study's outcome is a quantitative and sectorial categorization of works on digital ethics, followed by a holistic review of concepts, maturity level, and conclusions on each category. This work aims to understand the trends from a technological and philosophical perspective towards designing a sustainable digital ethical framework applied in digital services that fulfill sufficiency thresholds of justice and do not foster overshooting of planetary boundaries. The first version of a holistic framework based on the literature review is presented at the end of this work. It will be extended in future work.",digital ethics; ethical challenge; humanistic; literature review; sustainability,978-1-4503-9011-8,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,57–64,ICEGOV '21,Association for Computing Machinery,WG3RQG8Q,0.1397849462365591,0.875,0.3333333333333333,0.1912146992792153
23,23,How Does Value Similarity Affect Human Reliance in AI-Assisted Ethical Decision Making?,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604709,"Narayanan, Saumik; Yu, Guanghui; Ho, Chien-Ju; Yin, Ming",2023.0,https://doi.org/10.1145/3600211.3604709,conferencePaper,"This paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.",AI ethics; ethical preference; human reliance on AI,9798400702310,,2023,2023-11-06 01:29:49,2023-11-06 01:29:49,49–57,AIES '23,Association for Computing Machinery,BYSWACNN,0.1493506493506493,0.875,0.25,0.1882661343758282
24,24,Ethical Framework for Artificial Intelligence and Digital Technologies,Int. J. Inf. Manag.,62.0,C,10.1016/j.ijinfomgt.2021.102433,"Ashok, Mona; Madan, Rohit; Joha, Anton; Sivarajah, Uthayasankar",2022.0,https://doi.org/10.1016/j.ijinfomgt.2021.102433,journalArticle,"The use of Artificial Intelligence (AI) in Digital technologies (DT) is proliferating a profound socio-technical transformation. Governments and AI scholarship have endorsed key AI principles but lack direction at the implementation level. Through a systematic literature review of 59 papers, this paper contributes to the critical debate on the ethical use of AI in DTs beyond high-level AI principles. To our knowledge, this is the first paper that identifies 14 digital ethics implications for the use of AI in seven DT archetypes using a novel ontological framework (physical, cognitive, information, and governance). The paper presents key findings of the review and a conceptual model with twelve propositions highlighting the impact of digital ethics implications on societal impact, as moderated by DT archetypes and mediated by organisational impact. The implications of intelligibility, accountability, fairness, and autonomy (under the cognitive domain), and privacy (under the information domain) are the most widely discussed in our sample. Furthermore, ethical implications related to the governance domain are shown to be generally applicable for most DT archetypes. Implications under the physical domain are less prominent when it comes to AI diffusion with one exception (safety). The key findings and resulting conceptual model have academic and professional implications.",Artificial Intelligence (AI) ethics; Digital ethics; Digital technologies and archetypes; Ontological framework; PRISMA; Systematic literature review,,0268-4012,2022-02,2023-11-06 01:29:50,2023-11-06 01:29:50,,,,RGCVHYXK,0.1243781094527363,0.5625,0.625,0.1866909628874262
25,25,Getting into the Engine Room: A Blueprint to Investigate the Shadowy Steps of AI Ethics,AI Soc.,36.0,2,10.1007/s00146-020-01069-w,"Rochel, Johan; Evéquoz, Florian",2021.0,https://doi.org/10.1007/s00146-020-01069-w,journalArticle,"Enacting an AI system typically requires three iterative phases where AI engineers are in command: selection and preparation of the data, selection and configuration of algorithmic tools, and fine-tuning of the different parameters on the basis of intermediate results. Our main hypothesis is that these phases involve practices with ethical questions. This paper maps these ethical questions and proposes a way to address them in light of a neo-republican understanding of freedom, defined as absence of domination. We thereby identify different types of responsibility held by AI engineers and link them to concrete suggestions on how to improve professional practices. This paper contributes to the literature on AI and ethics by focusing on the work necessary to configure AI systems, thereby offering an input to better practices and an input for societal debates.",AI ethics; Applied ethics; Data ethics; Data science; Responsible innovation,,0951-5666,2021-06,2023-11-06 01:29:48,2023-11-06 01:29:48,609–622,,,GFZ5PDIF,0.1203007518796992,0.9,0.2,0.1844392959142041
26,26,Trustworthy AI for the People?,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462470,"Figueras, Clàudia; Verhagen, Harko; Cerratto Pargman, Teresa",2021.0,https://doi.org/10.1145/3461702.3462470,conferencePaper,"While AI systems become more pervasive, their social impact is increasingly hard to measure. To help mitigate possible risks and guide practitioners into a more responsible design, diverse organizations have released AI ethics frameworks. However, it remains unclear how ethical issues are dealt with in the everyday practices of AI developers. To this end, we have carried an exploratory empirical study interviewing AI developers working for Swedish public organizations to understand how ethics are enacted in practice. Our analysis found that several AI ethics issues are not consistently tackled, and AI systems are not fully recognized as part of a broader sociotechnical system.",AI ethics in practice; AI in public organizations; citizen empowerment; responsible AI; responsible AI principles in practice; trustworthy AI,978-1-4503-8473-5,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,269–270,AIES '21,Association for Computing Machinery,4VB2H2EN,0.145631067961165,0.3684210526315789,0.2,0.1842344041757601
27,27,Design Science Research and Designing Ethical Guidelines for the SHAPES AI Developers,Procedia Comput. Sci.,192.0,C,10.1016/j.procs.2021.08.223,"Nevanperä, Minna; Rajamäki, Jyri; Helin, Jaakko",2021.0,https://doi.org/10.1016/j.procs.2021.08.223,journalArticle,"This article targets the design process of ethical guidelines for the SHAPES project (Smart and Healthy Aging through People Engaging in Supportive Systems) which is a H2020 Innovation Action project. The aim of the project is to build solutions that can make it easier for older individuals to live at home, such as, robots, wearables and sensor technologies that apply artificial intelligence (AI). The guiding method of the design process of ethical guidelines is Alan Hevner’s Design Science Research. Theoretical background consists of a form of literature overview, which contains the most relevant ethical theories and research on AI ethics, machine ethics and human rights. This article introduces the process of building the ethical guidelines for the SHAPES project and further discussion if providing guidelines is the sufficient tool to developers to take ethical action in development of the AI systems. The SHAPES guidelines include the following themes; accountability, transparency and explainability, diversity, inclusion and fairness, safety and security and societal wellbeing and humanity.",Design Science Reseach; Ethical Competence; Ethical Guidelines; SHAPES,,1877-0509,2021-01,2023-11-06 01:30:03,2023-11-06 01:30:03,2330–2339,,,9ZGXWGLX,0.1341463414634146,0.75,0.3333333333333333,0.1816468560255222
29,29,"Education, Ethical Dilemmas and AI: From Ethical Design to Artificial Morality","Adaptive Instructional Systems. Design and Evaluation: Third International Conference, AIS 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I",,,10.1007/978-3-030-77857-6_11,"Casas-Roma, Joan; Conesa, Jordi; Caballé, Santi",2021.0,https://doi.org/10.1007/978-3-030-77857-6_11,conferencePaper,"Ethical dilemmas are complex scenarios involving a decision between conflicting choices related to ethical principles. While considering a case of an ethical dilemma in education presented in [17], it can be seen how, in these situations, it might be needed to take into consideration the student’s needs, preferences, and potentially conflicting goals, as well as their personal and social contexts. Due to this, planning and foreseeing ethically challenging situations in advance, which would be how ethical design is normally used in technological artifacts, is not enough. As AI systems become more autonomous, the amount of possible situations, choices and effects their actions can have grow exponentially. In this paper, we bring together the analysis of ethical dilemmas in education and the need to incorporate moral reasoning into the AI systems’ decision procedures. We argue how ethical design, although necessary, is not sufficient for that task and that artificial morality, or equivalent tools, are needed in order to integrate some sort of “ethical sensor” into autonomous systems taking a deeper role in an educational settings in order to enable them to, if not resolve, at least identify new ethically-relevant scenarios they are faced with.",AI Ethics; Artificial morality; Ethical sensors; Online learning,978-3-030-77856-9,,2021,2023-11-06 01:30:00,2023-11-06 01:30:00,167–182,,Springer-Verlag,9R7MDFDB,0.1191709844559585,0.75,0.6363636363636364,0.1767416576682392
30,30,Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3534626,"Boyd, Karen",2022.0,https://doi.org/10.1145/3531146.3534626,conferencePaper,"If “studying up,” or researching powerful actors in a social system, can offer insight into the workings and effects of power in social systems, this paper argues that “designing up” will give researchers and designers a tool to intervene. This paper offers a conception of “designing up,” applies the structure of Value Sensitive Design (VSD) to accomplish it, and submits an example of a tool designed to support ethical sensitivity, especially particularization and judgment. The designed artifact is a field guide for ethical mitigation strategies that uses tool profiles and filters to aid machine learning (ML) engineers as they build understanding of an ethical issue they have recognized and as they match the particulars of their problem to a technical ethical mitigation. This guide may broaden its users’ awareness of potential ethical issues, important features of ethical issues and their mitigations, and the breadth of available mitigations. Additionally, it may encourage ethical sensitivity in future ML projects. Feedback from ML engineers and technology ethics researchers rendered several usability improvements and ideas for future development. The tool can be found at: https://ml-ethics-tool.web.app/.",AI ethics; ethics; machine learning; datasets; development practices; ethical sensitivity,978-1-4503-9352-2,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,2069–2082,FAccT '22,Association for Computing Machinery,TH8CRF2C,0.1270718232044199,0.8,0.2307692307692307,0.1764500794887535
31,31,Ethical Awareness of UXers in the Loop: Ethical Issues in the Uxer-AI Collaboration Process from a UX Perspective,Proceedings of the 25th International Conference on Mobile Human-Computer Interaction,,,10.1145/3565066.3608691,"Yoon, Harin; Jun, Soojin",2023.0,https://doi.org/10.1145/3565066.3608691,conferencePaper,"Artificial Intelligence (AI) has emerged as a prominent collaborative tool across diverse domains, driving innovation in various tasks. However, this human–AI process brings forth a range of ethical considerations that require careful examination. This study investigates the ethical concerns that arise during the user experience designer (UXer)–AI co-creation process and the evolving role of UXers. Employing a mixed methods approach, combining observational task performance experiments and in-depth interviews, the study captures UXers' perceptions of ethical issues in the UXer-AI co-creation process. The findings shed light on three prominent ethical challenges in the UXer–AI co-creation process: reliability, bias, and unemployment. Consequently, this study emphasizes the crucial role of UXers, such as fact-checking, empathy-based decision making, and effective communication with AI, mitigating these ethical challenges. These findings enhance our understanding of UXers' responsibilities and shed light on the potential of leveraging AI as an effective collaborative tool for task completion.",AI Ethics; Generative AI; Co-Creation; Ethical UX; Uxer-AI Co-Creation Model,978-1-4503-9924-1,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,,MobileHCI '23 Companion,Association for Computing Machinery,72KLRSH2,0.1216216216216216,0.7,0.3333333333333333,0.1740387491738843
32,32,Mapping Research Strands of Ethics of Artificial Intelligence in Healthcare: A Bibliometric and Content Analysis,Comput. Biol. Med.,135.0,C,10.1016/j.compbiomed.2021.104660,"Saheb, Tahereh; Saheb, Tayebeh; Carpenter, David O.",2021.0,https://doi.org/10.1016/j.compbiomed.2021.104660,journalArticle,"The growth of artificial intelligence in promoting healthcare is rapidly progressing. Notwithstanding its promising nature, however, AI in healthcare embodies certain ethical challenges as well. This research aims to delineate the most influential elements of scientific research on AI ethics in healthcare by conducting bibliometric, social network analysis, and cluster-based content analysis of scientific articles. Not only did the bibliometric analysis identify the most influential authors, countries, institutions, sources, and documents, but it also recognized four ethical concerns associated with 12 medical issues. These ethical categories are composed of normative, meta-ethics, epistemological and medical practice. The content analysis complemented this list of ethical categories and distinguished seven more ethical categories: ethics of relationships, medico-legal concerns, ethics of robots, ethics of ambient intelligence, patients' rights, physicians’ rights, and ethics of predictive analytics. This analysis likewise identified 40 general research gaps in the literature and plausible future research strands. This analysis furthers conversations on the ethics of AI and associated emerging technologies such as nanotech and biotech in healthcare, hence, advances convergence research on the ethics of AI in healthcare. Practically, this research will provide a map for policymakers and AI engineers and scientists on what dimensions of AI-based medical interventions require stricter policies and guidelines and robust ethical design and development.",Ethics; Artificial intelligence; Content analysis; Healthcare; Bibliometric analysis; Network visualization; Robotics,,0010-4825,2021-08,2023-11-06 01:30:04,2023-11-06 01:30:04,,,,2BYKVZ92,0.1761904761904762,0.1818181818181818,0.1333333333333333,0.1738921640697445
33,33,Mental Contents in Designing AI Ethics,"Culture and Computing: 10th International Conference, C&amp;C 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26 – July 1, 2022, Proceedings",,,10.1007/978-3-031-05434-1_32,"Saariluoma, Pertti; Myllylä, Mari; Karvonen, Antero",2022.0,https://doi.org/10.1007/978-3-031-05434-1_32,conferencePaper,"In future intelligent digital society, the way people organize their life around technologies shall change because intelligent machines can follow ethical rules in their behavior. In the human mind, ethics exist as contents of mental representations. Therefore, it is important to investigate the information contents of mental representations or mental contents. One can also call this approach content-based cognitive research. The analysis of mental contents makes it possible to mimic human ethical information processing and construct human digital twins for designing ethical machine information processes. In this paper, we analyze the relevant AI aspects of Hume’s guillotine. Hume asked critically whether facts can be used to derive values. His answer was negative. However, modern information technology can collect huge masses of facts, but can these facts be used in improving how we should live? Content-based analysis of human ethical information processing opens possibilities to bypass the logical dilemma of Hume’s guillotine.",Ethical discourse; Hume’s guillotine; Mental contents; Weak and strong ethical AI,978-3-031-05433-4,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,477–487,,Springer-Verlag,ZWPT3FNA,0.1258278145695364,0.6363636363636364,0.5,0.173611085946733
34,34,From the Ground Truth up: Doing AI Ethics from Practice to Principles,AI Soc.,38.0,4,10.1007/s00146-021-01336-4,"Brusseau, James",2022.0,https://doi.org/10.1007/s00146-021-01336-4,journalArticle,"Recent AI ethics has focused on applying abstract principles downward to practice. This paper moves in the other direction. Ethical insights are generated from the lived experiences of AI-designers working on tangible human problems, and then cycled upward to influence theoretical debates surrounding these questions: (1) Should AI as trustworthy be sought through explainability, or accurate performance? (2) Should AI be considered trustworthy at all, or is reliability a preferable aim? (3) Should AI ethics be oriented toward establishing protections for users, or toward catalyzing innovation? Specific answers are less significant than the larger demonstration that AI ethics is currently unbalanced toward theoretical principles, and will benefit from increased exposure to grounded practices and dilemmas.",AI case studies; AI ethics; Healthcare AI; Philosophy and AI; Trustworthy AI,,0951-5666,2022-01,2023-11-06 01:29:48,2023-11-06 01:29:48,1651–1657,,,8GLT86ME,0.1217391304347826,0.5833333333333334,0.3333333333333333,0.1735443018845113
35,35,A Study on the Modeling of Major Factors for the Principles of AI Ethics,DG.O2021: The 22nd Annual International Conference on Digital Government Research,,,10.1145/3463677.3463733,"Lim, Ji Hun; Kwon, Hun Yeong",2021.0,https://doi.org/10.1145/3463677.3463733,conferencePaper,"The fourth industrial revolution, centered on artificial intelligence (AI), signals a significant transformation in human society. For this social transformation to ultimately be for humankind's prosperity and happiness, serious consideration of AI ethics is needed. As a result, many countries have begun to establish AI ethics principles, and the international community is pushing for standardization on AI ethics principles. This study aims to derive general factors of ethical principles that should be considered to establish and standardize AI ethics principles. To this end, we present a ""general AI ethics principles model,"" including 12 major factors, based on the recently published AI ethics principles in 15 countries. Furthermore, the major factors for AI ethics principles that we derived through this study ultimately confirmed that AI should be useful to all humans and is oriented toward the value of building a ""Trustworthy"" AI society. Based on these fundamental ideologies, we confirmed that each factor interconnects with each other. It is hoped that the AI ethics principles model that reflects this will be referred to national and international communities that have yet to develop Principles of AI Ethics. However, Factors for AI ethics principles should be constructed following the principles' intent and goal orientation, ensuring its feasibility, rather than merely duplicating the principles model.",,978-1-4503-8492-6,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,208–218,DG.O'21,Association for Computing Machinery,CP5RISCE,0.1706161137440758,0.0,0.2142857142857142,0.1727108580738546
36,36,The European Commission Report on Ethics of Connected and Automated Vehicles and the Future of Ethics of Transportation,Ethics and Inf. Technol.,23.0,4,10.1007/s10676-021-09609-8,"Santoni de Sio, Filippo",2021.0,https://doi.org/10.1007/s10676-021-09609-8,journalArticle,"The paper has two goals. The first is presenting the main results of the recent report Ethics of Connected and Automated Vehicles: recommendations on road safety, privacy, fairness, explainability and responsibility written by the Horizon 2020 European Commission Expert Group to advise on specific ethical issues raised by driverless mobility, of which the author of this paper has been member and rapporteur. The second is presenting some broader ethical and philosophical implications of these recommendations, and using these to contribute to the establishment of Ethics of Transportation as an independent&nbsp;branch of applied ethics. The recent debate on the ethics of Connected and Automated Vehicles (CAVs) presents a paradox and an opportunity. The paradox is the presence of a flourishing debate on the ethics of one very specific transportation technology without ethics of transportation being in itself a well-established academic discipline. The opportunity is that now that a spotlight has been switched on the ethical dimensions of CAVs it may be easier to establish a broader debate on ethics of transportation. While the 20 recommendations of the EU report are grouped in three macro-areas: road safety, data ethics, and responsibility, in this paper they will be grouped according to eight philosophical themes: Responsible Innovation, road justice, road safety, freedom, human control, privacy, data fairness, responsibility. These are proposed as the first topics for a new ethics of transportation.",Ethics of self-driving cars; Ethics of transportation; European Commission Report on ethics of CAVs; Responsible innovation in self-driving cars,,1388-1957,2021-12,2023-11-06 01:30:04,2023-11-06 01:30:04,713–726,,,BSYCZR64,0.1541850220264317,0.3157894736842105,0.2222222222222222,0.1717578331536133
37,37,Rebuilding 'ethics' to Govern AI: How to Re-Set the Boundaries for the Legal Sector?,Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law,,,10.1145/3594536.3595156,"Unver, Mehmet B.",2023.0,https://doi.org/10.1145/3594536.3595156,conferencePaper,"Artificial intelligence (AI) has been transforming the legal sector and profession given every day enhancing AI-driven legal tech tools. Considering the far-reaching ethical implications of such tools and the disparate functionalities of 'AI ethics' and 'legal ethics', this paper puts into question the interplay between these ethical domains and their underlying rules. After fleshing out the governance of ethics under each domain, e.g. respectively professional conduct rules and self-regulatory principles, and signposting the unresolved ethical challenges of status quo, e.g. particularly concerning cross-domain issues, the paper discusses how they need to interact, based on the three policy options: 'revision of the conduct rules', 'individual (company level) collaboration' and 'higher-level collaboration'. It is concluded that 'higher-level collaboration' between the stakeholders is found to be the most sustainable and long-term option given the need to mitigate the ethical challenges concerning the legal sector from a holistic point of view.",AI ethics; fairness; accountability; transparency; Legal ethics,9798400701979,,2023,2023-11-06 01:29:59,2023-11-06 01:29:59,306–315,ICAIL '23,Association for Computing Machinery,R8UG3WLT,0.1360544217687075,0.7142857142857143,0.2142857142857142,0.1717094798118252
38,38,Making Art with and about Artificial Intelligence: Three Approaches to Teaching AI and AI Ethics to Middle and High School Students,Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2,,,10.1145/3478432.3499157,"Walsh, Benjamin; Ali, Safinah; Castro, Francisco; Desportes, Kayla; DiPaola, Daniella; Lee, Irene; Payne, William; Sieke, Scott; Zhang, Helen",2022.0,https://doi.org/10.1145/3478432.3499157,conferencePaper,"In this hands-on workshop participants will experience the curricula from three NSF funded projects, which engage youth in creating art with and about AI technologies while exploring related ethical concerns.danceON is a web-based creative coding environment that engages learners in creating multimedia dance performances. Besides writing reactive code that generates animations to augment dance performances, students also use the system to explore the boundaries and biases of AI.DAILy is a curriculum focused on developing AI literacy among middle school students through the integration of technical concepts and processes, ethical and societal implications, and career futures in AI. Participants will be focusing on the AI + art modules of DAILy.Imagine AI develops project-based curricula to support youth in exploring critical ethical issues related to AI. Students read short stories featuring youth at the center of AI ethical dilemmas, build and manipulate AI systems, and create digital media to express ethical stances.Participants will leave the workshop with multiple AI teaching strategies that blend technical learning with social purpose and creative expression.",ai ethics education; art; artificial intelligence education,978-1-4503-9071-2,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,1203,SIGCSE 2022,Association for Computing Machinery,EZQ5A6TG,0.1538461538461538,0.4285714285714285,0.1904761904761904,0.1691687840484339
39,39,AI Ethics and the Banality of Evil,Ethics and Inf. Technol.,23.0,3,10.1007/s10676-021-09587-x,"Tajalli, Payman",2021.0,https://doi.org/10.1007/s10676-021-09587-x,journalArticle,"In this paper, I draw on Hannah Arendt’s notion of ‘banality of evil’ to argue that as long as AI systems are designed to follow codes of ethics or particular normative ethical theories chosen by us and programmed in them, they are Eichmanns destined to commit evil. Since intelligence alone is not sufficient for ethical decision making, rather than strive to program AI to determine the right ethical decision based on some ethical theory or criteria, AI should be concerned with avoiding making the wrong decisions, and this requires hardwiring the thinking activity as a prerequisite for decision making.",Artificial Intelligence; Artificial Thinking; Banality of Evil,,1388-1957,2021-09,2023-11-06 01:29:48,2023-11-06 01:29:48,447–454,,,5CDJRHE9,0.1717171717171717,0.0,0.4285714285714285,0.1689992457037911
40,40,Seeing Like a Toolkit: How Toolkits Envision the Work of AI Ethics,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579621,"Wong, Richmond Y.; Madaio, Michael A.; Merrill, Nick",2023.0,https://doi.org/10.1145/3579621,journalArticle,"Numerous toolkits have been developed to support ethical AI development. However, toolkits, like all tools, encode assumptions in their design about what work should be done and how. In this paper, we conduct a qualitative analysis of 27 AI ethics toolkits to critically examine how the work of ethics is imagined and how it is supported by these toolkits. Specifically, we examine the discourses toolkits rely on when talking about ethical issues, who they imagine should do the work of ethics, and how they envision the work practices involved in addressing ethics. Among the toolkits, we identify a mismatch between the imagined work of ethics and the support the toolkits provide for doing that work. In particular, we identify a lack of guidance around how to navigate labor, organizational, and institutional power dynamics as they relate to performing ethical work. We use these omissions to chart future work for researchers and designers of AI ethics toolkits.",ethics; fairness; labor; toolkits; work,,,2023-04,2023-11-06 01:29:49,2023-11-06 01:29:49,,,,57LMLERK,0.1538461538461538,0.4,0.25,0.1686935969058874
41,41,What Does It Mean to Embed Ethics in Data Science? An Integrative Approach Based on Microethics and Virtues,AI Soc.,36.0,3,10.1007/s00146-020-01112-w,"Bezuidenhout, Louise; Ratti, Emanuele",2021.0,https://doi.org/10.1007/s00146-020-01112-w,journalArticle,"In the past few years, scholars have been questioning whether the current approach in data ethics based on the higher level case studies and general principles is effective. In particular, some have been complaining that such an approach to ethics is difficult to be applied and to be taught in the context of data science. In response to these concerns, there have been discussions about how ethics should be “embedded” in the practice of data science, in the sense of showing how ethical issues emerge in small technical choices made by data scientists in their day-to-day activities, and how such an approach can be used to teach data ethics. However, a precise description of how such proposals have to be theoretically conceived and could be operationalized has been lacking. In this article, we propose a full-fledged characterization of ‘embedding’ ethics, and how this can be applied especially to the problem of teaching data science ethics. Using the emerging model of ‘microethics’, we propose a way of teaching daily responsibility in digital activities that is connected to (and draws from) the higher level ethical challenges discussed in digital/data ethics. We ground this microethical approach into a virtue theory framework, by stressing that the goal of a microethics is to foster the cultivation of moral virtues. After delineating this approach of embedding ethics in theoretical detail, this article discusses a concrete example of how such a ‘micro-virtue ethics’ approach could be practically taught to data science students.",Embedded ethics; Data science; Virtue ethics; Microethics; Teaching ethics,,0951-5666,2021-09,2023-11-06 01:30:03,2023-11-06 01:30:03,939–953,,,B9TXHEP8,0.1387755102040816,0.7777777777777778,0.1666666666666666,0.167833615141625
42,42,A Sector-Based Approach to AI Ethics: Understanding Ethical Issues of AI-Related Incidents within Their Sectoral Context,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604680,"Burema, Dafna; Debowski-Weimann, Nicole; von Janowski, Alexander; Grabowski, Jil; Maftei, Mihai; Jacobs, Mattis; van der Smagt, Patrick; Benbouzid, Djalel",2023.0,https://doi.org/10.1145/3600211.3604680,conferencePaper,"Acknowledging that society is made up of different sectors with their own rules and structures, this paper studies the relevance of a sector-specific perspective to AI ethics. Incidents with AI are studied in relation to five sectors (police, healthcare, education and academia, politics, automotive) using the AIAAIC repository. A total of 125 incidents are sampled and analyzed by conducting a qualitative content analysis on media reports. The results show that certain ethical principles are found breached across sectors: accuracy/reliability, bias/discrimination, transparency, surveillance/privacy, security. However, results also show that 1) some ethical issues (misinformation, safety, premise/intent) are sector specific, 2) the consequences and meaning of the same ethical issue is able to vary across sectors and 3) pre-existing sector-specific issues are reproduced with these ethical breaches. The paper concludes that general ethical principles are relevant to discuss across sectors, yet, a sector-based approach to AI ethics gives in-depth information on sector-specific structural issues.",,9798400702310,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,705–714,AIES '23,Association for Computing Machinery,4REIZHKM,0.1447368421052631,0.0,0.375,0.1672564663492472
43,43,Beyond Fairness and Explanation: Foundations of Trustworthiness of Artificial Agents,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539570,"Malle, Bertram F.",2022.0,https://doi.org/10.1145/3514094.3539570,conferencePaper,"The topics of fairness and explainability have dominated recent discussions of ethical AI. However, these are only two criteria that would make artificial agents anywhere close to ethical. I frame the question of ethical AI, and especially ethical social robots, as the question of what would make them worthy of human trust and actually eliciting human trust. Relying on a recent investigation of the multi-dimensionality of human trust, I lay out five criteria of trustworthiness-being competent, reliable, transparent, benevolent, and having ethical integrity. I will argue that an essential ingredient of such trustworthiness is norm competence-the ability to represent, comply with, and learn relevant social-moral norms (including fairness as one among many). I discuss the challenges to implementing norm competence and the critical role that justification, not just explanation, will play in providing evidence for such competence.",fairness; explainability; norms; ethical ai; trust; xai; robotethics; trustworthiness,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,4,AIES '22,Association for Computing Machinery,2I4CL7D7,0.145985401459854,0.5555555555555556,0.0,0.1663380091994811
44,44,Ethical Data Curation for AI: An Approach Based on Feminist Epistemology and Critical Theories of Race,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462598,"Leavy, Susan; Siapera, Eugenia; O'Sullivan, Barry",2021.0,https://doi.org/10.1145/3461702.3462598,conferencePaper,"The potential for bias embedded in data to lead to the perpetuation of social injustice though Artificial Intelligence (AI) necessitates an urgent reform of data curation practices for AI systems, especially those based on machine learning. Without appropriate ethical and regulatory frameworks there is a risk that decades of advances in human rights and civil liberties may be undermined. This paper proposes an approach to data curation for AI, grounded in feminist epistemology and informed by critical theories of race and feminist principles. The objective of this approach is to support critical evaluation of the social dynamics of power embedded in data for AI systems. We propose a set of fundamental guiding principles for ethical data curation that address the social construction of knowledge, call for inclusion of subjugated and new forms of knowledge, support critical evaluation of theoretical concepts within data and recognise the reflexive nature of knowledge. In developing this ethical framework for data curation, we aim to contribute to a virtue ethics for AI and ensure protection of fundamental and human rights.",critical theories of race; data curation; ethical ai; feminist theory,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,695–703,AIES '21,Association for Computing Machinery,4G3QRRXZ,0.1314285714285714,0.5,0.3125,0.1649661682854959
45,45,Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3593990,"Ali, Sanna J.; Christin, Angèle; Smart, Andrew; Katila, Riitta",2023.0,https://doi.org/10.1145/3593013.3593990,conferencePaper,"Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate “ethics washing.” Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.",AI ethics; decoupling; institutional change; neo-institutionalism; organizations; responsible AI; Science and Technology Studies,9798400701924,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,217–226,FAccT '23,Association for Computing Machinery,54WS3WZI,0.1363636363636363,0.3076923076923077,0.3125,0.1647601814073776
46,46,Ethical Design for AI in Medicine,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539564,"Pardoux, Éric",2022.0,https://doi.org/10.1145/3514094.3539564,conferencePaper,"This one-page abstract briefly presents the current status of my doctoral research in philosophy and ethics about the introduction of AI-based systems (AIS) in the field of healthcare. My main aim is to explore the ways in which ethical AIS for medicine could be ethically designed. I will briefly present the background of my research, my main activities so far and the upcoming avenues for research I envision.",AI for healthcare; artificial intelligence systems; design methodologies; ethical design,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,907,AIES '22,Association for Computing Machinery,VGBFQYQ8,0.0735294117647058,0.4,0.6666666666666666,0.1641540668800353
47,47,"Ethical Tools, Methods And&nbsp;Principles In&nbsp;Software Engineering And&nbsp;Development: Case Ethical User Stories","Product-Focused Software Process Improvement: 23rd International Conference, PROFES 2022, Jyväskylä, Finland, November 21–23, 2022, Proceedings",,,10.1007/978-3-031-21388-5_48,"Halme, Erika",2022.0,https://doi.org/10.1007/978-3-031-21388-5_48,conferencePaper,"The great leap with the development of Artificial Intelligence (AI) and Machine Learning (ML) technology has increased the range of different requirements for software quality, especially in terms of ethics. To implement high-level requirements, like ethical principles, into the workflow of software engineering, new requirements engineer tools are to be developed. Ethical User Stories (EUS) offers a simple way of implementing ethics in software development. This research has investigated the idea of using familiar requirements engineering artifacts, User Stories, to implement ethical principles, into the workflow of software engineering and operationalizing the studied phenomena of EUS. The preliminary results, found through two ongoing empirical studies with a data collection of 600+ EUS, show that EUS is a pressure-free, human-centric and accessible approach to Ethically Aligned Design (EAD) that intertwines with quality characteristics and relieves the developer from the heavy burden of ethical consideration to a smooth workflow of software engineering. An effective EUS is consistent throughout the user story and shares the idea that user-driven ethical motivation generates system functionality or benefits non-functional software design for quality assurance.",AI ethics; Agile software engineering; User stories,978-3-031-21387-8,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,631–637,,Springer-Verlag,9R98XI2L,0.1179775280898876,0.4285714285714285,0.5454545454545454,0.1639210914445383
48,48,Data Ethics Decision Aid (DEDA): A Dialogical Framework for Ethical Inquiry of AI and Data Projects in the Netherlands,Ethics and Inf. Technol.,23.0,3,10.1007/s10676-020-09577-5,"Franzke, Aline Shakti; Muis, Iris; Schäfer, Mirko Tobias",2021.0,https://doi.org/10.1007/s10676-020-09577-5,journalArticle,"This contribution discusses the development of the Data Ethics Decision Aid (DEDA), a framework for reviewing government data projects that considers their social impact, the embedded values and the government’s responsibilities in times of data-driven public management. Drawing from distinct qualitative research approaches, the DEDA framework was developed in an iterative process (2016–2018) and has since then been applied by various Dutch municipalities, the Association of Dutch Municipalities, and the Ministry of General Affairs (NL). We present the DEDA framework as an effective process to moderate case-deliberation and advance the development of responsible data practices. In addition, by thoroughly documenting the deliberation process, the DEDA framework establishes accountability. First, this paper sheds light on the necessity for data ethical case deliberation. Second, it describes the prototypes, the final design of the framework, and its evaluation. After a comparison with other frameworks, and a discussion of the findings, the paper concludes by arguing that the DEDA framework is a useful process for ethical evaluation of data projects for public management and an effective tool for creating awareness of ethical issues in data practices.",Big data; Data ethics; Data-driven public management; Ethical assessment; Information ethics; IoT; Value-sensitive design,,1388-1957,2021-09,2023-11-06 01:29:49,2023-11-06 01:29:49,551–567,,,PEMPIPHN,0.0934065934065934,0.6428571428571429,0.4210526315789473,0.1631781587601795
49,49,The Ethical Gravity Thesis: Marrian Levels and the Persistence of Bias in Automated Decision-Making Systems,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462606,"Kasirzadeh, Atoosa; Klein, Colin",2021.0,https://doi.org/10.1145/3461702.3462606,conferencePaper,"Computers are used to make decisions in an increasing number of domains. There is widespread agreement that some of these uses are ethically problematic. Far less clear is where ethical problems arise, and what might be done about them. This paper expands and defends the Ethical Gravity Thesis: ethical problems that arise at higher levels of analysis of an automated decision-making system are inherited by lower levels of analysis. Particular instantiations of systems can add new problems, but not ameliorate more general ones. We defend this thesis by adapting Marr's famous 1982 framework for understanding information-processing systems. We show how this framework allows one to situate ethical problems at the appropriate level of abstraction, which in turn can be used to target appropriate interventions.",algorithmic bias; algorithmic fairness; ethics of artificial intelligence; ethical artificial intelligence; justice; ethical machine learning; philosophy of artificial intelligence; politics of artificial intelligence,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,618–626,AIES '21,Association for Computing Machinery,ZQV2FINA,0.0967741935483871,0.391304347826087,0.2,0.1626425632923092
50,50,Ethical Principles for Reasoning about Value Preferences,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604728,"Woodgate, Jessica",2023.0,https://doi.org/10.1145/3600211.3604728,conferencePaper,"To ensure alignment with human interests, AI must consider the preferences of stakeholders, which includes reasoning about values and norms. However, stakeholders may have different preferences, and dilemmas can arise concerning conflicting values or norms. My work applies normative ethical principles to resolve dilemma scenarios in satisfactory ways that promote fairness.",fairness; norms; values; sociotechnical systems; normative ethical principles,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,972–974,AIES '23,Association for Computing Machinery,6FTV6AAW,0.0784313725490196,0.375,0.4285714285714285,0.1619818202871699
51,51,Automated Kantian Ethics,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539527,"Singh, Lavanya",2022.0,https://doi.org/10.1145/3514094.3539527,conferencePaper,"As we grant artificial intelligence increasing power and independence in contexts like healthcare, policing, and driving, AI faces moral dilemmas but lacks the tools to solve them. The dangers of unethical AI motivate automated ethics-i.e., the development of machines that can perform ethical reasoning. Though philosophically sophisticated ethical theories enable nuanced judgements, prior work in automated ethics rarely engages with philosophical literature. I contribute an implementation of automated Kantian ethics that is faithful to the Kantian philosophical tradition. My system can morally judge actions and is an early step towards philosophically mature ethical AI agents. I hope to develop an ""input parser"" that can parse natural language sentences into logical formulas that my system can evaluate.",Kant; automated ethics; Dyadic Deontic Logic; Isabelle/HOL,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,915,AIES '22,Association for Computing Machinery,UMPN42CE,0.1379310344827586,0.2857142857142857,0.6666666666666666,0.1616599225897255
52,52,The Ethics of Emotion in Artificial Intelligence Systems,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445939,"Stark, Luke; Hoey, Jesse",2021.0,https://doi.org/10.1145/3442188.3445939,conferencePaper,"In this paper, we develop a taxonomy of conceptual models and proxy data used for digital analysis of human emotional expression and outline how the combinations and permutations of these models and data impact their incorporation into artificial intelligence (AI) systems. We argue we should not take computer scientists at their word that the paradigms for human emotions they have developed internally and adapted from other disciplines can produce ground truth about human emotions; instead, we ask how different conceptualizations of what emotions are, and how they can be sensed, measured and transformed into data, shape the ethical and social implications of these AI systems.",AI ethics; artificial intelligence; ethics; fairness; AI; machine learning; emotion AI; affective computing; Action Control Theory; affect; Basic Emotion Theory; emotion; ML; norms; privacy,978-1-4503-8309-7,,2021,2023-11-06 01:30:03,2023-11-06 01:30:03,782–793,FAccT '21,Association for Computing Machinery,DSP85A5A,0.1047619047619047,0.3333333333333333,0.25,0.1600184530216823
53,53,Organisational Responses to the Ethical Issues of Artificial Intelligence,AI Soc.,37.0,1,10.1007/s00146-021-01148-6,"Stahl, Bernd Carsten; Antoniou, Josephina; Ryan, Mark; Macnish, Kevin; Jiya, Tilimbe",2022.0,https://doi.org/10.1007/s00146-021-01148-6,journalArticle,"The ethics of artificial intelligence (AI) is a widely discussed topic. There are numerous initiatives that aim to develop the principles and guidance to ensure that the development, deployment and use of AI are ethically acceptable. What is generally unclear is how organisations that make use of AI understand and address these ethical issues in practice. While there is an abundance of conceptual work on AI ethics, empirical insights are rare and often anecdotal. This paper fills the gap in our current understanding of how organisations deal with AI ethics by presenting empirical findings collected using a set of ten case studies and providing an account of the cross-case analysis. The paper reviews the discussion of ethical issues of AI as well as mitigation strategies that have been proposed in the literature. Using this background, the cross-case analysis categorises the organisational responses that were observed in practice. The discussion shows that organisations are highly aware of the AI ethics debate and keen to engage with ethical issues proactively. However, they make use of only a relatively small subsection of the mitigation strategies proposed in the literature. These insights are of importance to organisations deploying or using AI, to the academic AI ethics debate, but maybe most valuable to policymakers involved in the current debate about suitable policy developments to address the ethical issues raised by AI.",Ethics; Artificial intelligence; AI policy; Case study; Organisational response,,0951-5666,2022-03,2023-11-06 01:30:00,2023-11-06 01:30:00,23–37,,,A8WHBU5L,0.1415929203539823,0.3333333333333333,0.3333333333333333,0.1597741697444135
54,54,Artificial Intelligence ELSI Score for Science and Technology: A Comparison between Japan and the US,AI Soc.,38.0,4,10.1007/s00146-021-01323-9,"Hartwig, Tilman; Ikkatai, Yuko; Takanashi, Naohiro; Yokoyama, Hiromi M.",2022.0,https://doi.org/10.1007/s00146-021-01323-9,journalArticle,"Artificial intelligence (AI) has become indispensable in our lives. The development of a quantitative scale for AI ethics is necessary for a better understanding of public attitudes toward AI research ethics and to advance the discussion on using AI within society. For this study, we developed an AI ethics scale based on AI-specific scenarios. We investigated public attitudes toward AI ethics in Japan and the US using online questionnaires. We designed a test set using four dilemma scenarios and questionnaire items based on a theoretical framework for ethics, legal, and social issues (ELSI). We found that country and age are the most informative sociodemographic categories for predicting attitudes for AI ethics. Our proposed scale, which consists of 13 questions, can be reduced to only three, covering ethics, tradition, and policies. This new AI ethics scale will help to quantify how AI research is accepted in society and which area of ELSI people are most concerned with.",Ethics; Artificial Intelligence; Dilemma; ELSI,,0951-5666,2022-01,2023-11-06 01:29:59,2023-11-06 01:29:59,1609–1626,,,D4YXJKJ7,0.1602564102564102,0.4,0.0,0.1558417142339404
55,55,How Does Predictive Information Affect Human Ethical Preferences?,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534165,"Narayanan, Saumik; Yu, Guanghui; Tang, Wei; Ho, Chien-Ju; Yin, Ming",2022.0,https://doi.org/10.1145/3514094.3534165,conferencePaper,"Artificial intelligence (AI) has been increasingly involved in decision making in high-stakes domains, including loan applications, employment screening, and assistive clinical decision making. Meanwhile, involving AI in these high-stake decisions has created ethical concerns on how to balance different trade-offs to respect human values. One approach for aligning AIs with human values is to elicit human ethical preferences and incorporate this information in the design of computer systems. In this work, we explore how human ethical preferences are impacted by the information shown to humans during elicitation. In particular, we aim to provide a contrast between verifiable information (e.g., patient demographics or blood test results) and predictive information (e.g., the probability of organ transplant success). Using kidney transplant allocation as a case study, we conduct a randomized experiment to elicit human ethical preferences on scarce resource allocation to understand how human ethical preferences are impacted by the verifiable and predictive information. We find that the presence of predictive information significantly changes how humans take into account other verifiable information in their ethical preferences. We also find that the source of the predictive information (e.g., whether the predictions are made by AI or human doctors) plays a key role in how humans incorporate the predictive information into their own ethical judgements.",ai ethics; ethical preference; preference elicitation,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,508–517,AIES '22,Association for Computing Machinery,4XBEB7VB,0.1142857142857142,1.0,0.375,0.1545182619647355
56,56,Epistemic Reasoning for Machine Ethics with Situation Calculus,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462586,"Pagnucco, Maurice; Rajaratnam, David; Limarga, Raynaldio; Nayak, Abhaya; Song, Yang",2021.0,https://doi.org/10.1145/3461702.3462586,conferencePaper,"With the rapid development of autonomous machines such as selfdriving vehicles and social robots, there is increasing realisation that machine ethics is important for widespread acceptance of autonomous machines. Our objective is to encode ethical reasoning into autonomous machines following well-defined ethical principles and behavioural norms. We provide an approach to reasoning about actions that incorporates ethical considerations. It builds on Scherl and Levesque's [29, 30] approach to knowledge in the situation calculus. We show how reasoning about knowledge in a dynamic setting can be used to guide ethical and moral choices, aligned with consequentialist and deontological approaches to ethics. We apply our approach to autonomous driving and social robot scenarios, and provide an implementation framework.",machine ethics; epistemic logic; knowledge representation; situation calculus,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,814–821,AIES '21,Association for Computing Machinery,AAT628QT,0.1379310344827586,0.25,0.25,0.1541408016075209
57,57,Broadening AI Ethics Narratives: An Indic Art View,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3593971,"Divakaran, Ajay; Sridhar, Aparna; Srinivasan, Ramya",2023.0,https://doi.org/10.1145/3593013.3593971,conferencePaper,"Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction–that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the ‘Natyashastra’), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability",AI ethics; Indian arts,9798400701924,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,2–11,FAccT '23,Association for Computing Machinery,4SQQGBXJ,0.1388888888888889,0.75,0.375,0.1528314620528867
58,58,Centring Dignity in Algorithm Development: Testing a Dignity Lens,Proceedings of the 34th Australian Conference on Human-Computer Interaction,,,10.1145/3572921.3572938,"Ruster, Lorenn P.; Oliva-Altamirano, Paola; Daniell, Katherine A.",2023.0,https://doi.org/10.1145/3572921.3572938,conferencePaper,"Against a backdrop of algorithms that disempower, dehumanise, disenfranchise and discriminate, there are increasing calls to centre the human in AI development processes and to humanise AI development in practice; centring dignity in AI development could provide a way forward. Despite the inclusion of dignity in many Artificial Intelligence (AI) ethics frameworks, like many other AI ethics principles, there is little operational understanding of what dignity can look like in practice when it comes to the development of algorithms. Drawing on cybernetics and a model of dignity developed in the field of international conflict resolution, this paper presents our work-in-progress tool - the Dignity Lens - for considering dignity throughout the AI development lifecycle, and practitioner reflections from using the tool. This work is an initial step towards articulating what dignity-centred AI development could look like in practice, assisting practitioners designing and developing algorithms to actively consider dignity.",AI Ethics; Cybernetics; Dignity; Interaction Design,9798400700248,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,1–8,OzCHI '22,Association for Computing Machinery,6GZV636E,0.1216216216216216,0.6666666666666666,0.2222222222222222,0.1515835206934683
59,59,Recommender Systems for Mental Health Apps: Advantages and Ethical Challenges,AI Soc.,38.0,4,10.1007/s00146-021-01322-w,"Valentine, Lee; D’Alfonso, Simon; Lederman, Reeva",2022.0,https://doi.org/10.1007/s00146-021-01322-w,journalArticle,"Recommender systems assist users in receiving preferred or relevant services and information. Using such technology could be instrumental in addressing the lack of relevance digital mental health apps have to the user, a leading cause of low engagement. However, the use of recommender systems for digital mental health apps, particularly those driven by personal data and artificial intelligence, presents a range of ethical considerations. This paper focuses on considerations particular to the juncture of recommender systems and digital mental health technologies. While separate bodies of work have focused on these two areas, to our knowledge, the intersection presented in this paper has not yet been examined. This paper identifies and discusses a set of advantages and ethical concerns related to incorporating recommender systems into the digital mental health (DMH) ecosystem. Advantages of incorporating recommender systems into DMH apps are identified as (1) a reduction in choice overload,&nbsp;(2) improvement to the digital therapeutic alliance, and (3) increased access to personal data &amp; self-management.&nbsp;Ethical challenges identified are (1) lack of explainability, (2) complexities pertaining to the privacy/personalization trade-off and recommendation quality, and (3) the control of app usage history data. These novel considerations will provide a greater understanding of how DMH apps can effectively and ethically implement recommender systems.",Digital ethics; Ethics; Artificial intelligence; Recommender systems; Digital health; Digital interventions; Mental health,,0951-5666,2022-01,2023-11-06 01:30:02,2023-11-06 01:30:02,1627–1638,,,7C9XSU6T,0.0917874396135265,0.7692307692307693,0.3,0.1506308822867895
60,60,Comparative Analysis of Students and Faculty Level of Awareness and Knowledge of Digital Citizenship Practices in a Distance Learning Environment: Case Study,Education and Information Technologies,27.0,5,10.1007/s10639-021-10868-7,"Hawamdeh, Mahmoud; Altınay, Zehra; Altınay, Fahriye; Arnavut, Ahmet; Ozansoy, Kezban; Adamu, Idris",2022.0,https://doi.org/10.1007/s10639-021-10868-7,journalArticle,"The COVID-19 pandemic increase the use of distance learning while studies have shown that there is insufficient digital knowledge among students in distance leaning as they do not adequately use technology as a digital citizenship indicator, while the awareness and knowledge of digital citizenship among teachers and students remains a key criterion for improving distance learning that mainly depends on information technology. Therefore, this study comes up to examine the awareness and knowledge of students and faculty of digital citizenship in distance environment by focusing on two different higher academic institutions, namely the Al-Quds Open University (QOU) in the Palestinian territories and the University of Kyrenia (KU) in the Turkish Republic of Northern Cyprus in 2020, using interview, descriptive analysis, and Z-test Technique. The results revealed that students and faculty in both institutions were aware of the digital citizenship concepts, but lacked the in-depth knowledge and understanding of concepts such as digital rights, digital security, and digital ethics. Furthermore, the awareness and knowledge of digital citizenship among KU students are higher than QOU students. Faculty in both institutions agreed with the importance of integrating digital citizenship practices such as digital rights, digital security, and digital ethics into elearning curriculum.",Comparative study; Digital citizenship; Distance learning; e-learning; ICT,,1360-2357,2022-06,2023-11-06 01:30:02,2023-11-06 01:30:02,6037–6068,,,RTE3ERVZ,0.1507537688442211,0.25,0.0909090909090909,0.1494898173160092
61,61,ECCOLA — A Method for Implementing Ethically Aligned AI Systems,J. Syst. Softw.,182.0,C,10.1016/j.jss.2021.111067,"Vakkuri, Ville; Kemell, Kai-Kristian; Jantunen, Marianna; Halme, Erika; Abrahamsson, Pekka",2021.0,https://doi.org/10.1016/j.jss.2021.111067,journalArticle,"Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners.",AI ethics; Ethics; Artificial intelligence; Implementing; Method,,0164-1212,2021-12,2023-11-06 01:29:59,2023-11-06 01:29:59,,,,MMWVMJPW,0.109375,0.7142857142857143,0.1,0.1485968731628453
62,62,AI Ethics: How Can Information Ethics Provide a Framework to Avoid Usual Conceptual Pitfalls? An Overview,AI Soc.,36.0,3,10.1007/s00146-020-01077-w,"Bruneault, Frédérick; Laflamme, Andréane Sabourin",2021.0,https://doi.org/10.1007/s00146-020-01077-w,journalArticle,"Artificial intelligence (AI) plays an important role in current discussions on information and communication technologies (ICT) and new modes of algorithmic governance. It is an unavoidable dimension of what social mediations and modes of reproduction of our information societies will be in the future. While several works in artificial intelligence ethics (AIE) address ethical issues specific to certain areas of expertise, these ethical reflections often remain confined to narrow areas of application, without considering the global ethical issues in which they are embedded. We, therefore, propose to clarify the main approaches to AIE, their philosophical assumptions and the specific characteristics of each one of them, to identify the most promising approach to develop an ethical reflection on the deployment of AI in our societies, which is the one based on information ethics as proposed by Luciano Floridi. We will identify the most important features of that approach to highlight areas that need further investigation.",Artificial intelligence; Ethics; Information; Liberalism; Transhumanism,,0951-5666,2021-09,2023-11-06 01:29:49,2023-11-06 01:29:49,757–766,,,ST55NMVY,0.1168831168831168,0.3333333333333333,0.3125,0.1465589334907516
63,63,An Analysis of Opinions on AI Ethics Based on Network Motifs Using the Case Method,Proceedings of the 2023 12th International Conference on Software and Computer Applications,,,10.1145/3587828.3587873,"Shao, Tengfei; Ieiri, Yuya; Hosoya, Hitoyoshi; Hishiyama, Reiko",2023.0,https://doi.org/10.1145/3587828.3587873,conferencePaper,"The development of artificial intelligence (AI) technologies has led to significant impacts on society. Simultaneously, the ethical issues involved in AI have become increasingly serious, and AI ethics has attracted attention as a topic of active research and discussion. Education in ethics is widely understood to play a crucial role in solving these emerging issues. In this study, we propose a model developed to address some ethical issues of AI based on the case method using network motifs to analyze educational consequences. We evaluated the method in terms of a network analysis of textual data of opinions to understand the changes in participants' perspectives before and after their experience with the case method. The experimental results validated the method we proposed, and the results of the analysis show that the participants’ opinions on the ethics of AI changed to some extent after participating in the case.",AI ethics; Network analysis,978-1-4503-9858-9,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,302–308,ICSCA '23,Association for Computing Machinery,KH2V9VIJ,0.1232876712328767,0.75,0.2,0.1455851196820759
64,64,From the Ground up: Developing a Practical Ethical Methodology for Integrating AI into Industry,AI Soc.,38.0,2,10.1007/s00146-022-01531-x,"Anderson, Marc M.; Fort, Karën",2022.0,https://doi.org/10.1007/s00146-022-01531-x,journalArticle,"In this article we present a new approach to practical artificial intelligence (AI) ethics in heavy industry, which was developed in the context of an EU Horizons 2020 multi partner project. We begin with a review of the concept of Industry 4.0, discussing the limitations of the concept, and of iterative categorization of heavy industry generally, for a practical human centered ethical approach. We then proceed to an overview of actual and potential AI ethics approaches to heavy industry, suggesting that current approaches with their emphasis on broad high-level principles are not well suited to AI ethics for industry. From there we outline our own approach in two sections. The first suggests tailoring ethics to the time and space situation of the shop floor level worker from the ground up, including giving specific and evolving ethical recommendations. The second describes the ethicist’s role as an ethical supervisor immersed in the development process and interpreting between industrial and technological (tech) development partners. In presenting our approach we draw heavily on our own experiences in applying the method in the Use Cases of our project, as examples of what can be done.",Artificial intelligence; Ethical supervisor; Industry 4.0; Tailored ethics; Workplace,,0951-5666,2022-07,2023-11-06 01:30:00,2023-11-06 01:30:00,631–645,,,4DNVM4J6,0.1052631578947368,0.5555555555555556,0.2857142857142857,0.1452759455799547
65,65,How Do Software Companies Deal with Artificial Intelligence Ethics? A Gap Analysis,Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering,,,10.1145/3530019.3530030,"Vakkuri, Ville; Kemell, Kai-Kristian; Tolvanen, Joel; Jantunen, Marianna; Halme, Erika; Abrahamsson, Pekka",2022.0,https://doi.org/10.1145/3530019.3530030,conferencePaper,"The public and academic discussion on Artificial Intelligence (AI) ethics is accelerating and the general public is becoming more aware AI ethics issues such as data privacy in these systems. To guide ethical development of AI systems, governmental and institutional actors, as well as companies, have drafted various guidelines for ethical AI. Though these guidelines are becoming increasingly common, they have been criticized for a lack of impact on industrial practice. There seems to be a gap between research and practice in the area, though its exact nature remains unknown. In this paper, we present a gap analysis of the current state of the art by comparing practices of 39 companies that work with AI systems to the seven key requirements for trustworthy AI presented in the “The Ethics Guidelines for Trustworthy Artificial Intelligence”. The key finding of this paper is that there is indeed notable gap between AI ethics guidelines and practice. Especially practices considering the novel requirements for software development, requirements of societal and environmental well-being and diversity, nondiscrimination and fairness were not tackled by companies.",Responsible AI; Artificial Intelligence Ethics; Gap Analysis,978-1-4503-9613-4,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,100–109,EASE '22,Association for Computing Machinery,DMIY2ZGR,0.1292134831460674,0.4285714285714285,0.1666666666666666,0.1452200567946296
66,66,Designing Ethical Artifacts Has Resulted in Creative Design: Empirical Studies on the Effect of an Ethical Design Support Tool,AI Soc.,36.0,1,10.1007/s00146-020-01043-6,"Sekiguchi, Kaira; Hori, Koichi",2021.0,https://doi.org/10.1007/s00146-020-01043-6,journalArticle,"Ethical aspects in engineering design have become increasingly important in recent years. A typical example is the recent rise of artificial intelligence (AI) ethics. This paper applies user studies of a design support tool to empirically verify that our ethical framework improves the creativity of an engineer’s design activity. The design support tool provides an environment for the promotion of ethical design perspectives and description. The experiments focus on two functionalities: semi-automatic generation and scenario path recommendation. These functions are designed around the application of ethical design theory, which extends the hierarchical representation of artifacts. Doing this enables users to reconsider their themes at the highest level of the hierarchy and to apply a wider conceptual space of design solutions. For example, by reconsidering the positions of their research themes in the space of the representation field, users can semi-automatically edit them and identify focal areas. Using the scenario path recommendation, designers can update their research themes after considering the ethical impacts of those themes on stakeholders. Both functions are realized by exploiting a knowledge base of ethical and technological discourses. Finally, the ethical design theory is updated based on some unexpected results of our user studies with regards to the cyclic relationship among theory, tools (i.e., experimental equipment), and observed data. For example, temporal dimensional aspects were confirmed as important.",Ethics; Creativity support; Design science; Design theory; Ethical design; User study,,0951-5666,2021-03,2023-11-06 01:30:04,2023-11-06 01:30:04,101–148,,,PYFRTC22,0.1131221719457013,0.4545454545454545,0.3157894736842105,0.1441381236455485
67,67,Robust Artificial Moral Agents and Metanormativity,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604703,"Cook, Tyler",2023.0,https://doi.org/10.1145/3600211.3604703,conferencePaper,"This paper explores the relationship between our ignorance concerning certain metanormative topics and the design of ethical artificial intelligence (AI). In particular, it will be maintained that because we cannot predict in advance which metanormative conclusions a sufficiently intelligent ethical AI might reach, we have reason to be apprehensive about the project of designing such AI. Even if we succeeded at designing an AI to engage in ethical behavior, there is a distinct possibility that the AI might eventually cease to behave ethically if it reaches certain metanormative conclusions. The candidate conclusions include ones such as the denial of the alleged authority or overridingness of ethics and the conclusion that there are no ethical facts or properties (i.e. moral error theory). It will be argued that the target AI could conceivably reach such conclusions, and in turn this could cause them to abandon their ethical routines and proceed to cause great harm.",,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,162–169,AIES '23,Association for Computing Machinery,B6A7UBID,0.1513157894736842,0.0,0.0,0.1439703628002044
68,68,Toward Best Practices in Embedded Ethics: Suggestions for Interdisciplinary Technology Development,Robot. Auton. Syst.,167.0,C,10.1016/j.robot.2023.104467,"Tigard, Daniel W.; Braun, Maximilian; Breuer, Svenja; Ritt, Konstantin; Fiske, Amelia; McLennan, Stuart; Buyx, Alena",2023.0,https://doi.org/10.1016/j.robot.2023.104467,journalArticle,"With current developments in robotics and artificial intelligence (AI) comes an increased focus on the ethical production and use of such novel technologies. Accordingly, a trend toward “embedded ethics” is seen in recent research, reflecting an increase in efforts to integrate social and ethical considerations in computer science education and early in the development phases of AI and robotics. What remains to be established, however, is a more concrete understanding of the best working modalities for such interdisciplinary collaborations. In this brief discussion paper, we provide reflections derived from our interdisciplinary Responsible Robotics project which integrates ethicists and social scientists at early stages of development. We put forward several suggestions on how to integrate ethics and social science within technological development. We believe these suggestions can serve as a working taxonomy of best practices for embedded ethics.",AI ethics; Artificial intelligence; Embedded ethics; Robotics; Interdisciplinary collaboration; Responsible robotics,,0921-8890,2023-09,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,3ZA8PK9R,0.1021897810218978,0.4545454545454545,0.1818181818181818,0.1436168800914879
69,69,Effects Of&nbsp;Fairness And&nbsp;Explanation On&nbsp;Trust In&nbsp;Ethical AI,"Machine Learning and Knowledge Extraction: 6th IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2022, Vienna, Austria, August 23–26, 2022, Proceedings",,,10.1007/978-3-031-14463-9_4,"Angerschmid, Alessa; Theuermann, Kevin; Holzinger, Andreas; Chen, Fang; Zhou, Jianlong",2022.0,https://doi.org/10.1007/978-3-031-14463-9_4,conferencePaper,"AI ethics has been a much discussed topic in recent years. Fairness and explainability are two important ethical principles for trustworthy AI. In this paper, the impact of AI explainability and fairness on user trust in AI-assisted decisions is investigated. For this purpose, a user study was conducted simulating AI-assisted decision making in a health insurance scenario. The study results demonstrated that fairness only affects user trust when the fairness level is low, with a low fairness level reducing user trust. However, adding explanations helped users increase their trust in AI-assisted decision making. The results show that the use of AI explanations and fairness statements in AI applications is complex: we need to consider not only the type of explanations, but also the level of fairness introduced. This is a strong motivation for further work.",AI ethics; AI explanation; AI fairness; Trust,978-3-031-14462-2,,2022,2023-11-06 01:29:59,2023-11-06 01:29:59,51–67,,Springer-Verlag,YJB49H38,0.1037037037037037,0.8571428571428571,0.1666666666666666,0.1429513002819372
70,70,Developing A&nbsp;Professional Profile Of&nbsp;a&nbsp;Digital Ethics Officer In&nbsp;an&nbsp;Educational Technology Unit In&nbsp;Higher Education,"Learning and Collaboration Technologies. Designing the Learner and Teacher Experience: 9th International Conference, LCT 2022, Held as Part of the 24th HCI International Conference, HCII 2022, Virtual Event, June 26 – July 1, 2022, Proceedings, Part I",,,10.1007/978-3-031-05657-4_12,"Andrews, David; Leitner, Philipp; Schön, Sandra; Ebner, Martin",2022.0,https://doi.org/10.1007/978-3-031-05657-4_12,conferencePaper,"The digitalisation of learning, teaching, and study processes has a major impact on possible evaluations and uses of data, for example with regard to individual learning recommendations, prognosis, or assessments. This also gives rise to ethical issues centered around digital teaching and possible challenges of data use. One possible approach to this challenge might be to install a Digital Ethics Officer (DEO), whose future profile this paper outlines for a Educational Technology unit of a Higher Education Institution (HEI). Therefore, an introductory overview of the tasks and roles of Ethics Officers (EO) is given based on the literature. The authors then describe the current ethics program of a university of technology and collect current and potential ethical issues from the field of educational technologies. Based on this, a first professional profile for a DEO at an educational technology unit of a university is described. From the authors’ point of view, the article thus prepares important considerations and steps for the future of this position.",Digital ethics officer; Educational technology; Higher education,978-3-031-05656-7,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,157–175,,Springer-Verlag,ET2QTBLC,0.1097560975609756,0.5714285714285714,0.1818181818181818,0.1410764112217594
71,71,Emotional AI: Legal and Ethical Challenges1,Info. Pol.,27.0,2,10.3233/IP-211529,"Gremsl, Thomas; Hödl, Elisabeth; Čas, Johann; De Hert, Paul; Porcedda, Maria Grazia; Raab, Charles D.",2022.0,https://doi.org/10.3233/IP-211529,journalArticle,"The European Commission has presented a draft for an Artificial Intelligence Act (AIA). This article deals with legal and ethical questions of the datafication of human emotions. In particular, it raises the question of how emotions are to be legally classified. In particular, the concept of “emotion recognition systems” in the sense of the draft Artificial Intelligence Act (AIA) published by the European Commission is addressed. As it turns out, the fundamental right to freedom of thought as well as the question of the common good and human dignity become relevant in this context, especially when such systems are combined with others, such as scoring models.",artificial intelligence; digital ethics; data protection; Emotional data; freedom of thought; social credit systems; social ethics; social scoring,,1570-1255,2022-01,2023-11-06 01:30:02,2023-11-06 01:30:02,163–174,,,7JLGYZRN,0.0377358490566037,0.4444444444444444,0.6666666666666666,0.1386978585003624
72,72,Examining Responsibility and Deliberation in AI Impact Statements and Ethics Reviews,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534155,"Liu, David; Nanayakkara, Priyanka; Sakha, Sarah Ariyan; Abuhamad, Grace; Blodgett, Su Lin; Diakopoulos, Nicholas; Hullman, Jessica R.; Eliassi-Rad, Tina",2022.0,https://doi.org/10.1145/3514094.3534155,conferencePaper,"The artificial intelligence research community is continuing to grapple with the ethics of its work by encouraging researchers to discuss potential positive and negative consequences. Neural Information Processing Systems (NeurIPS), a top-tier conference for machine learning and artificial intelligence research, first required a statement of broader impact in 2020. In 2021, NeurIPS updated their call for papers such that 1) the impact statement focused on negative societal impacts and was not required but encouraged, 2) a paper checklist and ethics guidelines were provided to authors, and 3) papers underwent ethics reviews and could be rejected on ethical grounds. In light of these changes, we contribute a qualitative analysis of 231 impact statements and all publicly-available ethics reviews. We describe themes arising around the ways in which authors express agency (or lack thereof) in identifying or mitigating negative consequences and assign responsibility for mitigating negative societal impacts. We also characterize ethics reviews in terms of the types of issues raised by ethics reviewers (falling into categories of policy-oriented and non-policy-oriented), recommendations ethics reviewers make to authors (e.g., in terms of adding or removing content), and interaction between authors, ethics reviewers, and original reviewers (e.g., consistency between issues flagged by original reviewers and those discussed by ethics reviewers). Finally, based on our analysis we make recommendations for how authors can be further supported in engaging with the ethical implications of their work.",ai ethics; broader impact; ethics review; impact statements,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,424–435,AIES '22,Association for Computing Machinery,AYFV3RUD,0.108695652173913,0.625,0.3636363636363636,0.1382720354604495
73,73,Enter the Metrics: Critical Theory and Organizational Operationalization of AI Ethics,AI Soc.,37.0,4,10.1007/s00146-021-01256-3,"Krijger, Joris",2022.0,https://doi.org/10.1007/s00146-021-01256-3,journalArticle,"As artificial intelligence (AI) deployment is growing exponentially, questions have been raised whether the developed AI ethics discourse is apt to address the currently pressing questions in the field. Building on critical theory, this article aims to expand the scope of AI ethics by arguing that in addition to ethical principles and design, the organizational dimension (i.e. the background assumptions and values influencing design processes) plays a pivotal role in the operationalization of ethics in AI development and deployment contexts. Through the prism of critical theory, and the notions of underdetermination and technical code as developed by Feenberg in particular, the organizational dimension is related to two general challenges in operationalizing ethical principles in AI: (a) the challenge of ethical principles placing conflicting demands on an AI design that cannot be satisfied simultaneously, for which the term ‘inter-principle tension’ is coined, and (b) the challenge of translating an ethical principle to a technological form, constraint or demand, for which the term ‘intra-principle tension’ is coined. Rather than discussing principles, methods or metrics, the notion of technical code precipitates a discussion on the subsequent questions of value decisions, governance and procedural checks and balances. It is held that including and interrogating the organizational context in AI ethics approaches allows for a more in depth understanding of the current challenges concerning the formalization and implementation of ethical principles as well as of the ways in which these challenges could be met.",Artificial intelligence; Critical theory; Ethics; Feenberg; Organizational operationalization; Philosophy,,0951-5666,2022-12,2023-11-06 01:29:49,2023-11-06 01:29:49,1427–1437,,,I82JZAME,0.1255230125523012,0.2222222222222222,0.2727272727272727,0.1379358160190122
74,74,Software Engineering for Responsible AI: An Empirical Study and Operationalised Patterns,Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice,,,10.1145/3510457.3513063,"Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon; Douglas, David; Sanderson, Conrad",2022.0,https://doi.org/10.1145/3510457.3513063,conferencePaper,"AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to develop responsible AI systems. To address this shortcoming, we perform an empirical study involving interviews with 21 scientists and engineers to understand the practitioners' views on AI ethics principles and their implementation. Our major findings are: (1) the current practice is often a done-once-and-forget type of ethical risk assessment at a particular development step, which is not sufficient for highly uncertain and continual learning AI systems; (2) ethical requirements are either omitted or mostly stated as high-level objectives, and not specified explicitly in verifiable way as system outputs or outcomes; (3) although ethical requirements have the characteristics of cross-cutting quality and non-functional requirements amenable to architecture and design analysis, system-level architecture and design are under-explored; (4) there is a strong desire for continuously monitoring and validating AI systems post deployment for ethical requirements but current operation practices provide limited guidance. To address these findings, we suggest a preliminary list of patterns to provide operationalised guidance for developing responsible AI systems.",artificial intelligence; ethics; responsible AI; AI; machine learning; DevOps; software architecture; software engineering,978-1-4503-9226-6,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,241–242,ICSE-SEIP '22,Association for Computing Machinery,9X9GA34E,0.1242937853107344,0.3076923076923077,0.0909090909090909,0.137356128471492
75,75,Mythical Ethical Principles For&nbsp;AI And&nbsp;How To&nbsp;Attain Them,Human-Centered Artificial Intelligence: Advanced Lectures,,,10.1007/978-3-031-24349-3_15,"Slavkovik, Marija",2023.0,https://doi.org/10.1007/978-3-031-24349-3_15,conferencePaper,"To have ethical AI two questions need to be answered: i) what is the ethical impact that an AI system can have, and, ii) what does it mean for an AI system to behave ethically. The lack of answers to both of these questions hinder the identification of what are the values or principles that we want upheld by AI and for AI. Identifying these principles is not enough, we also want to define them so that they can be operational, or at least understand what operational means here. There is a gap between moral philosophy and ethically behaving AI. The tutorial contributes towards closing this gap, by motivating researchers to interpret an abstract principle from moral philosophy into an algorithmic property that can be formally specified and measured or computationally implemented. The tutorial uses recent articles in AI ethics that attempt to define and identify pertinent ethical principles, as well as ethically motivated desirable algorithmic properties.",,978-3-031-24348-6,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,275–303,,Springer-Verlag,GD9HATSX,0.1146496815286624,0.0,0.4285714285714285,0.1364455983918053
76,76,A Transparency Index Framework for AI in Education,"Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners’ and Doctoral Consortium: 23rd International Conference, AIED 2022, Durham, UK, July 27–31, 2022, Proceedings, Part II",,,10.1007/978-3-031-11647-6_33,"Chaudhry, Muhammad Ali; Cukurova, Mutlu; Luckin, Rose",2022.0,https://doi.org/10.1007/978-3-031-11647-6_33,conferencePaper,"Numerous AI ethics checklists and frameworks have been proposed focusing on different dimensions of ethical AI such as fairness, explainability, and safety. Yet, no such work has been done on developing transparent AI systems for real-world educational scenarios. This paper presents a Transparency Index framework that has been iteratively co-designed with different stakeholders of AI in education, including educators, ed-tech experts, and AI practitioners. We map the requirements of transparency for different categories of stakeholders of AI in education. The main contribution of this study is that it highlights the importance of transparency in developing AI-powered educational technologies and proposes an index framework for its conceptualization for AI in education.",AI development pipelines; AI in education; Algorithmic transparency; Bias in AI; Human-centred AI; Transparency in AI,978-3-031-11646-9,,2022,2023-11-06 01:30:02,2023-11-06 01:30:02,195–198,,Springer-Verlag,8NAGWT6Q,0.109090909090909,0.3125,0.125,0.1350891312704208
77,77,"More Data Types More Problems: A Temporal Analysis of Complexity, Stability, and Sensitivity in Privacy Policies","Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594065,"Lovato, Juniper; Mueller, Philip; Suchdev, Parisa; Dodds, Peter",2023.0,https://doi.org/10.1145/3593013.3594065,conferencePaper,"Collecting personally identifiable information (PII) on data subjects has become big business. Data brokers and data processors are part of a multi-billion-dollar industry that profits from collecting, buying, and selling consumer data. Yet there is little transparency in the data collection industry which makes it difficult to understand what types of data are being collected, used, and sold, and thus the risk to individual data subjects. In this study, we examine a large textual dataset of privacy policies from 1997-2019 in order to investigate the data collection activities of data brokers and data processors. We also develop an original lexicon of PII-related terms representing PII data types curated from legislative texts. This mesoscale analysis looks at privacy policies over time on the word, topic, and network levels to understand the stability, complexity, and sensitivity of privacy policies over time. We find that (1) privacy legislation may be correlated with changes in stability and turbulence of PII data types in privacy policies; (2) the complexity of privacy policies decreases over time and becomes more regularized; (3) sensitivity rises over time and shows spikes that appear to be correlated with events when new privacy legislation is introduced.",Privacy; NLP; Data Ethics; Data Privacy; Data Science; Networks; Privacy Policies,9798400701924,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,1088–1100,FAccT '23,Association for Computing Machinery,X2H4WL2U,0.0974358974358974,0.7272727272727273,0.125,0.1341165501165501
78,78,AI-Competent Individuals and Laypeople Tend to Oppose Facial Analysis AI,"Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",,,10.1145/3551624.3555294,"Ullstein, Chiara; Engelmann, Severin; Papakyriakopoulos, Orestis; Hohendanner, Michel; Grossklags, Jens",2022.0,https://doi.org/10.1145/3551624.3555294,conferencePaper,"Recent advances in computer vision analysis have led to a debate about the kinds of conclusions artificial intelligence (AI) should make about people based on their faces. Some scholars have argued for supposedly “common sense” facial inferences that can be reliably drawn from faces using AI. Other scholars have raised concerns about an automated version of “physiognomic practices” that facial analysis AI could entail. We contribute to this multidisciplinary discussion by exploring how individuals with AI competence and laypeople evaluate facial analysis AI inference-making. Ethical considerations of both groups should inform the design of ethical computer vision AI. In a two-scenario vignette study, we explore how ethical evaluations of both groups differ across a low-stake advertisement and a high-stake hiring context. Next to a statistical analysis of AI inference ratings, we apply a mixed methods approach to evaluate the justification themes identified by a qualitative content analysis of participants’ 2768 justifications. We find that people with AI competence (N=122) and laypeople (N=122; validation N=102) share many ethical perceptions about facial analysis AI. The application context has an effect on how AI inference-making from faces is perceived. While differences in AI competence did not have an effect on inference ratings, specific differences were observable for the ethical justifications. A validation laypeople dataset confirms these results. Our work offers a participatory AI ethics approach to the ongoing policy discussions on the normative dimensions and implications of computer vision AI. Our research seeks to inform, challenge, and complement conceptual and theoretical perspectives on computer vision AI ethics.",artificial intelligence; ethics; computer vision; human faces; public participation,978-1-4503-9477-2,,2022,2023-11-06 01:30:02,2023-11-06 01:30:02,,EAAMO '22,Association for Computing Machinery,EHYI7YDL,0.1299212598425196,0.2222222222222222,0.1,0.1327982970258289
79,79,An Analysis of Engineering Students' Responses to an AI Ethics Scenario,Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence,,,10.1609/aaai.v37i13.26880,"Orchard, Alexi; Radke, David",2023.0,https://doi.org/10.1609/aaai.v37i13.26880,conferencePaper,"In light of significant issues in the technology industry, such as algorithms that worsen racial biases, the spread of online misinformation, and the expansion of mass surveillance, it is increasingly important to teach the ethics and sociotechnical implications of developing and using artificial intelligence (AI). Using 53 survey responses from engineering undergraduates, this paper measures students' abilities to identify, mitigate, and reflect on a hypothetical AI ethics scenario. We engage with prior research on pedagogical approaches to and considerations for teaching AI ethics and highlight some of the obstacles that engineering undergraduate students experience in learning and applying AI ethics concepts.",,978-1-57735-880-0,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,,AAAI'23/IAAI'23/EAAI'23,AAAI Press,ZFKEBJF8,0.1188118811881188,0.0,0.2727272727272727,0.1326098518942803
80,80,Ethics in Conversation: Building an Ethics Assurance Case for Autonomous AI-Enabled Voice Agents in Healthcare,Proceedings of the First International Symposium on Trustworthy Autonomous Systems,,,10.1145/3597512.3599713,"Kaas, Marten H. L.; Porter, Zoe; Lim, Ernest; Higham, Aisling; Khavandi, Sarah; Habli, Ibrahim",2023.0,https://doi.org/10.1145/3597512.3599713,conferencePaper,"The deployment and use of AI systems should be both safe and broadly ethically acceptable. The principles-based ethics assurance argument pattern is one proposal in the AI ethics landscape that seeks to support and achieve that aim. The purpose of this argument pattern or framework is to structure reasoning about, and to communicate and foster confidence in, the ethical acceptability of uses of specific real-world AI systems in complex socio-technical contexts. This paper presents the interim findings of a case study applying this ethics assurance framework to the use of Dora, an AI-based telemedicine system, to assess its viability and usefulness as an approach. The case study process to date has revealed some of the positive ethical impacts of the Dora platform, as well as unexpected insights and areas to prioritise for evaluation, such as risks to the frontline clinician, particularly in respect of clinician autonomy. The ethics assurance argument pattern offers a practical framework not just for identifying issues to be addressed, but also to start to construct solutions in the form of adjustments to the distribution of benefits, risks and constraints on human autonomy that could reduce ethical disparities across affected stakeholders. Though many challenges remain, this research represents a step in the direction towards the development and use of safe and ethically acceptable AI systems and, ideally, a shift towards more comprehensive and inclusive evaluations of AI systems in general.",case study; AI-based telemedicine; Dora platform; ethical acceptability; ethics assurance; medical device,9798400707346,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,,TAS '23,Association for Computing Machinery,5CJ7D64L,0.1030042918454935,0.4166666666666667,0.2666666666666666,0.1324308721958992
81,81,The Contested Role of AI Ethics Boards in Smart Societies: A Step towards Improvement Based on Board Composition by Sortition,Ethics and Inf. Technol.,25.0,4,10.1007/s10676-023-09724-8,"Conti, Ludovico Giacomo; Seele, Peter",2023.0,https://doi.org/10.1007/s10676-023-09724-8,journalArticle,"The recent proliferation of AI scandals led private and public organisations to implement new ethics guidelines, introduce AI ethics boards, and list ethical principles. Nevertheless, some of these efforts remained a façade not backed by any substantive action. Such behaviour made the public question the legitimacy of the AI industry and prompted scholars to accuse the sector of ethicswashing, machinewashing, and ethics trivialisation—criticisms that spilt over to institutional AI ethics boards. To counter this widespread issue, contributions in the literature have proposed fixes that do not consider its systemic character and are based on a top-down, expert-centric governance. To fill this gap, we propose to make use of qualified informed lotteries: a two-step model that transposes the documented benefits of the ancient practice of sortition into the selection of AI ethics boards’ members and combines them with the advantages of a stakeholder-driven, participative, and deliberative bottom-up process typical of Citizens’ Assemblies. The model permits increasing the public’s legitimacy and participation in the decision-making process and its deliverables, curbing the industry’s over-influence and lobbying, and diminishing the instrumentalisation of ethics boards. We suggest that this sortition-based approach may provide a sound base for both public and private organisations in smart societies for constructing a decentralised, bottom-up, participative digital democracy.",AI ethics; Ethics boards; Ethicswashing; Legitimacy; Sortition,,1388-1957,2023-10,2023-11-06 01:29:49,2023-11-06 01:29:49,,,,M2D93MBP,0.1057692307692307,0.7142857142857143,0.15,0.1315327019615286
82,82,Responsible AI Pattern Catalogue: A Collection of Best Practices for AI Governance and Engineering,ACM Comput. Surv.,,,10.1145/3626234,"Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon; Zowghi, Didar; Jacquet, Aurelie",2023.0,https://doi.org/10.1145/3626234,journalArticle,"Responsible AI is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of AI. Recently, a number of AI ethics principles frameworks have been published. However, without further guidance on best practices, practitioners are left with nothing much beyond truisms. Also, significant efforts have been placed at algorithm-level rather than system-level, mainly focusing on a subset of mathematics-amenable ethical principles, such as fairness. Nevertheless, ethical issues can arise at any step of the development lifecycle, cutting across many AI and non-AI components of systems beyond AI algorithms and models. To operationalize responsible AI from a system perspective, in this paper, we present a Responsible AI Pattern Catalogue based on the results of a Multivocal Literature Review (MLR). Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The Responsible AI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and responsible-AI-by-design product patterns. These patterns provide systematic and actionable guidance for stakeholders to implement responsible AI.",Responsible AI; AI governance; trustworthy AI; software architecture; software engineering; AI engineering; best practice; ethical AI; MLOps; pattern,,0360-0300,2023-10,2023-11-06 01:30:03,2023-11-06 01:30:03,,,,BUD6YJRS,0.0959595959595959,0.4444444444444444,0.1428571428571428,0.1304231337767923
83,83,Ethical Implications of Digital Infrastructures for Pluralistic Perspectives,Ethics and Inf. Technol.,23.0,3,10.1007/s10676-021-09582-2,"Israel, Maria Joseph; Amer, Ahmed",2021.0,https://doi.org/10.1007/s10676-021-09582-2,journalArticle,"It is important to design digital infrastructure that can better accommodate multicultural and pluralistic views from its foundations. It is insufficient to look at only the responses and influences of culture on technology without considering how the technology can be adapted in anticipation of, and to support, pluralistic multicultural perspectives in its original design. This goes beyond the simple act of supporting multiple languages and interfaces, but should include the ability of digital and data infrastructure to capture and accommodate pluralistic views, supporting multiple perspectives in the representation and processing of the data itself. In this work, we look at how rethinking the representation of data can allow us to more directly tackle domains that are typically hampered due to intercultural differences, and their inevitable losses in translation, particularly losses of valuable information like context and intention. When we refer to a loss of context and intention, we are referring to the loss of semantic information when practices such as referencing and citation are hard-coded to a particular set of cultural norms. We show that it is possible to expand the way in which we track referential data to capture richer semantic information regarding the contexts and intentions of the creators of this data, and thereby better serve the varied needs of those who consume, study, and refer to such data. We demonstrate this concept through a prototype system for a multicultural digital infrastructure, which we have&nbsp;named MultiVerse, and discuss its ethical implications from the perspectives of ‘multistability’, Intercultural Information Ethics framework, and poststructuralism.",Philosophy of technology; Data storage technology; Ethical pluralism; Intercultural digital ethics; Provenance; Technology and ethics,,1388-1957,2021-09,2023-11-06 01:30:04,2023-11-06 01:30:04,399–417,,,BLS7KP3T,0.0669291338582677,0.6666666666666666,0.625,0.1302668620543738
84,84,German AI Start-Ups and “AI Ethics”: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533156,"Sloane, Mona; Zakrzewski, Janina",2022.0,https://doi.org/10.1145/3531146.3533156,conferencePaper,"The current AI ethics discourse focuses on developing computational interpretations of ethical concerns, normative frameworks, and concepts for socio-technical innovation. There is less emphasis on understanding how AI practitioners themselves understand ethics and socially organize to operationalize ethical concerns. This is particularly true for AI start-ups, despite their significance as a conduit for the cultural production of innovation and progress, especially in the US and European context. This gap in empirical research intensifies the risk of a disconnect between scholarly research, innovation and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to rapidly assess and implement socio-technical innovation focused on fairness, accountability, and transparency. In this paper, we address this need. Building on social practice theory, we propose a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of “ethical AI” to define appropriate strategies for effectively implementing socio-technical innovations. We argue that this approach is needed because socio-technical innovation “sticks” better if it sustains the cultural meaning of socially shared (ethical) AI practices, rather than breaking them. By doing so, it creates pathways for technical and socio-technical innovations to be integrated into already existing routines. Against that backdrop, our contributions are threefold: (1) we introduce a practice-based approach for understanding “ethical AI”; (2) we present empirical findings from our study on the operationalization of “ethics” in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and (3) based on our empirical findings, suggest that “ethical AI” practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. We conclude with critical reflections and practical implications of our work, as well as recommendations for future research.",accountability; AI ethics; fairness; innovation; organizations; regulation; social practice; socio-cultural history; start-ups; transparency,978-1-4503-9352-2,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,935–947,FAccT '22,Association for Computing Machinery,VZ5HVRCU,0.1182108626198083,0.2307692307692307,0.2352941176470588,0.1300288139949144
85,85,"Integrating Digital Citizenship into a Primary School Course “Ethics and the Rule of Law”: Necessity, Strategies and a Pilot Study","Blended Learning : Lessons Learned and Ways Forward : 16th International Conference on Blended Learning, ICBL 2023, Hong Kong, China, July 17-20, 2023, Proceedings",,,10.1007/978-3-031-35731-2_7,"Li, Yumeng; Deng, Shaoshan; Wu, Xiaomin; Zhao, Bin; Xie, Yufei; Luo, Xianfei; Zheng, Yunxiang",2023.0,https://doi.org/10.1007/978-3-031-35731-2_7,conferencePaper,"Information technology has made modern life accessible and abundant, but it has also led to cyberbullying, online fraud, and other technology abuses. Enhancing digital citizenship can help solve these issues. This paper aims to figure out how to integrate digital citizenship into a primary school course named “Ethics and the Rule of Law” so that students can become good digital citizens in the new age. This paper first examined the need for this integration. Secondly, several strategies were proposed, including: content strategy (being closely linked with teaching materials), method strategy (innovating teaching methods), real-world strategy (connecting to real-world scenarios), and evaluation strategy (improving the evaluation criteria). Finally, a strategy-based pilot study was conducted. The results demonstrated that integrating digital citizenship into a moral education course could help students develop good digital ethics, behaviors, and habits.",Digital Citizenship; Ethics and the Rule of Law; Primary School Course; Teaching Strategies,978-3-031-35730-5,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,59–70,,Springer-Verlag,3NPYYVAX,0.1037037037037037,0.3076923076923077,0.2,0.1299106864030978
86,86,Ethically Compliant Planning within Moral Communities,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462522,"Nashed, Samer; Svegliato, Justin; Zilberstein, Shlomo",2021.0,https://doi.org/10.1145/3461702.3462522,conferencePaper,"Ethically compliant autonomous systems (ECAS) are the state-of-the-art for solving sequential decision-making problems under uncertainty while respecting constraints that encode ethical considerations. This paper defines a novel concept in the context of ECAS that is from moral philosophy, the moral community, which leads to a nuanced taxonomy of explicit ethical agents. We then propose new ethical frameworks that extend the applicability of ECAS to domains where a moral community is required. Next, we provide a formal analysis of the proposed ethical frameworks and conduct experiments that illustrate their differences. Finally, we discuss the implications of explicit moral communities that could shape research on standards and guidelines for ethical agents in order to better understand and predict common errors in their design and communicate their capabilities.",ethical artificial intelligence; markov decision process; moral communities; moral decision making,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,188–198,AIES '21,Association for Computing Machinery,TL6JN7WY,0.12,0.2727272727272727,0.0,0.1283891547049441
87,87,Invigorating Ubuntu Ethics in AI for Healthcare: Enabling Equitable Care,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594024,"Amugongo, Lameck Mbangula; Bidwell, Nicola J.; Corrigan, Caitlin C.",2023.0,https://doi.org/10.1145/3593013.3594024,conferencePaper,"The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of “I am because we are” in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly.",algorithmic fairness; algorithmic trust; equitable care; relational ethics,9798400701924,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,583–592,FAccT '23,Association for Computing Machinery,9IZB6NIH,0.1093117408906882,0.375,0.3,0.1277595231171852
88,88,Artificial Intelligence Ethics Has a Black Box Problem,AI Soc.,38.0,4,10.1007/s00146-021-01380-0,"Bélisle-Pipon, Jean-Christophe; Monteferrante, Erica; Roy, Marie-Christine; Couture, Vincent",2022.0,https://doi.org/10.1007/s00146-021-01380-0,journalArticle,"It has become a truism that the ethics of artificial intelligence (AI) is necessary and must help guide technological developments. Numerous ethical guidelines have emerged from academia, industry, government and civil society in recent years. While they provide a basis for discussion on appropriate regulation of AI, it is not always clear how these ethical guidelines were developed, and by whom. Using content analysis, we surveyed a sample of the major documents (n = 47) and analyzed the accessible information regarding their methodology and stakeholder engagement. Surprisingly, only 38% report some form of stakeholder engagement (with 9% involving citizens) and most do not report their methodology for developing normative insights (15%). Our results show that documents with stakeholder engagement develop more comprehensive ethical guidance with greater applicability, and that the private sector is least likely to engage stakeholders. We argue that the current trend for enunciating AI ethical guidance not only poses widely discussed challenges of applicability in practice, but also of transparent development (as it rather behaves as a black box) and of active engagement of diversified, independent and trustworthy stakeholders. While most of these documents consider people and the common good as central to their telos, engagement with the general public is significantly lacking. As AI ethics moves from the initial race for enunciating general principles to more sustainable, inclusive and practical guidance, stakeholder engagement and citizen involvement will need to be embedded into the framing of ethical and societal expectations towards this technology.",Artificial intelligence; Responsible AI; AI ethics principles; Citizen involvement; Ethical guidance; Stakeholder engagement,,0951-5666,2022-01,2023-11-06 01:29:59,2023-11-06 01:29:59,1507–1522,,,ZCB5ESMZ,0.0934959349593495,0.5384615384615384,0.25,0.1276855933937754
89,89,Operationalising AI Ethics: How Are Companies Bridging the Gap between Practice and Principles? An Exploratory Study,AI Soc.,37.0,4,10.1007/s00146-021-01267-0,"Ibáñez, Javier Camacho; Olmeda, Mónica Villas",2022.0,https://doi.org/10.1007/s00146-021-01267-0,journalArticle,"Despite the increase in the research field of ethics in artificial intelligence, most efforts have focused on the debate about principles and guidelines for responsible AI, but not enough attention has been given to the “how” of applied ethics. This paper aims to advance the research exploring the gap between practice and principles in AI ethics by identifying how companies are applying those guidelines and principles in practice. Through a qualitative methodology based on 22 semi-structured interviews and two focus groups, the goal of the current study is to understand how companies approach ethical issues related to AI systems. A structured analysis of the transcripts brought out many actual practices and findings, which are presented around the following main research topics: ethics and principles, privacy, explainability, and fairness. The interviewees also raised issues of accountability and governance. Finally, some recommendations are suggested such as developing specific sector regulations, fostering a data-driven organisational culture, considering the algorithm’s complete life cycle, developing and using a specific code of ethics, and providing specific training on ethical issues. Despite some obvious limitations, such as the type and number of companies interviewed, this work identifies real examples and direct priorities to advance the research exploring the gap between practice and principles in AI ethics, with a specific focus on Spanish companies.",Artificial intelligence; Ethics; Explainability; Fairness; Principles; Privacy,,0951-5666,2022-12,2023-11-06 01:29:49,2023-11-06 01:29:49,1663–1687,,,DMXLIV59,0.1064814814814814,0.4285714285714285,0.1875,0.127029793398841
90,90,Responsible AI for Trusted AI-Powered Enterprise Platforms,Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,,,10.1145/3539597.3575784,"Hoi, Steven C.H.",2023.0,https://doi.org/10.1145/3539597.3575784,conferencePaper,"With the rapidly growing AI market opportunities and the accelerated adoption of AI technologies for a wide range of real-world applications, responsible AI has attracted increasing attention in both academia and industries. In this talk, I will focus on the topics of responsible AI in the industry settings towards building trusted AI-powered enterprise platforms. I will share our efforts and experience of responsible AI for enterprise at Salesforce, from defining the principles to putting them into practice to build trust in AI. Finally, I will also address some emerging challenges and open issues of recent generative AI advances and call for actions of joint responsible AI efforts from academia, industries and governments.",sustainability; fairness; accountability; transparency; ai ethics; enterprise platforms; explainability; responsible ai; robustness; trusted ai,978-1-4503-9407-9,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,1277–1278,WSDM '23,Association for Computing Machinery,QHS2ZRQI,0.0803571428571428,0.3571428571428571,0.1428571428571428,0.1265660809778457
91,91,You Can't Sit With Us: Exclusionary Pedagogy in AI Ethics Education,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445914,"Raji, Inioluwa Deborah; Scheuerman, Morgan Klaus; Amironesei, Razvan",2021.0,https://doi.org/10.1145/3442188.3445914,conferencePaper,"Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)—and its implications for AI—has led to the current ""ethics crisis"". However, we claim that the current AI ethics education space relies on a form of ""exclusionary pedagogy,"" where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as ""ethical unicorns"" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.",,978-1-4503-8309-7,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,515–525,FAccT '21,Association for Computing Machinery,2TC6F9VQ,0.1194029850746268,0.0,0.2727272727272727,0.1264729969622574
92,92,Towards a Feminist Metaethics of AI,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534197,"Siapka, Anastasia",2022.0,https://doi.org/10.1145/3514094.3534197,conferencePaper,"The proliferation of Artificial Intelligence (AI) has sparked an overwhelming number of AI ethics guidelines, boards and codes of conduct. These outputs primarily analyse competing theories, principles and values for AI development and deployment. However, as a series of recent problematic incidents about AI ethics/ethicists demonstrate, this orientation is insufficient. Before proceeding to evaluate other professions, AI ethicists should critically evaluate their own; yet, such an evaluation should be more explicitly and systematically undertaken in the literature. I argue that these insufficiencies could be mitigated by developing a research agenda for a feminist metaethics of AI. Contrary to traditional metaethics, which reflects on the nature of morality and moral judgements in a non-normative way, feminist metaethics expands its scope to ask not only what ethics is but also what our engagement with it should be like. Applying this perspective to the context of AI, I suggest that a feminist metaethics of AI would examine: (i) the continuity between theory and action in AI ethics; (ii) the real-life effects of AI ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the effects of AI on power relations through methods that pay attention to context, emotions and narrative.",AI ethics; artificial intelligence; ethics-washing; feminist philosophy; metaethics,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,665–674,AIES '22,Association for Computing Machinery,YY2382AC,0.1094527363184079,0.375,0.1666666666666666,0.1262073777454192
93,93,Automated Kantian Ethics: A Faithful Implementation,"KI 2022: Advances in Artificial Intelligence: 45th German Conference on AI, Trier, Germany, September 19–23, 2022, Proceedings",,,10.1007/978-3-031-15791-2_16,"Singh, Lavanya",2022.0,https://doi.org/10.1007/978-3-031-15791-2_16,conferencePaper,"As we grant artificial intelligence increasing power and independence in contexts like healthcare, policing, and driving, AI faces moral dilemmas but lacks the tools to solve them. Warnings from regulators, philosophers, and computer scientists about the dangers of unethical AI have spurred interest in automated ethics-i.e., the development of machines that can perform ethical reasoning. However, prior work in automated ethics rarely engages with philosophical literature. Philosophers have spent centuries debating moral dilemmas so automated ethics will be most nuanced, consistent, and reliable when it draws on philosophical literature. In this paper, I present an implementation of automated Kantian ethics that is faithful to the Kantian philosophical tradition. I formalize Kant’s categorical imperative in an embedding of Dyadic Deontic Logic in HOL, implement this formalization in the Isabelle/HOL theorem prover, and develop a testing framework to evaluate how well my implementation coheres with expected properties of Kantian ethic. My system is an early step towards philosophically mature ethical AI agents and it can make nuanced judgements in complex ethical dilemmas because it is grounded in philosophical literature. Because I use an interactive theorem prover, my system’s judgements are explainable.",Ai ethics; Automated ethics; Isabelle; Kant,978-3-031-15790-5,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,187–208,,Springer-Verlag,JHTCXSLA,0.0952380952380952,0.8333333333333334,0.3333333333333333,0.1261619517433471
94,94,Value Sensitive Design to Achieve the UN SDGs with AI: A Case of Elderly Care Robots,Minds Mach.,31.0,3,10.1007/s11023-021-09561-y,"Umbrello, Steven; Capasso, Marianna; Balistreri, Maurizio; Pirni, Alberto; Merenda, Federica",2021.0,https://doi.org/10.1007/s11023-021-09561-y,journalArticle,"Healthcare is becoming increasingly automated with the development and deployment of care robots. There are many benefits to care robots but they also pose many challenging ethical issues. This paper takes care robots for the elderly as the subject of analysis, building on previous literature in the domain of the ethics and design of care robots. Using the value sensitive design (VSD) approach to technology design, this paper extends its application to care robots by integrating the values of care, values that are specific to AI, and higher-scale values such as the United Nations Sustainable Development Goals (SDGs). The ethical issues specific to care robots for the elderly are discussed at length alongside examples of specific design requirements that work to ameliorate these ethical concerns.",AI ethics; Applied ethics; Autonomous systems; Care robots; Value sensitive design,,0924-6495,2021-09,2023-11-06 01:30:01,2023-11-06 01:30:01,395–419,,,G585XSVA,0.096,0.4545454545454545,0.0625,0.1233526000748223
95,95,A ‘biased’ Emerging Governance Regime for Artificial Intelligence? How AI Ethics Get Skewed Moving from Principles to Practices,Telecommun. Policy,47.0,5,10.1016/j.telpol.2022.102479,"Palladino, Nicola",2023.0,https://doi.org/10.1016/j.telpol.2022.102479,journalArticle,"Over the past few years, the awareness that the full potential of artificial intelligence (AI) could be attained only through the establishment of a trustworthy and human-centric framework has expanded, thereby prompting demand for regulatory frameworks as well as engendering a flourish of initiatives that set ethical codes and good governance principles for AI development. This study investigates whether the convergence of many of the proposed ethical frameworks around a narrow set of values and principles may be interpreted as a case of transnational norms emergence, a pre-condition for a more structured global regulatory framework or policy regime. Moreover, it explores how this emerging normative framework is reframed in its concrete implementation. Findings suggest that AI governance poses a complex dilemma: while its hybrid governance ecosystem entrusts developers and deployers, mainly from the private sector and technical communities, with the task of translating principles into workable tools, their institutional logics substantially narrow the scope and purposes of the ethical approach.",AI ethical Tools; AI Ethics; Artificial intelligence; Internet governance,,0308-5961,2023-06,2023-11-06 01:29:49,2023-11-06 01:29:49,,,,32HNCWZU,0.075,0.7777777777777778,0.1666666666666666,0.1229759485094851
96,96,Reclaiming the Digital Commons: A Public Data Trust for Training Data,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604658,"Chan, Alan; Bradley, Herbie; Rajkumar, Nitarshan",2023.0,https://doi.org/10.1145/3600211.3604658,conferencePaper,"Democratization of AI means not only that people can freely use AI, but also that people can collectively decide how AI is to be used. In particular, collective decision-making power is required to redress the negative externalities from the development of increasingly advanced AI systems, including degradation of the digital commons and unemployment from automation. The rapid pace of AI development and deployment currently leaves little room for this power. Monopolized in the hands of private corporations, the development of the most capable foundation models has proceeded largely without public input. There is currently no implemented mechanism for ensuring that the economic value generated by such models is redistributed to account for their negative externalities. The citizens that have generated the data necessary to train models do not have input on how their data are to be used. In this work, we propose that a public data trust assert control over training data for foundation models. In particular, this trust should scrape the internet as a digital commons, to license to commercial model developers for a percentage cut of revenues from deployment. First, we argue in detail for the existence of such a trust. We also discuss feasibility and potential risks. Second, we detail a number of ways for a data trust to incentivize model developers to use training data only from the trust. We propose a mix of verification mechanisms, potential regulatory action, and positive incentives. We conclude by highlighting other potential benefits of our proposed data trust and connecting our work to ongoing efforts in data and compute governance.",data rights; data trust; digital commons; training data,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,855–868,AIES '23,Association for Computing Machinery,XX4AT3N9,0.0881226053639846,0.75,0.4545454545454545,0.1226614817909176
97,97,"Situated Accountability: Ethical Principles, Certification Standards, and Explanation Methods in Applied AI","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462564,"Henriksen, Anne; Enni, Simon; Bechmann, Anja",2021.0,https://doi.org/10.1145/3461702.3462564,conferencePaper,"Artificial intelligence (AI) has the potential to benefit humans and society by its employment in important sectors. However, the risks of negative consequences have underscored the importance of accountability for AI systems, their outcomes, and the users of such systems. In recent years, various accountability mechanisms have been put forward in pursuit of the responsible design, development, and use of AI. In this article, we provide an in-depth study of three such mechanisms, as we analyze Scandinavian AI developers' encounter with (1) ethical principles, (2) certification standards, and (3) explanation methods. By doing so, we contribute to closing a gap in the literature between discussions of accountability on the research and policy level, and accountability as a responsibility put on the shoulders of developers in practice. Our study illustrates important flaws in the current enactment of accountability as an ethical and social value which, if left unchecked, risks undermining the pursuit of responsible AI. By bringing attention to these flaws, the article signals where further work is needed in order to build effective accountability systems for AI.",accountability; AI; AI ethics; algorithmic systems; case study; certification; ethnography; explainable AI; machine learning; responsible AI,978-1-4503-8473-5,,2021,2023-11-06 01:29:50,2023-11-06 01:29:50,574–585,AIES '21,Association for Computing Machinery,C2UST5VA,0.0734463276836158,0.375,0.3333333333333333,0.1226214847965466
98,98,Artificial Intelligence Diffusion in Public Administration,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539529,"Madan, Rohit",2022.0,https://doi.org/10.1145/3514094.3539529,conferencePaper,"The use of Artificial Intelligence (AI) in public administration has immense benefits but also embodies ethical dilemmas of fairness, transparency, privacy, and human rights. Several frameworks have been developed by governments and technology companies to guide the ethical development of AI solutions. However, within public administration implementations, there is a lack of clarity on how decisions on these dilemmas are made and what is the effect of such decisions on public values. This research aims to undertake a mixed-method study exploring the mechanisms and causal links between AI tensions and public value creation.",AI ethics; algorithmic governance; machine learning; public value,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,903,AIES '22,Association for Computing Machinery,72GLWRK9,0.1075268817204301,0.375,0.0,0.1223332476545431
99,99,Ethical Guidelines and Principles in the Context of Artificial Intelligence,Proceedings of the XVII Brazilian Symposium on Information Systems,,,10.1145/3466933.3466969,"Siqueira de Cerqueira, José Antonio; Acco Tives, Heloise; Dias Canedo, Edna",2021.0,https://doi.org/10.1145/3466933.3466969,conferencePaper,"The interest in Artificial Intelligence (AI) based systems has been gaining momentum at a fast pace, both for software development teams and for society as a whole. This work aims to identify the guidelines and ethical principles for systems based on Artificial Intelligence. Design Science Research methodology was adopted in order to understand the various guidelines and principles existing in the literature. From the current landscape, a body of knowledge in the field of AI ethics is presented, with the purpose of supporting developers and Product Owners in identifying the guidelines and ethical principles in the literature so that they can be used during the software development process. Thus, this work will contribute to the various stakeholders in the development of ethical systems in the context of AI, such as: policy makers, ethicists, users, organizations, data scientists, development teams, among others.",,978-1-4503-8491-9,,2021,2023-11-06 01:30:02,2023-11-06 01:30:02,,SBSI '21,Association for Computing Machinery,JHS4C3A3,0.1063829787234042,0.0,0.3,0.1209042553191489
100,100,Honor Ethics: The Challenge of Globalizing Value Alignment in AI,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594026,"Wu, Stephen Tze-Inn; Demetriou, Daniel; Husain, Rudwan Ali",2023.0,https://doi.org/10.1145/3593013.3594026,conferencePaper,"Some researchers have recognized that privileged communities dominate the discourse on AI Ethics, and other voices need to be heard. As such, we identify the current ethics milieu as arising from WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and aim to expand the discussion to non-WEIRD global communities, who are also stakeholders in global sociotechnical systems. We argue that accounting for honor, along with its values and related concepts, would better approximate a global ethical perspective. This complex concept already underlies some of the WEIRD discourse on AI ethics, but certain cultural forms of honor also bring overlooked issues and perspectives to light. We first describe honor according to recent empirical and philosophical scholarship. We then review “consensus” principles for AI ethics framed from an honor-based perspective, grounding comparisons and contrasts via example settings such as content moderation, job hiring, and genomics databases. A better appreciation of the marginalized concept of honor could, we hope, lead to more productive AI value alignment discussions, and to AI systems that better reflect the needs and values of users around the globe.",AI Ethics; artificial intelligence; cultures of honor; ethics alignment; honor cultures; universal human rights,9798400701924,,2023,2023-11-06 01:29:59,2023-11-06 01:29:59,593–602,FAccT '23,Association for Computing Machinery,68KUGHIA,0.0893854748603352,0.3571428571428571,0.3,0.1204872740268567
101,101,Bridging the Civilian-Military Divide in Responsible AI Principles and Practices,Ethics and Inf. Technol.,25.0,2,10.1007/s10676-023-09693-y,"Azafrani, Rachel; Gupta, Abhishek",2023.0,https://doi.org/10.1007/s10676-023-09693-y,journalArticle,"Advances in AI research have brought increasingly sophisticated capabilities to AI systems and heightened the societal consequences of their use. Researchers and industry professionals have responded by contemplating responsible principles and practices for AI system design. At the same time, defense institutions are contemplating ethical guidelines and requirements for the development and use of AI for warfare. However, varying ethical and procedural approaches to technological development, research emphasis on offensive uses of AI, and lack of appropriate venues for multistakeholder dialogue have led to differing operationalization of responsible AI principles and practices among civilian and defense entities. We argue that the disconnect between civilian and defense responsible development and use practices leads to underutilization of responsible AI research and hinders the implementation of responsible AI principles in both communities. We propose a research roadmap and recommendations for dialogue to increase exchange of responsible AI development and use practices for AI systems between civilian and defense communities. We argue that generating more opportunities for exchange will stimulate global progress in the implementation of responsible AI principles.",AI ethics; Responsible AI; Machine learning; Artificial intelligence (AI); Military; Military applications,,1388-1957,2023-04,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,GZNJWG6X,0.0971428571428571,0.4166666666666667,0.1,0.1203388402980239
102,102,Operationalising AI Ethics: Conducting Socio-Technical Assessment,Human-Centered Artificial Intelligence: Advanced Lectures,,,10.1007/978-3-031-24349-3_16,"Methnani, Leila; Brännström, Mattias; Theodorou, Andreas",2023.0,https://doi.org/10.1007/978-3-031-24349-3_16,conferencePaper,"Several high profile incidents that involve Artificial Intelligence (AI) have captured public attention and increased demand for regulation. Low public trust and attitudes towards AI reinforce the need for concrete policy around its development and use. However, current guidelines and standards rolled out by institutions globally are considered by many as high-level and open to interpretation, making them difficult to put into practice. This paper presents ongoing research in the field of Responsible AI and explores numerous methods of operationalising AI ethics. If AI is to be effectively regulated, it must not be considered as a technology alone—AI is embedded in the fabric of our societies and should thus be treated as a socio-technical system, requiring multi-stakeholder involvement and employment of continuous value-based methods of assessment. When putting guidelines and standards into practice, context is of critical importance. The methods and frameworks presented in this paper emphasise this need and pave the way towards operational AI ethics.",AI ethics; Responsible AI; Socio-technical assessment,978-3-031-24348-6,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,304–321,,Springer-Verlag,8RFHG845,0.0700636942675159,0.6666666666666666,0.5,0.1202878236180778
103,103,Culture of Ethics in Adopting Learning Analytics,"Augmented Intelligence and Intelligent Tutoring Systems: 19th International Conference, ITS 2023, Corfu, Greece, June 2–5, 2023, Proceedings",,,10.1007/978-3-031-32883-1_52,"Tzimas, Dimitrios; Demetriadis, Stavros",2023.0,https://doi.org/10.1007/978-3-031-32883-1_52,conferencePaper,"Learning analytics (LA) collects, analyzes, and reports large amounts of data about learners in order to improve learning in intelligent tutoring systems. Because LA ethics is an interdisciplinary field that addresses moral, legal, and social issues, institutions are responsible for implementing frameworks that address these concerns. Many ethical concerns apply to educational data sets of any size. However, in this study, we concentrate on big data, which increases the scale and granularity of the data collected. We present a synthesis on a growing subject of interest based on ethics regarding the capture of data by LA. This research aims twofold: (a) to extend the review of the scientific literature on LA ethics issues and (b) to identify emerging trends and answer open-field questions discussing three case studies. The following are the research questions for this study: what does LA ethics mean for educational stakeholders, and what are students’ and teachers’ perspectives on ethics as a factor in adopting LA? We developed a multi-stage design process that included a literature review, empirical research, and community involvement. The literature review identified 68 articles after searching journals and conferences. The selected articles were thoroughly examined using qualitative content analysis. The findings point to a lack of evidence-based guidelines on data ethics and the need to develop codes of practice to evaluate LA ethics policies. Finally, this work applies an ethical checklist to three case studies as an instructional design model for scholars, policymakers, and instructional designers, so partners can use LA responsibly to improve learning and teaching efficacy.",Ethics; Co-design; Data privacy and management; Distance education; Learning analytics adoption; Stakeholders’ perspectives,978-3-031-32882-4,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,591–603,,Springer-Verlag,N9QVXBSU,0.1019607843137254,0.3076923076923077,0.2857142857142857,0.1201221177053127
104,104,"Models for Classifying AI Systems: The Switch, the Ladder, and the Matrix","Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533162,"Mökander, Jakob; Sheth, Margi; Watson, David; Floridi, Luciano",2022.0,https://doi.org/10.1145/3531146.3533162,conferencePaper,"Organisations that design and deploy systems based on artificial intelligence (AI) increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. A major obstacle to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the practical purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.",Ethics; Artificial intelligence; Governance; Material scope,978-1-4503-9352-2,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,1016,FAccT '22,Association for Computing Machinery,5HSCI7L9,0.1141732283464567,0.3333333333333333,0.0833333333333333,0.1199457331347095
105,105,"The Switch, the Ladder, and the Matrix: Models for Classifying AI Systems",Minds Mach.,33.0,1,10.1007/s11023-022-09620-y,"Mökander, Jakob; Sheth, Margi; Watson, David S.; Floridi, Luciano",2023.0,https://doi.org/10.1007/s11023-022-09620-y,journalArticle,"Organisations that design and deploy artificial intelligence (AI) systems increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. One major obstacle organisations face when attempting to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the purpose of implementing AI governance in practice. We find that attempts to classify AI systems proposed in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, input data, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organisations that design, deploy, or regulate AI systems with the vocabulary needed to demarcate the material scope of their AI governance frameworks.",Ethics; Artificial intelligence; Governance; Material scope; AI systems; Classification; Mental models,,0924-6495,2023-01,2023-11-06 01:30:01,2023-11-06 01:30:01,221–248,,,JMXN2K4T,0.1124031007751938,0.2727272727272727,0.0833333333333333,0.1197766616642384
106,106,Ethics of Digital Contact Tracing and COVID-19: Who is (Not) Free to Go?,Ethics and Inf. Technol.,23.0,Suppl 1,10.1007/s10676-020-09544-0,"Klenk, Michael; Duijf, Hein",2021.0,https://doi.org/10.1007/s10676-020-09544-0,journalArticle,"Digital tracing technologies are heralded as an effective way of containing SARS-CoV-2 faster than it is spreading, thereby allowing the possibility of easing draconic measures of population-wide quarantine. But existing technological proposals risk addressing the wrong problem. The proper objective is not solely to maximise the ratio of people freed from quarantine but to also ensure that the composition of the freed group is fair. We identify several factors that pose a risk for fair group composition along with an analysis of general lessons for a philosophy of technology. Policymakers, epidemiologists, and developers can use these risk factors to benchmark proposal technologies, curb the pandemic, and keep public trust.",Digital ethics; Fairness; Active responsibility; COVID-19; Digital contact tracing,,1388-1957,2021-11,2023-11-06 01:30:01,2023-11-06 01:30:01,69–77,,,N32QR5LL,0.0275229357798165,0.7777777777777778,0.3076923076923077,0.1196233775177333
107,107,The Role of Explainable AI in the Research Field of AI Ethics,ACM Trans. Interact. Intell. Syst.,,,10.1145/3599974,"Vainio-Pekka, Heidi; Agbese, Mamia Ori-otse; Jantunen, Marianna; Vakkuri, Ville; Mikkonen, Tommi; Rousi, Rebekah; Abrahamsson, Pekka",2023.0,https://doi.org/10.1145/3599974,journalArticle,"Ethics of Artificial Intelligence (AI) is a growing research field that has emerged in response to the challenges related to AI. Transparency poses a key challenge for implementing AI ethics in practice. One solution to transparency issues is AI systems that can explain their decisions. Explainable AI (XAI) refers to AI systems that are interpretable or understandable to humans. The research fields of AI ethics and XAI lack a common framework and conceptualization. There is no clarity of the field’s depth and versatility. A systematic approach to understanding the corpus is needed. A systematic review offers an opportunity to detect research gaps and focus points. This paper presents the results of a systematic mapping study (SMS) of the research field of the Ethics of AI. The focus is on understanding the role of XAI and how the topic has been studied empirically. An SMS is a tool for performing a repeatable and continuable literature search. This paper contributes to the research field with a Systematic Map that visualizes what, how, when, and why XAI has been studied empirically in the field of AI ethics. The mapping reveals research gaps in the area. Empirical contributions are drawn from the analysis. The contributions are reflected on in regards to theoretical and practical implications. As the scope of the SMS is a broader research area of AI ethics the collected dataset opens possibilities to continue the mapping process in other directions.",AI Ethics; Artificial Intelligence; Explainable AI; Systematic Mapping Study,,2160-6455,2023-06,2023-11-06 01:29:48,2023-11-06 01:29:48,,,,RTDWKEEI,0.0924369747899159,0.4444444444444444,0.3333333333333333,0.1181806055755635
108,108,Civil War Twin: Exploring Ethical Challenges in Designing an Educational Face Recognition Application,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534141,"Kusuma, Manisha; Mohanty, Vikram; Wang, Marx; Luther, Kurt",2022.0,https://doi.org/10.1145/3514094.3534141,conferencePaper,"Facial recognition systems pose numerous ethical challenges around privacy, racial and gender bias, and accuracy, yet little guidance is available for designers and developers. We explore solutions to these challenges in a three-phase design process to create Civil War Twin (CWT), an educational web-based application where users can discover their lookalikes from the American Civil War era (1861–65) while learning more about facial recognition and history. Through this design process, we operationalize a framework for AI literacy, consult with scholars of history, gender, and race, and evaluate CWT in feedback sessions with diverse prospective users. We iteratively formulate design goals to incorporate transparency, inclusivity, speculative design, and empathy into our application. We found that users' perceived learning about the strengths and limitations of facial recognition and Civil War history improved after using CWT, and that our design successfully met users' ethical standards. We also discuss how our ethical design process can be applied to future facial recognition applications.",ethical design; ai literacy; digital history; education; facial recognition; human-ai interaction,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,369–384,AIES '22,Association for Computing Machinery,7Y58BAZ7,0.0696202531645569,0.5454545454545454,0.2307692307692307,0.1174975331417049
109,109,Ethics and AI Issues: Old Container with New Wine?,Information Systems Frontiers,25.0,1,10.1007/s10796-022-10305-1,"Niederman, Fred; Baker, Elizabeth White",2022.0,https://doi.org/10.1007/s10796-022-10305-1,journalArticle,"This paper reflects on what differentiates AI ethics issues from concerns raised by all IS applications. AI ethics issues can be viewed in three distinct categories. One can view AI as another IS application like any other. We examine this category of AI applications focusing primarily on Mason’s (MIS Quarterly, 10, 5–12, 1986) PAPA framework as a way to position AI ethics within the IS domain. One can also view AI as adding a generative capacity producing outputs that cannot be pre-determined from inputs and code. We examine this by adding “inference” to the informational pyramid and exploring its implications. AI can also be viewed as a basis for reexamining questions of the nature of mental phenomena such as reasoning and imagination. At this time, AI-based systems seem far from replicating or replacing human capabilities. However, if/when such abilities emerge as computing machinery continues growing in capacity and capability, it will be helpful to have anticipated arising ethical issues and developed plans for avoiding, detecting, and resolving them to the extent possible.",Ethics; Artificial intelligence; AI; PAPA framework; Philosophy of science,,1387-3326,2022-06,2023-11-06 01:30:00,2023-11-06 01:30:00,9–28,,,M7Q2SRCC,0.0930232558139534,0.3333333333333333,0.3333333333333333,0.1174682513942677
110,110,"AI Literacy: Finding Common Threads between Education, Design, Policy, and Explainability",Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems,,,10.1145/3544549.3573808,"Long, Duri; Roberts, Jessica; Magerko, Brian; Holstein, Kenneth; DiPaola, Daniella; Martin, Fred",2023.0,https://doi.org/10.1145/3544549.3573808,conferencePaper,"Fostering public AI literacy has been a growing area of interest at CHI for several years, and a substantial community is forming around issues such as teaching children how to build and program AI systems, designing learning experiences to broaden public understanding of AI, developing explainable AI systems, understanding how novices make sense of AI, and exploring the relationship between public policy, ethics, and AI literacy. Previous workshops related to AI literacy have been held at other conferences (e.g., SIGCSE, AAAI) that have been mostly focused on bringing together researchers and educators interested in AI education in K-12 classroom environments, an important subfield of this area. Our workshop seeks to cast a wider net that encompasses both HCI research related to introducing AI in K-12 education and also HCI research that is concerned with issues of AI literacy more broadly, including adult education, interactions with AI in the workplace, understanding how users make sense of and learn about AI systems, research on developing explainable AI (XAI) for non-expert users, and public policy issues related to AI literacy.",AI ethics; AI literacy; AI education; AI4K12; explainable AI; public policy,978-1-4503-9422-2,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,,CHI EA '23,Association for Computing Machinery,H3LFDVM9,0.0903954802259887,0.5454545454545454,0.0909090909090909,0.1163841807909604
111,111,Examining the Technology-Mediated Cycles of Injustice That Contribute to Digital Ageism: Advancing the Conceptualization of Digital Ageism: Evidence and Implications,Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments,,,10.1145/3529190.3534765,"Chu, Charlene; Nyrup, Rune; Donato-Woodger, Simon; Leslie, Kathleen; Khan, Shehroz; Bernett, Corinne; Grenier, Amanda",2022.0,https://doi.org/10.1145/3529190.3534765,conferencePaper,"Our work draws attention to digital ageism referring to the nexus of ageism (discrimination or bias related to age) that is mediated and perpetuated by artificial intelligent (AI) systems and technologies. Building on the World Health Organization's recently published policy brief entitled “Ageism in AI for Health” and our previous work about digital ageism, this paper aims to advance our current understanding and conceptualization of digital ageism in technology and AI systems broadly and beyond health alone. To do this, we will 1) elaborate on our conceptual model and the ageist technology-mediated cycles of injustice that can produce and reinforce digital ageism; 2) present empirical evidence of our descriptive analysis of seven commonly used facial image datasets to highlight data disparities for older adults which will provide real-world evidence that substantiates one of the elements in our ageist cycles of injustice; and 3) summarize results from our grey literature search of various grey literature databases including the AI ethics guidelines Global Inventory to identify guidance documents that address ageism in AI in research or technology development. This paper uniquely contributes conceptual and empirical evidence of digital ageism which will advance knowledge in the field and deepen our understanding of how ageism in AI is fostered by broader ageist cycles of injustice. Lastly, we will briefly provide future considerations to address digital ageism.&nbsp;",bias; AI; algorithmic bias; ageism; discrimination; inequity,978-1-4503-9631-8,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,545–551,PETRA '22,Association for Computing Machinery,C35V87FC,0.0990990990990991,0.2857142857142857,0.2,0.1153166645304094
112,112,"The Chinese Approach to Artificial Intelligence: An Analysis of Policy, Ethics, and Regulation",AI Soc.,36.0,1,10.1007/s00146-020-00992-2,"Roberts, Huw; Cowls, Josh; Morley, Jessica; Taddeo, Mariarosaria; Wang, Vincent; Floridi, Luciano",2021.0,https://doi.org/10.1007/s00146-020-00992-2,journalArticle,"In July 2017, China’s State Council released the country’s strategy for developing artificial intelligence (AI), entitled ‘New Generation Artificial Intelligence Development Plan’ (新一代人工智能发展规划). This strategy outlined China’s aims to become the world leader in AI by 2030, to monetise AI into a trillion-yuan (ca. 150 billion dollars) industry, and to emerge as the driving force in defining ethical norms and standards for AI. Several reports have analysed specific aspects of China’s AI policies or have assessed the country’s technical capabilities. Instead, in this article, we focus on the socio-political background and policy debates that are shaping China’s AI strategy. In particular, we analyse the main strategic areas in which China is investing in AI and the concurrent ethical debates that are delimiting its use. By focusing on the policy backdrop, we seek to provide a more comprehensive and critical understanding of China’s AI policy by bringing together debates and analyses of a wide array of policy documents.",Digital ethics; Privacy; Artificial intelligence; Governance; China; Cyber warfare; Economic growth; Innovation; International competition; New Generation Artificial Intelligence Development Plan; Policy; Social governance,,0951-5666,2021-03,2023-11-06 01:30:03,2023-11-06 01:30:03,59–77,,,YRCLPHXK,0.0891719745222929,0.217391304347826,0.1538461538461538,0.1148362123214841
113,113,"Co-Designing AI Agents to Support Social Connectedness Among Online Learners: Functionalities, Social Characteristics, and Ethical Challenges",Proceedings of the 2022 ACM Designing Interactive Systems Conference,,,10.1145/3532106.3533534,"Wang, Qiaosi; Jing, Shan; Goel, Ashok K.",2022.0,https://doi.org/10.1145/3532106.3533534,conferencePaper,"Due to the lack of face-to-face interactions, online learners frequently experience social isolation that negatively impacts students’ well-being and learning experiences. Many text-based AI agents have been equipped with different social characteristics and functionalities to support people who are socially isolated. However, the design of agent’s functionalities, social characteristics, and ethical challenges in promoting social connectedness among online learners are underexplored. Taking a co-design approach, we included 23 online learners enrolled in an online for-degree graduate program as active participants in two virtual co-design workshop studies. Through four different co-design activities, we identified online learners’ preferences for AI agent’s functionalities and social characteristics in promoting their social connectedness as well as potential ethical concerns. Based on our findings, we establish the role of AI agent as a facilitator to continuously scaffold online learners’ social connection process. We further discuss the unique ethical challenges regarding agent-mediated social interaction in online learning.",AI ethics; artificial intelligence; AI agent; AI-mediated social interaction; co-design; online learning; social isolation,978-1-4503-9358-4,,2022,2023-11-06 01:30:02,2023-11-06 01:30:02,541–556,DIS '22,Association for Computing Machinery,9JP23WH5,0.08,0.2857142857142857,0.25,0.1148240900334583
114,114,Societal and Ethical Issues of Digitalization,Proceedings of the 2020 International Conference on Big Data in Management,,,10.1145/3437075.3437093,"Nabbosa, Veronica; Kaar, Claudia",2021.0,https://doi.org/10.1145/3437075.3437093,conferencePaper,"The paper intends to analyze two timely trends: Digitalization and associated Digital Ethics, both of which are deepening their roots globally. Data is thought to be the cornerstone of these trends: where once firms were overwhelmed by large quantities of unused structured and unstructured data, they are increasingly adapting their operations and value creation models, guided both by new digital tools and the data themselves. Website cookies, mobile applications, and surveillance cameras, as well as data from third-party vendors, have thus become the new ""digital oil"", as firms exploit process and customer data in pursuit of digitalization. Established firms' core business models have shifted in response to data availability: Apple Pay and Google Pay (operating systems), AliPay (e-commerce), and Lufthansa's Miles &amp; More purchase enabled loyalty card (travel) emerged from old, established businesses. Still, other firms are partially or wholly digitizing existing business processes in order to respond to the challenges posed by digitalization. Banks, for instance, are using fingerprint and facial recognition to make their services more convenient and to improve security. These developments are not only visible among competitive private sector firms: the public sector is also becoming digitalized, not only to promote efficiency but also to promote transparency and accountability. However, customers and citizens are waking up to the fact that their information is being collected by both private and public entities, and have begun to demand control and transparency. Governments and other regulating bodies (ISO, ACM, and IEEE, among others) are taking a more proactive role in responding to these demands. This paper will delve into the tensions inherent in digitalization, zooming in on digital ethics, shifts in societal values, utilitarian benefits and risks, the future of digitalization, the role of technology in digital ethics, and other themes which could impact all society stakeholders and raises question about ethical issues in several topics.",Digital Ethics; Digitalization; Social and Ethical Issues,978-1-4503-7506-1,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,118–124,ICBDM 2020,Association for Computing Machinery,CKSE9P8N,0.0814332247557003,1.0,0.5,0.1140460562146643
115,115,Applying Ethics to AI in the Workplace: The Design of a Scorecard for Australian Workplace Health and Safety,AI Soc.,38.0,2,10.1007/s00146-022-01460-9,"Cebulla, Andreas; Szpak, Zygmunt; Howell, Catherine; Knight, Genevieve; Hussain, Sazzad",2022.0,https://doi.org/10.1007/s00146-022-01460-9,journalArticle,"Artificial Intelligence (AI) is taking centre stage in economic growth and business operations alike. Public discourse about the practical and ethical implications of AI has mainly focussed on the societal level. There is an emerging knowledge base on AI risks to human rights around data security and privacy concerns. A separate strand of work has highlighted the stresses of working in the gig economy. This prevailing focus on human rights and gig impacts has been at the expense of a closer look at how AI may be reshaping traditional workplace relations and, more specifically, workplace health and safety. To address this gap, we outline a conceptual model for developing an AI Work Health and Safety (WHS) Scorecard as a tool to assess and manage the potential risks and hazards to workers resulting from AI use in a workplace. A qualitative, practice-led research study of AI adopters was used to generate and test a novel list of potential AI risks to worker health and safety. Risks were identified after cross-referencing Australian AI Ethics Principles and Principles of Good Work Design with AI ideation, design and implementation stages captured by the AI Canvas, a framework otherwise used for assessing the commercial potential of AI to a business. The unique contribution of this research is the development of a novel matrix itemising currently known or anticipated risks to the WHS and ethical aspects at each AI adoption stage.",AI Canvas; Australia; Ethics principles; Risk assessment; WHS/OHS; Workers,,0951-5666,2022-05,2023-11-06 01:30:04,2023-11-06 01:30:04,919–935,,,3RN2BZYN,0.0978723404255319,0.3333333333333333,0.1666666666666666,0.113120132271679
116,116,Reconfiguring Diversity and Inclusion for AI Ethics,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462622,"Chi, Nicole; Lurie, Emma; Mulligan, Deirdre K.",2021.0,https://doi.org/10.1145/3461702.3462622,conferencePaper,"Activists, journalists, and scholars have long raised critical questions about the relationship between diversity, representation, and structural exclusions in data-intensive tools and services. We build on work mapping the emergent landscape of corporate AI ethics to center one outcome of these conversations: the incorporation of diversity and inclusion in corporate AI ethics activities. Using interpretive document analysis and analytic tools from the values in design field, we examine how diversity and inclusion work is articulated in public-facing AI ethics documentation produced by three companies that create application and services layer AI infrastructure: Google, Microsoft, and Salesforce.We find that as these documents make diversity and inclusion more tractable to engineers and technical clients, they reveal a drift away from civil rights justifications that resonates with the ""managerialization of diversity"" by corporations in the mid-1980s. The focus on technical artifacts - such as diverse and inclusive datasets - and the replacement of equity with fairness make ethical work more actionable for everyday practitioners. Yet, they appear divorced from broader DEI initiatives and relevant subject matter experts that could provide needed context to nuanced decisions around how to operationalize these values and new solutions. Finally, diversity and inclusion, as configured by engineering logic, positions firms not as ""ethics owners"" but as ethics allocators; while these companies claim expertise on AI ethics, the responsibility of defining who diversity and inclusion are meant to protect and where it is relevant is pushed downstream to their customers.",AI ethics; corporate ethics; DEI; diversity; equity; fairness; human rights; inclusion; law,978-1-4503-8473-5,,2021,2023-11-06 01:29:48,2023-11-06 01:29:48,447–457,AIES '21,Association for Computing Machinery,BWQIFITZ,0.0871369294605809,0.4166666666666667,0.4285714285714285,0.1130529303027936
117,117,Prototyping Ethics-As-Practice: A Framework For Designing Data-Driven Product Service Systems,Companion Publication of the 2023 ACM Designing Interactive Systems Conference,,,10.1145/3563703.3593060,"Rattay, Sonja",2023.0,https://doi.org/10.1145/3563703.3593060,conferencePaper,"The rising interest in tech ethics, design ethics and AI ethics highlight the need for deeper engagement with the moral dimensions of current design practice. Following a surge of criticism on traditional consequentialist, value- and principle-based suggestions, recent studies have turned towards relational ethics as a promising alternative to capture the messy, complex, and fluid characteristics of moral considerations in technical systems. This project follows these approaches, seeking to prototype new methods of ethics-as-practice for design practices of data driven technologies. I use sociotechnical imaginaries as a means of constructing alternative futures which position ethics as a contextual, situated doing, inherently embedded in design practice. Through this combination of relational ethics and sociotechnical imaginaries I seek to develop a conceptual framework that enables designers to engage with the ethical dimensions of their work productively and intentionally in practice.",,978-1-4503-9898-5,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,22–26,DIS '23 Companion,Association for Computing Machinery,D4KETRCG,0.1231884057971014,0.0,0.0,0.1127162652677317
118,118,Human Capital Development via Digital Inclusion,Proceedings of the 3rd International Scientific Conference on Innovations in Digital Economy,,,10.1145/3527049.3527074,"Golovina, Tatiana; Anoshina, Julia; Markov, Roman; Melnikov, Pavel; Zaborovskaya, Olga",2022.0,https://doi.org/10.1145/3527049.3527074,conferencePaper,"The research is relevant and up-to-date by the necessity of creating evidence-based approaches to human capital management on the basis of the complex socio-economic analysis of risks and opportunities for digital inclusion of population in particular territories. Whereas a number of major urban-centered businesses have adopted remote working, rural areas and small towns with their small and medium-sized businesses show the insignificant development of remote working. Due to this, it is essential to characterize the regional ecosystem of digital inclusion, digital divide between the center and the periphery. Another complex problem that involves different stakeholders – citizens, state, corporations, public institutions etc. – is to define the boundaries of digital ethics. This problem requires a complex solution. The need of regulation to ensure compliance with ethical principles is increasing along with the accumulation of sufficient data that can be used in a way when the consequences can be as positive and as negative. The purpose of the research is to study the problem of human capital development and to elaborate on its structure and content via digital inclusion of the population. The tasks of the research are to systematize theoretical approaches to studying digital inclusion of population; to specify the content and factors of digital inequality; to explain the authors’ structure of human capital in the framework of digital inclusion. In order to analyze the issues of the stated problems, the authors used such methodological techniques and instruments as sociological, economic, systematic and spatial approaches, methods of structural and systems analysis, retrospective assessment and text mining. The main results of the research are the updated content and systematization of factors of digital inequality; detected social and economic grounds for digital inclusion as the instrument of human capital development in a particular territory; the authors’ level structure of human capital in the framework of digital inclusion; specification of the role and trends of modernizing the system of higher education in context of reducing the digital divide and increasing the efficiency of forming and using the human capital. The scientific importance of the research aims to expand the theoretical framework of approaches to the development of human capital via digital inclusion. The practical importance focuses on the formation of the methodological grounds for digital inclusion as an instrument for improving the quality of human capital in a territory.",digital divide; digital inclusion; digital technologies; human capital; the system of education,978-1-4503-8694-4,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,7–14,SPBPU IDE '21,Association for Computing Machinery,URTCKAK5,0.0932642487046632,0.5,0.3333333333333333,0.1115243596595295
119,119,Technological Support to Foster Students’ Artificial Intelligence Ethics: An Augmented Reality-Based Contextualized Dilemma Discussion Approach,Comput. Educ.,201.0,C,10.1016/j.compedu.2023.104813,"Lin, Xiao-Fan; Wang, Zhaoyang; Zhou, Wei; Luo, Guoyu; Hwang, Gwo-Jen; Zhou, Yue; Wang, Jing; Hu, Qintai; Li, Wenyi; Liang, Zhong-Mei",2023.0,https://doi.org/10.1016/j.compedu.2023.104813,journalArticle,"Research evidence has emphasized the potential of questioning-based dilemmas and has contextualized role-play simulation to foster students' artificial intelligence (AI) ethics. Therefore, this study combined the viewpoints to design a contextualized dilemma discussion approach in the context of developing primary school students' AI ethics. However, without case-specific and suitable guidelines, students might have difficulties participating in the learning tasks to play different roles in the AI ethical dilemma discussion. Accordingly, for this study, we developed an augmented reality (AR) learning system-based contextualized dilemma discussion approach to foster students' active engagement in learning AI ethics with dilemma discussion using visualized AR guidance and feedback. A quasi-experiment and lag sequential analysis were executed by recruiting 79 primary school students to examine the effects of the proposed approach. The experimental results showed that the proposed approach was better able to improve students' learning achievements of AI ethical awareness, ethical reasoning, and higher order thinking tendency than the mobile learning system-guided contextualized dilemma discussion approach. Lag sequential analysis findings identified that the proposed approach could promote students’ AI ethical learning behavior patterns by some crucial guidelines, including (1) embedding the non-intrusive AR guidance and feedback in the contextualized dilemma discussion, (2) immersive role-playing scenarios, and (3) case-based visual discussion. Administrators could provide a supportive environment to promote the combination of AR guidance and the contextualized dilemma discussion approach to improve the effectiveness of learning AI ethics.",Augmented and virtual reality; Elementary education; Interactive learning environment; Pedagogical issues,,0360-1315,2023-08,2023-11-06 01:30:00,2023-11-06 01:30:00,,,,C9IAXEFV,0.1163793103448275,0.0,0.1333333333333333,0.1115156547843258
120,120,Teaching Responsible Data Science,1st International Workshop on Data Systems Education,,,10.1145/3531072.3535318,"Stoyanovich, Julia",2022.0,https://doi.org/10.1145/3531072.3535318,conferencePaper,"Responsible Data Science (RDS) and Responsible AI (RAI) have emerged as prominent areas of research and practice. Yet, educational materials and methodologies on this important subject still lack. In this paper, I will recount my experience in developing, teaching, and refining a technical course called “Responsible Data Science”, which tackles the issues of ethics in AI, legal compliance, data quality, algorithmic fairness and diversity, transparency of data and algorithms, privacy, and data protection. I will also describe a public education course called “We are AI: Taking Control of Technology” that brings these topics of AI ethics to the general audience in a peer-learning setting. I made all course materials are publicly available online, hoping to inspire others in the community to come together to form a deeper understanding of the pedagogical needs of RDS and RAI, and to develop and share the much-needed concrete educational materials and methodologies.",Responsible AI; Responsible Data Science,978-1-4503-9350-8,,2022,2023-11-06 01:30:03,2023-11-06 01:30:03,4–9,DataEd '22,Association for Computing Machinery,YI5LH69S,0.0945945945945946,0.4,0.25,0.1111290031690413
121,121,Development and Validation of an Instrument to Measure Undergraduate Students’ Attitudes toward the Ethics of Artificial Intelligence (AT-EAI) and Analysis of Its Difference by Gender and Experience of AI Education,Education and Information Technologies,27.0,8,10.1007/s10639-022-11086-5,"Jang, Yeonju; Choi, Seongyune; Kim, Hyeoncheol",2022.0,https://doi.org/10.1007/s10639-022-11086-5,journalArticle,"As artificial intelligence (AI) becomes more prevalent, so does the interest in AI ethics. To address issues related to AI ethics, many government agencies, non-governmental organizations (NGOs), and corporations have published AI ethics guidelines. However, only a few test instruments have been developed to assess students’ attitudes toward AI ethics. A related instrument&nbsp;is required to effectively prepare lecture curricula and materials on AI ethics, as well as to quantitatively evaluate the learning effect of students. In this study, we developed and validated the instrument (AT-EAI) to assess undergraduate&nbsp;students’ attitudes toward AI ethics. The instrument’s reliability, content validity, and construct validity were evaluated following its development and application in a sample of 1,076 undergraduate students. Initially, the instrument comprised five dimensions that totaled 42 items, while the final version had 17 items. When it came to content validity, experts (n = 8) were involved in the process. Exploratory factor analysis identified five dimensions, and confirmatory factor analysis found that the model was good-fitting. The reliability analysis using Cronbach’s alpha and the corrected item-total correlation were both satisfactory. Considering all the results, the developed instrument possesses the psychometric properties necessary to be considered a valid and reliable instrument for measuring undergraduate students’ attitudes toward AI ethics. This study also found that there were gender differences in fairness, privacy, and non-maleficence dimensions. Furthermore, it revealed the difference in students’ attitudes toward fairness based on their prior experience with AI education.",AI ethics education; Instrument validation; Self-evaluation; Students’ attitude scale,,1360-2357,2022-09,2023-11-06 01:29:59,2023-11-06 01:29:59,11635–11667,,,T2T7HIIF,0.1012658227848101,0.3333333333333333,0.1,0.1108542172257539
122,122,"The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future Improvements","Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",,,10.1145/3617694.3623223,"Feffer, Michael; Martelaro, Nikolas; Heidari, Hoda",2023.0,https://doi.org/10.1145/3617694.3623223,conferencePaper,"Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students’ initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students’ feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.",AI safety; AI harms; classroom exploration; educational tool; incident database,9798400703812,,2023,2023-11-06 01:30:00,2023-11-06 01:30:00,,EAAMO '23,Association for Computing Machinery,4ATX49RJ,0.1067193675889328,0.2,0.0869565217391304,0.1090331391385055
123,123,"Hey, Google, Leave Those Kids Alone: Against Hypernudging Children in the Age of Big Data",AI Soc.,38.0,4,10.1007/s00146-021-01314-w,"Smith, James; de Villiers-Botha, Tanya",2021.0,https://doi.org/10.1007/s00146-021-01314-w,journalArticle,"Children continue to be overlooked as a topic of concern in discussions around the ethical use of people’s data and information. Where children are the subject of such discussions, the focus is often primarily on privacy concerns and consent relating to the use of their data. This paper highlights the unique challenges children face when it comes to online interferences with their decision-making, primarily due to their vulnerability, impressionability, the increased likelihood of disclosing personal information online, and their developmental capacities. These traits allow for practices such as hypernudging to be executed on them more accurately and with more serious consequences, specifically by potentially undermining their autonomy. We argue that children are autonomous agents in the making and thus require additional special protections to ensure that the development of their autonomy is safeguarded. This means that measures should be taken to prohibit most forms of hypernudging children and thus ensure that they are protected from this powerful technique of digital manipulation.",Big data; Data ethics; Autonomy; Children; Digital manipulation; Historical account of autonomy; Hypernudging,,0951-5666,2021-11,2023-11-06 01:30:02,2023-11-06 01:30:02,1639–1649,,,TWU8S79L,0.062111801242236,0.6153846153846154,0.0666666666666666,0.10884938381675
124,124,From Human to Data to Dataset: Mapping the Traceability of Human Subjects in Computer Vision Datasets,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579488,"Scheuerman, Morgan Klaus; Weathington, Katy; Mugunthan, Tarun; Denton, Emily; Fiesler, Casey",2023.0,https://doi.org/10.1145/3579488,journalArticle,"Computer vision is a ""data hungry"" field. Researchers and practitioners who work on human-centric computer vision, like facial recognition, emphasize the necessity of vast amounts of data for more robust and accurate models. Humans are seen as a data resource which can be converted into datasets. The necessity of data has led to a proliferation of gathering data from easily available sources, including ""public"" data from the web. Yet the use of public data has significant ethical implications for the human subjects in datasets. We bridge academic conversations on the ethics of using publicly obtained data with concerns about privacy and agency associated with computer vision applications. Specifically, we examine how practices of dataset construction from public data-not only from websites, but also from public settings and public records-make it extremely difficult for human subjects to trace their images as they are collected, converted into datasets, distributed for use, and, in some cases, retracted. We discuss two interconnected barriers current data practices present to providing an ethics of traceability for human subjects: awareness and control. We conclude with key intervention points for enabling traceability for data subjects. We also offer suggestions for an improved ethics of traceability to enable both awareness and control for individual subjects in dataset curation practices.",data ethics; machine learning; datasets; computer vision; data subjects,,,2023-04,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,GT7T3MIR,0.0952380952380952,0.4444444444444444,0.0625,0.1087860795852599
125,125,An Interdisciplinary Conceptual Study of Artificial Intelligence (AI) for Helping Benefit-Risk Assessment Practices,AI Commun.,34.0,2,10.3233/AIC-201523,"Chassang, Gauthier; Thomsen, Mogens; Rumeau, Pierre; Sèdes, Florence; Delfin, Alejandra",2021.0,https://doi.org/10.3233/AIC-201523,journalArticle,"We propose a comprehensive analysis of existing concepts of AI coming from different disciplines: Psychology and engineering tackle the notion of intelligence, while ethics and law intend to regulate AI innovations. The aim is to identify shared notions or discrepancies to consider for qualifying AI systems. Relevant concepts are integrated into a matrix intended to help defining more precisely when and how computing tools (programs or devices) may be qualified as AI while highlighting critical features to serve a specific technical, ethical and legal assessment of challenges in AI development. Some adaptations of existing notions of AI characteristics are proposed. The matrix is a risk-based conceptual model designed to allow an empirical, flexible and scalable qualification of AI technologies in the perspective of benefit-risk assessment practices, technological monitoring and regulatory compliance: it offers a structured reflection tool for stakeholders in AI development that are engaged in responsible research and innovation.",AI Ethics; AI qualification matrix; benefit-risk assessment; conceptual analysis; interdisciplinary study,,0921-7126,2021-01,2023-11-06 01:30:01,2023-11-06 01:30:01,121–146,,,URPKUUM3,0.0866666666666666,0.3636363636363636,0.0769230769230769,0.1087704653712558
126,126,From Black Box To&nbsp;Glass Box: Advancing Transparency In&nbsp;Artificial Intelligence Systems For&nbsp;Ethical And&nbsp;Trustworthy AI,"Computational Science and Its Applications – ICCSA 2023 Workshops: Athens, Greece, July 3–6, 2023, Proceedings, Part IV",,,10.1007/978-3-031-37114-1_9,"Franzoni, Valentina",2023.0,https://doi.org/10.1007/978-3-031-37114-1_9,conferencePaper,"The rapid development of Artificial Intelligence (AI) systems has raised significant ethical concerns, particularly with the problem of transparency in their decision-making processes. As AI systems become increasingly integrated into various aspects of society, there is an urgent need to transform these ‘black box’ models into more transparent and understandable ‘glass-box’ systems. This paper explores the methods, challenges, and implications associated with supplementing transparency in AI systems in the promotion of ethical and trustworthy AI. We will examine the significance of transparency for stakeholders (i.e., users, developers, and policymakers), and the trade-offs between transparency and other objectives, e.g. accuracy and privacy. Recent US legislation prohibiting copyright claims on neural network-generated documents is used to illustrate the issues presented by the opaque nature of black-box AI models.A thorough literature review is carried out to investigate current approaches and tools for AI transparency and identify gaps and areas for future research.By moving from black box to glass box AI systems, we can ensure that AI technologies are not only powerful but also ethically sound and aligned with human values. This study adds to the ongoing debate about AI ethics and prepares the way for future research into the complex landscape of transparency, trust, and decision-making in AI systems.",artificial intelligence; transparency; explainable AI; algorithmic auditing; decision-making; trust; neural networks; black box; deep learning ethics,978-3-031-37113-4,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,118–130,,Springer-Verlag,EFE9HQR4,0.0970873786407767,0.25,0.0769230769230769,0.1087617058986189
127,127,South Korean Public Value Coproduction Towards‘AI for Humanity’: A Synergy of Sociocultural Norms and Multistakeholder Deliberation in Bridging the Design and Implementation of National AI Ethics Guidelines,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533091,"Ha, You Jeen",2022.0,https://doi.org/10.1145/3531146.3533091,conferencePaper,"As emerging technologies such as Big Data, Artificial Intelligence (AI), robotics, and the Internet of Things (IoT) pose fundamental challenges for global and domestic technological governance, the‘Fourth Industrial Revolution’ (4IR) comes to the fore with AI as a frontrunner, generating discussions on the ethical elements of AI amongst key stakeholder groups, such as government, academia, industry, and civil society. However, in recent AI ethics and governance scholarship, AI ethics design appears to be divorced from AI ethics implementation, an implicit partition that results in two separate matters of theory and practice, respectively, and thus invokes efforts to bridge the‘gap’ between the two. Such a partition potentially overcomplicates the discussion surrounding AI ethics and limits its productivity. This paper thus presents South Korea’s people-centered‘National Guidelines for Artificial Intelligence Ethics’(국가인공지능윤리기준;‘Guidelines’) and their development under the Moon administration as a case study that can help readers conceptualize AI ethics design and implementation as a continuous process rather than a partitioned one. From a public value perspective, the case study examines the Guidelines and the multistakeholder policymaking infrastructure that serves as the foundation for both the Guidelines’ design and implementation. This examination draws from literature in AI ethics and governance, public management and administration, and Korean policy and cultural studies as well as government and public documents alongside 9 interviews with members from the four stakeholder groups that collectively designed and continue to deliberate upon the Guidelines. Further, the study specifically focuses on (i) identifying public values that were highlighted by the Guidelines, (ii) investigating how such values reflect prevalent Korean sociocultural norms, and (iii) exploring how these values, in a way made possible by Korean sociocultural norms and policymaking, have been negotiated amongst the four stakeholder groups in a democratic public sphere to be ultimately incorporated into the Guidelines and prepared for implementation. This paper hopes to contribute to theory-building in AI ethics and provide a point of comparison in the international stage for future research concerning AI ethics design and implementation.",4IR; AI ethics; AI policy; Artificial intelligence; Deliberative democracy; Multistakeholder; National Guidelines for Artificial Intelligence Ethics; Networked governance; Public value; Sociocultural norms; South Korea,978-1-4503-9352-2,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,267–277,FAccT '22,Association for Computing Machinery,7TT4ETW9,0.094224924012158,0.25,0.1111111111111111,0.1077013534965835
128,128,Human Autonomy in Algorithmic Management,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534168,"Unruh, Charlotte Franziska; Haid, Charlotte; Johannes, Fottner; Büthe, Tim",2022.0,https://doi.org/10.1145/3514094.3534168,conferencePaper,"Algorithmic management tools support or replace managerial decision making in areas such as task allocation, shift planning, or team formation. These tools can have a significant impact on the lives of workers. In this paper, we contribute to the emerging literature on the ethics of algorithmic management by developing a conceptual framework for autonomy at work. Further, we use this framework to discuss risks and opportunities for autonomy in the context of work decision algorithms in Industry 4.0.",ai ethics; algorithmic management; autonomy; industry 4.0; work decision algorithms,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,753–762,AIES '22,Association for Computing Machinery,EVPKYH7L,0.0512820512820512,0.4,0.2,0.1069316648264016
129,129,The Problem with Trust: On the Discursive Commodification of Trust in AI,AI Soc.,38.0,4,10.1007/s00146-022-01401-6,"Krüger, Steffen; Wilson, Christopher",2022.0,https://doi.org/10.1007/s00146-022-01401-6,journalArticle,"This commentary draws critical attention to the ongoing commodification of trust in policy and scholarly discourses of artificial intelligence (AI) and society. Based on an assessment of publications discussing the implementation of AI in governmental and private services, our findings indicate that this discursive trend towards commodification is driven by the need for a trusting population of service users to harvest data at scale and leads to the discursive construction of trust as an essential good on a par with data as raw material. This discursive commodification is marked by a decreasing emphasis on trust understood as the expected reliability of a trusted agent, and increased emphasis on instrumental and extractive framings of trust as a resource. This tendency, we argue, does an ultimate disservice to developers, users, and systems alike, insofar as it obscures the subtle mechanisms through which trust in AI systems might be built, making it less likely that it will be.",AI ethics; Artificial intelligence; Trust; Commodification; Discourse analysis,,0951-5666,2022-02,2023-11-06 01:30:00,2023-11-06 01:30:00,1753–1761,,,J9LZRYGZ,0.064516129032258,0.5,0.25,0.1059182118364236
130,130,Measuring Automated Influence: Between Empirical Evidence and Ethical Values,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462532,"Susser, Daniel; Grimaldi, Vincent",2021.0,https://doi.org/10.1145/3461702.3462532,conferencePaper,"Automated influence, delivered by digital targeting technologies such as targeted advertising, digital nudges, and recommender systems, has attracted significant interest from both empirical researchers, on one hand, and critical scholars and policymakers on the other. In this paper, we argue for closer integration of these efforts. Critical scholars and policymakers, who focus primarily on the social, ethical, and political effects of these technologies, need empirical evidence to substantiate and motivate their concerns. However, existing empirical research investigating the effectiveness of these technologies (or lack thereof), neglects other morally relevant effects-which can be felt regardless of whether or not the technologies ""work"" in the sense of fulfilling the promises of their designers. Drawing from the ethics and policy literature, we enumerate a range of questions begging for empirical analysis-the outline of a research agenda bridging these fields—and issue a call to action for more empirical research that takes these urgent ethics and policy questions as their starting point.",ethics; autonomy; privacy; dark patterns; influence; law and policy; nudges; targeted advertising,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,242–253,AIES '21,Association for Computing Machinery,WEQAA78U,0.0700636942675159,0.3333333333333333,0.3333333333333333,0.1056184542662563
131,131,Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534158,"Bringas Colmenarejo, Alejandra; Nannini, Luca; Rieger, Alisa; Scott, Kristen M.; Zhao, Xuan; Patro, Gourab K; Kasneci, Gjergji; Kinder-Kurlanda, Katharina",2022.0,https://doi.org/10.1145/3514094.3534158,conferencePaper,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",ai regulation; consequential ethics; deontological ethics; egalitarian welfare; eu ai proposal; localization; standardization; utilitarian welfare,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,107–118,AIES '22,Association for Computing Machinery,P5I5PLWM,0.0689655172413793,0.4,0.0833333333333333,0.1036978538702676
132,132,Challenges to Incorporate Accountability into Artificial Intelligence,Procedia Comput. Sci.,204.0,C,10.1016/j.procs.2022.08.063,"Baldi, Vania; Oliveira, Lídia",2022.0,https://doi.org/10.1016/j.procs.2022.08.063,journalArticle,"The ethical challenge lies in the fact that Artificial Intelligence systems act and learn based on data and the correlations between them that are internally projected to them. In this sense, this paper articulates some theoretical positions on the design that Ai should have in order to be more responsible and ethical in its relations with culturally diverse human beings.",Artificial Intelligence; Agency; Awareness,,1877-0509,2022-01,2023-11-06 01:30:03,2023-11-06 01:30:03,519–523,,,KLQPC8N7,0.1333333333333333,0.0,0.0,0.1028178694158075
133,133,AI Ethics – a Review of Three Recent Publications,AI Soc.,36.0,2,10.1007/s00146-020-01087-8,"Põder, Johann-Christian",2021.0,https://doi.org/10.1007/s00146-020-01087-8,journalArticle,"In summary, then, all three books can serve as solid and accessible introductions to AI ethics, and can profitably be read both by the scientific community and by a wider public. They offer valuable insights into a wide range of ethical issues relating to AI, thereby inviting further reflection upon how AI is shaping and changing our present and future lives. All three books fruitfully and critically complement one another, helping us to see how AI ethics oscillates between anthropocentric and posthumanist approaches, thereby aiming to find ways to ethically articulate and conceptualize issues and developments that humanity is yet to face. To tackle these issues, we need an equal and participatory discourse (Loh), practical wisdom (Coeckelbergh), and the right moral attitudes towards those who do not belong to our species (Misselhorn). Together, these closing remarks from each of the authors offer an inspiring vision and impetus for further reflection and development of AI ethics.",,,0951-5666,2021-06,2023-11-06 01:29:49,2023-11-06 01:29:49,661–664,,,DBSHCG5U,0.0903225806451612,0.0,0.3333333333333333,0.1017282577349318
134,134,AI Audit: A Card Game to Reflect on Everyday AI Systems,Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence,,,10.1609/aaai.v37i13.26897,"Ali, Safinah; Kumar, Vishesh; Breazeal, Cynthia",2023.0,https://doi.org/10.1609/aaai.v37i13.26897,conferencePaper,"An essential element of K-12 AI literacy is educating learners about the ethical and societal implications of AI systems. Previous work in AI ethics literacy have developed curriculum and classroom activities that engage learners in reflecting on the ethical implications of AI systems and developing responsible AI. There is little work in using game-based learning methods in AI literacy. Games are known to be compelling media to teach children about complex STEM concepts. In this work, we developed a competitive card game for middle and high school students called ""AI Audit"" where they play as AI start-up founders building novel AI-powered technology. Players can challenge other players with potential harms of their technology or defend their own businesses by features that mitigate these harms. The game mechanics reward systems that are ethically developed or that take steps to mitigate potential harms. In this paper, we present the game design, teacher resources for classroom deployment and early playtesting results. We discuss our reflections about using games as teaching tools for AI literacy in K-12 classrooms.",,978-1-57735-880-0,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,,AAAI'23/IAAI'23/EAAI'23,AAAI Press,FWU4K8XE,0.0977011494252873,0.0,0.1818181818181818,0.1015954564805139
135,135,"AI in My Life: AI, Ethics &amp; Privacy Workshops for 15-16-Year-Olds",Companion Publication of the 13th ACM Web Science Conference 2021,,,10.1145/3462741.3466664,"Bendechache, Malika; Tal, Irina; Wall, Pj; Grehan, Laura; Clarke, Emma; Odriscoll, Aidan; Der Haegen, Laurence Van; Leong, Brenda; Kearns, Anne; Brennan, Rob",2021.0,https://doi.org/10.1145/3462741.3466664,conferencePaper,"AI in My Life’ project will engage 500 Dublin teenagers from disadvantaged backgrounds in a 15-week (20-hour) co-created, interactive workshop series encouraging them to reflect on their experiences in a world shaped by Artificial Intelligence (AI), personal data processing and digital transformation. Students will be empowered to evaluate the ethical and privacy implications of AI in their lives, to protect their digital privacy and to activate STEM careers and university awareness. It extends the ‘DCU TY’ programme for innovative educational opportunities for Transition Year students from underrepresented communities in higher education. Privacy and cybersecurity researchers and public engagement professionals from the SFI Centres ADAPT1 and Lero2 will join experts from the Future of Privacy Forum3 and the INTEGRITY H20204 project to deliver the programme to the DCU Access5 22-school network. DCU Access has a mission of creating equality of access to third-level education for students from groups currently underrepresented in higher education. Each partner brings proven training activities in AI, ethics and privacy. A novel blending of material into a youth-driven narrative will be the subject of initial co-creation workshops and supported by pilot material delivery by undergraduate DCU Student Ambassadors. Train-the-trainer workshops and a toolkit for teachers will enable delivery. The material will use a blended approach (in person and online) for delivery during COVID-19. It will also enable wider use of the material developed. An external study of programme effectiveness will report on participants’: enhanced understanding of AI and its impact, improved data literacy skills in terms of their understanding of data privacy and security, empowerment to protect privacy, growth in confidence in participating in public discourse about STEM, increased propensity to consider STEM subjects at all levels, and greater capacity of teachers to facilitate STEM interventions. This paper introduces the project, presents more details about co-creation workshops that is a particular step in the proposed methodology and reports some preliminary results.",Artificial Intelligence; Co-creation; Data; Ethics.; Privacy; Security; STEM,978-1-4503-8525-1,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,34–39,WebSci '21 Companion,Association for Computing Machinery,WLMG8KHR,0.0764331210191082,0.5,0.4545454545454545,0.1015217279135833
136,136,"Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation",Inf. Fusion,99.0,C,10.1016/j.inffus.2023.101896,"Díaz-Rodríguez, Natalia; Del Ser, Javier; Coeckelbergh, Mark; López de Prado, Marcos; Herrera-Viedma, Enrique; Herrera, Francisco",2023.0,https://doi.org/10.1016/j.inffus.2023.101896,journalArticle,"Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system’s entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective. However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system’s life cycle, and considers previous aspects from different lenses. A more holistic vision contemplates four essential axes: the global principles for ethical use and development of AI-based systems, a philosophical take on AI ethics, a risk-based approach to AI regulation, and the mentioned pillars and requirements. The seven requirements (human agency and oversight; robustness and safety; privacy and data governance; transparency; diversity, non-discrimination and fairness; societal and environmental wellbeing; and accountability) are analyzed from a triple perspective: What each requirement for trustworthy AI is, Why it is needed, and How each requirement can be implemented in practice. On the other hand, a practical approach to implement trustworthy AI systems allows defining the concept of responsibility of AI-based systems facing the law, through a given auditing process. Therefore, a responsible AI system is the resulting notion we introduce in this work, and a concept of utmost necessity that can be realized through auditing processes, subject to the challenges posed by the use of regulatory sandboxes. Our multidisciplinary vision of trustworthy AI culminates in a debate on the diverging views published lately about the future of AI. Our reflections in this matter conclude that regulation is a key for reaching a consensus among these views, and that trustworthy and responsible AI systems will be crucial for the present and future of our society.",AI ethics; AI regulation; Trustworthy AI; Regulatory sandbox; Responsible AI systems,,1566-2535,2023-11,2023-11-06 01:29:59,2023-11-06 01:29:59,,,,XMDJMV9H,0.0729166666666666,0.5454545454545454,0.2,0.1002493466129829
137,137,Digital Voodoo Dolls,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462626,"Slavkovik, Marija; Stachl, Clemens; Pitman, Caroline; Askonas, Jonathan",2021.0,https://doi.org/10.1145/3461702.3462626,conferencePaper,"An institution, be it a body of government, commercial enterprise, or a service, cannot interact directly with a person. Instead, a model is created to represent us. We argue the existence of a new high-fidelity type of person model which we call a digital voodoo doll. We conceptualize it and compare its features with existing models of persons. Digital voodoo dolls are distinguished by existing completely beyond the influence and control of the person they represent. We discuss the ethical issues that such a lack of accountability creates and argue how these concerns can be mitigated.",value alignment; power and inequality; impact of AI on society; rights and representation,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,967–977,AIES '21,Association for Computing Machinery,8824DQDW,0.0729166666666666,0.1538461538461538,0.6666666666666666,0.1001316800175573
138,138,Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics?,Minds Mach.,31.0,1,10.1007/s11023-020-09531-w,"Pinka, Robert",2021.0,https://doi.org/10.1007/s11023-020-09531-w,journalArticle,"Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethics—we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behavior—close enough that we may consider it so.",Artificial intelligence; Machine learning; Machine ethics; Philosophy of technology; Pragmatism; STS,,0924-6495,2021-03,2023-11-06 01:30:03,2023-11-06 01:30:03,121–136,,,FWWSQAZ4,0.082901554404145,0.1818181818181818,0.25,0.0983075634163061
139,139,Exploring Responsible AI Practices in Dutch Media Organizations,"Human-Computer Interaction – INTERACT 2023: 19th IFIP TC13 International Conference, York, UK, August 28 – September 1, 2023, Proceedings, Part IV",,,10.1007/978-3-031-42293-5_58,"Mioch, Tina; Stembert, Nathalie; Timmers, Cathelijn; Hajri, Oumaima; Wiggers, Pascal; Harbers, Maaike",2023.0,https://doi.org/10.1007/978-3-031-42293-5_58,conferencePaper,"Artificial Intelligence (AI) is increasingly used in the media industry, for instance, for the automatic creation, personalization, and distribution of media content. This development raises concerns in society and the media sector itself about the responsible use of AI. This study examines how different stakeholders in media organizations perceive ethical issues in their work concerning AI development and application, and how they interpret and put them into practice. We conducted an empirical study consisting of 14 semi-structured qualitative interviews with different stakeholders in public and private media organizations, and mapped the results of the interviews on stakeholder journeys to specify how AI applications are initiated, designed, developed, and deployed in the different media organizations. This results in insights into the current situation and challenges regarding responsible AI practices in media organizations.",Responsible AI; AI Ethics in Practice; Empirical Studies on Ethics,978-3-031-42292-8,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,481–485,,Springer-Verlag,SMPKGGX4,0.0610687022900763,0.6,0.125,0.0980753727616465
140,140,"Assessing MyData Scenarios: Ethics, Concerns, and the Promise",Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems,,,10.1145/3411764.3445213,"Alorwu, Andy; Kheirinejad, Saba; van Berkel, Niels; Kinnula, Marianne; Ferreira, Denzil; Visuri, Aku; Hosio, Simo",2021.0,https://doi.org/10.1145/3411764.3445213,conferencePaper,"Public controversies around the unethical use of personal data are increasing, spotlighting data ethics as an increasingly important field of study. MyData is a related emerging vision that emphasizes individuals’ control of their personal data. In this paper, we investigate people’s perceptions of various data management scenarios by measuring the perceived ethicality and level of felt concern concerning the scenarios. We deployed a set of 96 unique scenarios to an online crowdsourcing platform for assessment and invited a representative sample of the participants to a second-stage questionnaire about the MyData vision and its potential in the field of healthcare. Our results provide a timely investigation into how topical data-related practices affect the perceived ethicality and the felt concern. The questionnaire analysis reveals great potential in the MyData vision. Through the combined quantitative and qualitative results, we contribute to the field of data ethics.",Privacy; Ethics; Crowdsourcing; Healthcare; MyData,978-1-4503-8096-6,,2021,2023-11-06 01:30:03,2023-11-06 01:30:03,,CHI '21,Association for Computing Machinery,YFE3IHV5,0.0629370629370629,0.6,0.25,0.0978494589352253
141,141,"Online Public Discourse on Artificial Intelligence and Ethics in China: Context, Content, and Implications",AI Soc.,38.0,1,10.1007/s00146-021-01309-7,"Mao, Yishu; Shi-Kupfer, Kristin",2021.0,https://doi.org/10.1007/s00146-021-01309-7,journalArticle,"The societal and ethical implications of artificial intelligence (AI) have sparked discussions among academics, policymakers and the public around the world. What has gone unnoticed so far are the likewise vibrant discussions in China. We analyzed a large sample of discussions about AI ethics on two Chinese social media platforms. Findings suggest that participants were diverse, and included scholars, IT industry actors, journalists, and members of the general public. They addressed a broad range of concerns associated with the application of AI in various fields. Some even gave recommendations on how to tackle these issues. We argue that these discussions are a valuable source for understanding the future trajectory of AI development in China as well as implications for global dialogue on AI governance.",Ethics; Artificial intelligence; Governance; China; Content analysis; Public opinion; Social media,,0951-5666,2021-11,2023-11-06 01:30:02,2023-11-06 01:30:02,373–389,,,NBF3PNNN,0.0806451612903225,0.1818181818181818,0.1428571428571428,0.0968467570290989
142,142,Governing Algorithms from the South: A Case Study of AI Development in Africa,AI Soc.,38.0,4,10.1007/s00146-022-01527-7,"Hassan, Yousif",2022.0,https://doi.org/10.1007/s00146-022-01527-7,journalArticle,"AI technology is capturing the African imaginations as a gateway to progress and prosperity. There is a growing interest in AI by different actors across the continent including scientists, researchers, humanitarian and aid organizations, academic institutions, tech start-ups, and media organizations. Several African states are looking to adopt AI technology to capture economic growth and development opportunities. On the other hand, African researchers highlight the gap in regulatory frameworks and policies that govern the development of AI in the continent. They argue that this could lead to AI technology exacerbating problems of inequalities and injustice in the continent. However, most of the literature on AI ethics is biased toward Euro-American perspectives and lack the understanding of how AI development is apprehended in the Global South, and particularly Africa. Drawing on the case study of the first African Master’s in Machine Intelligence program, this paper argues for looking beyond the question of ethics in AI and examining AI governance issues through the analytical lens of the raciality of computing and the political economy of technoscience to understand AI development in Africa. By doing so, this paper seeks a different theorization for AI ethics from the South that is based on lived experiences of those in the margins and avoids the framings of technological futures that simplistically pathologize or celebrate Africa.",AI ethics; Decolonizing AI; Political economy of AI; Science and technology studies; Technoscientific capitalism,,0951-5666,2022-07,2023-11-06 01:29:59,2023-11-06 01:29:59,1429–1442,,,N2T5UUI3,0.0776255707762557,0.3571428571428571,0.0769230769230769,0.096589068377205
143,143,"Stronger Together: On the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML","Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594002,"Pistilli, Giada; Muñoz Ferrandis, Carlos; Jernite, Yacine; Mitchell, Margaret",2023.0,https://doi.org/10.1145/3593013.3594002,conferencePaper,"The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others.",AI Governance; Applied Ethics; AI Policy; Documentation; ML Licensing,9798400701924,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,343–354,FAccT '23,Association for Computing Machinery,JAHAAK6W,0.0737704918032786,0.4444444444444444,0.2,0.0955462495456447
144,144,Ethical Implications of Fairness Interventions: What Might Be Hidden behind Engineering Choices?,Ethics and Inf. Technol.,24.0,1,10.1007/s10676-022-09636-z,"Aler Tubella, Andrea; Barsotti, Flavia; Koçer, Rüya Gökhan; Mendez, Julian Alfredo",2022.0,https://doi.org/10.1007/s10676-022-09636-z,journalArticle,"The importance of fairness in machine learning models is widely acknowledged, and ongoing academic debate revolves around how to determine the appropriate fairness definition, and how to tackle the trade-off between fairness and model performance. In this paper we argue that besides these concerns, there can be ethical implications behind seemingly purely technical choices in fairness interventions in a typical model development pipeline. As an example we show that the technical choice between in-processing and post-processing is not necessarily value-free and may have serious implications in terms of who will be affected by the specific fairness intervention. The paper reveals how assessing the technical choices in terms of their ethical consequences can contribute to the design of fair models and to the related societal discussions.",AI Ethics; Responsible AI; Fairness; Bias mitigation,,1388-1957,2022-03,2023-11-06 01:30:00,2023-11-06 01:30:00,,,,WAT4H4D5,0.048,0.5714285714285714,0.25,0.0948916355274504
145,145,Responsible AI Systems: Who Are the Stakeholders?,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534187,"Deshpande, Advait; Sharp, Helen",2022.0,https://doi.org/10.1145/3514094.3534187,conferencePaper,"As of 2021, there were more than 170 guidelines on AI ethics and responsible, trustworthy AI in circulation according to the AI Ethics Guidelines Global Inventory maintained by AlgorithmWatch, an organisation which tracks the effects of increased digitalisation on everyday lives. However, from the perspective of day-to-day work, for those engaged in designing, developing, and maintaining AI systems identifying relevant guidelines and translating them into practice presents a challenge.The aim of this paper is to help anyone engaged in building a responsible AI system by identifying an indicative long-list of potential stakeholders. This list of impacted stakeholders is intended to enable such AI system builders to decide which guidelines are most suited to their practice. The paper draws on a literature review of articles short-listed based on searches conducted in the ACM Digital Library and Google Scholar. The findings are based on content analysis of the short-listed literature guided by probes which draw on the ISO 26000:2010 Guidance on social responsibility.The paper identifies three levels of potentially relevant stakeholders when responsible AI systems are considered: individual stakeholders (including users, developers, and researchers), organisational stakeholders, and national / international stakeholders engaged in making laws, rules, and regulations. The main intended audience for this paper is software, requirements, and product engineers engaged in building AI systems. In addition, business executives, policy makers, legal/regulatory experts, AI researchers, public, private, and third sector organisations developing responsible AI guidelines, and anyone interested in seeing functional responsible AI systems are the other intended audience for this paper.",AI ethics; AI system builders; corporate social responsibility; ISO 26000:2010 guidance on social responsibility; responsible AI systems; stakeholder identification,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,227–236,AIES '22,Association for Computing Machinery,NM5B4798,0.0677290836653386,0.3684210526315789,0.1428571428571428,0.0940684265136795
146,146,Data Curation as Collective Action during COVID‐19,J. Assoc. Inf. Sci. Technol.,72.0,3,10.1002/asi.24406,"Shankar, Kalpana; Jeng, Wei; Thomer, Andrea; Weber, Nicholas; Yoon, Ayoung",2021.0,https://doi.org/10.1002/asi.24406,journalArticle,"In this commentary, the authors, an international group data curation researchers and educators, reflect on some of the challenges and opportunities for data curation in the wake of the COVID‐19 pandemic. We focus on some topics of particular interest to the information science community: data infrastructures for scholarly communication and research, the politicization of data curation and visualization for public‐facing “dashboards,” and human subjects research and policies. We conclude with some areas of opportunity and need, including broader and richer data curation education in the information schools, the establishment of better data management policy implementations by research funders, the award of formal academic credit for data curation activities and data sharing, and engagement in cooperative action around data ethics and security.",,,2330-1635,2021-02,2023-11-06 01:30:04,2023-11-06 01:30:04,280–284,,,JN5XWVX8,0.0909090909090909,0.0,0.1428571428571428,0.0937791490277678
147,147,"The AI Gambit: Leveraging Artificial Intelligence to Combat Climate Change—opportunities, Challenges, and Recommendations",AI Soc.,38.0,1,10.1007/s00146-021-01294-x,"Cowls, Josh; Tsamados, Andreas; Taddeo, Mariarosaria; Floridi, Luciano",2021.0,https://doi.org/10.1007/s00146-021-01294-x,journalArticle,"In this article, we analyse the role that artificial intelligence (AI) could play, and is playing, to combat global climate change. We identify two crucial opportunities that AI offers in this domain: it can help improve and expand current understanding of climate change, and it can contribute to combatting the climate crisis effectively. However, the development of AI also raises two sets of problems when considering climate change: the possible exacerbation of social and ethical challenges already associated with AI, and the contribution to climate change of the greenhouse gases emitted by training data and computation-intensive AI systems. We assess the carbon footprint of AI research, and the factors that influence AI’s greenhouse gas (GHG) emissions in this domain. We find that the carbon footprint of AI research may be significant and highlight the need for more evidence concerning the trade-off between the GHG emissions generated by AI research and the energy and resource efficiency gains that AI can offer. In light of our analysis, we argue that leveraging the opportunities offered by AI for global climate change whilst limiting its risks is a gambit which requires responsive, evidence-based, and effective governance to become a winning strategy. We conclude by identifying the European Union as being especially well-placed to play a leading role in this policy response and provide 13 recommendations that are designed to identify and harness the opportunities of AI for combatting climate change, while reducing its impact on the environment.",Digital ethics; Artificial intelligence; Carbon footprint; Climate change; Digital governance; Environment; Sustainability,,0951-5666,2021-10,2023-11-06 01:30:01,2023-11-06 01:30:01,283–307,,,4WPHP9IN,0.0617283950617283,0.5,0.0769230769230769,0.0921841727642021
148,148,"Democratising AI: Multiple Meanings, Goals, and Methods","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604693,"Seger, Elizabeth; Ovadya, Aviv; Siddarth, Divya; Garfinkel, Ben; Dafoe, Allan",2023.0,https://doi.org/10.1145/3600211.3604693,conferencePaper,"Numerous parties are calling for “the democratisation of AI”, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of “AI democratisation” that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to “democratising AI”, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.",AI Governance; AI Benefits; AI Democratisation; Misuse of AI; Model Sharing,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,715–722,AIES '23,Association for Computing Machinery,YTJVXS4A,0.0680272108843537,0.3636363636363636,0.1428571428571428,0.0918588815234327
149,149,Privacy Preserving Machine Learning Systems,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539530,"EL MESTARI, Soumia Zohra",2022.0,https://doi.org/10.1145/3514094.3539530,conferencePaper,"Machine learning(ML) tools are among the promising data-driven techniques that can help solve many real-life problems. However these tools rely on the collection of large volumes of data, which raises many privacy concerns and more broadly trustworthiness concerns. Privacy Preserving technologies aim at solving the issue by integrating privacy enhancing technologies (PETs) within the machine learning pipelines.",differential privacy; homomorphic encryption; secure multi-party computation,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,898,AIES '22,Association for Computing Machinery,B4R4CSFU,0.0701754385964912,0.1428571428571428,0.2,0.0910127100114263
150,150,What People Think AI Should Infer From Faces,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533080,"Engelmann, Severin; Ullstein, Chiara; Papakyriakopoulos, Orestis; Grossklags, Jens",2022.0,https://doi.org/10.1145/3531146.3533080,conferencePaper,"Faces play an indispensable role in human social life. At present, computer vision artificial intelligence (AI) captures and interprets human faces for a variety of digital applications and services. The ambiguity of facial information has recently led to a debate among scholars in different fields about the types of inferences AI should make about people based on their facial looks. AI research often justifies facial AI inference-making by referring to how people form impressions in first-encounter scenarios. Critics raise concerns about bias and discrimination and warn that facial analysis AI resembles an automated version of physiognomy. What has been missing from this debate, however, is an understanding of how “non-experts” in AI ethically evaluate facial AI inference-making. In a two-scenario vignette study with 24 treatment groups, we show that non-experts (N = 3745) reject facial AI inferences such as trustworthiness and likability from portrait images in a low-stake advertising and a high-stake hiring context. In contrast, non-experts agree with facial AI inferences such as skin color or gender in the advertising but not the hiring decision context. For each AI inference, we ask non-experts to justify their evaluation in a written response. Analyzing 29,760 written justifications, we find that non-experts are either “evidentialists” or “pragmatists”: they assess the ethical status of a facial AI inference based on whether they think faces warrant sufficient or insufficient evidence for an inference (evidentialist justification) or whether making the inference results in beneficial or detrimental outcomes (pragmatist justification). Non-experts’ justifications underscore the normative complexity behind facial AI inference-making. AI inferences with insufficient evidence can be rationalized by considerations of relevance while irrelevant inferences can be justified by reference to sufficient evidence. We argue that participatory approaches contribute valuable insights for the development of ethical AI in an increasingly visual data culture.",artificial intelligence; computer vision; human faces; participatory AI ethics,978-1-4503-9352-2,,2022,2023-11-06 01:30:02,2023-11-06 01:30:02,128–141,FAccT '22,Association for Computing Machinery,S3ZRYKFD,0.0808080808080808,0.3333333333333333,0.125,0.0906227810989715
151,151,The Role of Governance in Bridging AI Responsibility Gaps: An Interdisciplinary Evaluation of Emerging AI Governance Measures,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604751,"Ganesh, Bhargavi",2023.0,https://doi.org/10.1145/3600211.3604751,conferencePaper,"The ubiquitous use of AI in a wide range of domain areas, including health, finance, social media, and many others, along with the well-publicised harms and concerns around the use of these systems, has generated questions around who is responsible, in the normative sense, for the outcomes of increasingly autonomous systems. Scholars within the interdisciplinary field of AI Ethics have noted that AI poses challenges to the attribution of moral and legal responsibility, due to the diminished knowledge and control of individual actors involved in bringing about system outcomes, and the existence of “many hands”- or a diffuse network of individuals and collectives who could potentially be responsible [6]. In my research, I draw from conceptual approaches in philosophy, examples from the history of technology, and domain-specific qualitative case studies to examine the extent to which AI presents new challenges to responsibility attribution. In addition, my research evaluates the effectiveness of emerging organisational and regulatory governance measures in meeting the challenges posed by apparent responsibility gaps.",AI Governance; Applied Ethics; History of Technology; Political Economy of AI; Responsibility,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,952–954,AIES '23,Association for Computing Machinery,VQHRKTZ2,0.0542168674698795,0.4166666666666667,0.1764705882352941,0.0905323207100411
152,152,Artificial Intelligence and the Purpose of Social Systems,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462526,"Benthall, Sebastian; Goldenfein, Jake",2021.0,https://doi.org/10.1145/3461702.3462526,conferencePaper,"The law and ethics of Western democratic states have their basis in liberalism. This extends to regulation and ethical discussion of technology and businesses doing data processing. Liberalism relies on the privacy and autonomy of individuals, their ordering through a public market, and, more recently, a measure of equality guaranteed by the state. We argue that these forms of regulation and ethical analysis are largely incompatible with the techno-political and techno-economic dimensions of artificial intelligence. By analyzing liberal regulatory solutions in the form of privacy and data protection, regulation of public markets, and fairness in AI, we expose how the data economy and artificial intelligence have transcended liberal legal imagination. Organizations use artificial intelligence to exceed the bounded rationality of individuals and each other. This has led to the private consolidation of markets and an unequal hierarchy of control operating mainly for the purpose of shareholder value. An artificial intelligence will be only as ethical as the purpose of the social system that operates it. Inspired by the science of artificial life as an alternative to artificial intelligence, we consider data intermediaries: sociotechnical systems composed of individuals associated around collectively pursued purposes. An attention cooperative, that prioritizes its incoming and outgoing data flows, is one model of a social system that could form and maintain its own autonomous purpose.",artificial intelligence; privacy; cybernetics; economics; intermediaries; liberalism; platforms,978-1-4503-8473-5,,2021,2023-11-06 01:29:58,2023-11-06 01:29:58,3–12,AIES '21,Association for Computing Machinery,WMI48KPR,0.091324200913242,0.125,0.0,0.090112680056471
153,153,Artificial Quasi Moral Agency,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539549,"Semler, Jen",2022.0,https://doi.org/10.1145/3514094.3539549,conferencePaper,"My research explores interrelated theoretical and practical questions about artificial intelligence (AI) and moral decision-making by focusing on the concept of moral agency. I propose a distinction between quasi (non-sentient) moral agents and sentient moral agents, and I argue that the key features of moral agency are conceptually possible to instantiate without phenomenal consciousness. Next, I will situate AI within the conversation of moral agency, exploring whether AI does, or can, meet the criteria for quasi moral agency. In the future, I will look at the relationship between quasi moral agency and responsibility, how we ought to use AI in moral decision-making, and policy proposals for the development and use of artificial moral agents.",AI ethics; autonomy; moral agency; moral responsibility; moral understanding; phenomenal consciousness,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,913,AIES '22,Association for Computing Machinery,IJCVJPJ8,0.043859649122807,0.4545454545454545,0.0,0.0897557195505455
154,154,"AI and Us: Ethical Concerns, Public Knowledge and Public Attitudes on Artificial Intelligence","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539518,"Budic, Marina",2022.0,https://doi.org/10.1145/3514094.3539518,conferencePaper,"In the first part I present a theoretical study of ethical challenges arising from the development and application of AI, while in the second part I present an empirical study of public attitudes towards the various aspects of AI use, which I have conducted. Artificial intelligence (AI) describes systems that mimic human cognitive functions and that are designed to achieve specific goals, execute tasks, make decisions or achieve goals in complex situations autonomously.AI can greatly facilitate people's daily and business lives, but it also brings challenges, such as privacy, transparency, discrimination, job loss, and responsibility. One of the main challenges related to the adoption and implementation of AI is the current connotations and perceptions of the subject. To some people, AI is a mysterious concept, and it is difficult to understand how it manifests in their daily lives. The development and application of AI raise several ethical issues, including the interaction of AI with human rights such as privacy and discrimination [2]. Previous studies [1, 3-5] have examined some aspects of AI. In this study, I have employed a new scale to measure the general attitudes of the public towards AI.I have conducted an empirical study of public attitudes towards various aspects of the use of AI. The participants were the public in Serbia (N=737) who filled out an online questionnaire. The questionnaire consisted of general socio-demographic questions, questions about familiarity with AI, a brief introduction to the concept of AI, and questions about participants' attitudes towards the use of AI. Participants were asked to what degree they (dis)agree with the statements on the 5-point Likert scale. I have investigated whether age, education, previous knowledge of AI, profession, and level of religiosity influences people's attitudes towards AI. Objectives were to (1) examine the public's attitudes in Serbia toward the use of AI; (2) determine which factors influence these attitudes; (3) examine whether the public shares the concerns identified by ethical philosophers in the debates on AI; (4) examine prior knowledge of the public on AI. Data were analyzed in the R programming language. The general attitude towards the use of AI was assessed by creating a composite score based on the questions in which the respondents expressed their attitudes towards different aspects of the use of AI.The results showed that the public in Serbia has a divided opinion on the use of AI. Half of them have positive, and the other half negative attitudes. These attitudes are influenced by variables such as the respondents' age, education, profession, level of religiosity, and prior knowledge of AI, in a way that young, highly educated, non-religious, those with highly qualified jobs, especially IT professionals, and those who are more familiar with the concept of AI, have more positive attitudes towards the use of AI. Also, the results showed that the public is concerned about the disappearance of professions due to the development of AI and discrimination by AI systems. The results suggest a need to educate the public about challenges and ways to prevent them. It is necessary to increase transparency in decision-making processes related to the implementation of AI and dialogue between the public on the one hand and the state and the private sector on the other. I argue that considering different aspects of public attitudes toward AI enhances this debate. The results are valuable for future work on this topic because similar public opinion polls have not been conducted, especially not in Serbia. Public opinion has not shaped conversations about the use of AI. Another focus should be on further research of the attitudes of the public and experts on specific applications of AI and related ethical challenges. This research has implications for future research, particularly when forming an AI attitude scale.",AI; ethical concerns; knowledge; public attitudes,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,892,AIES '22,Association for Computing Machinery,4E48YYP8,0.0771704180064308,0.6666666666666666,0.3076923076923077,0.0894685245375007
155,155,Beyond the ML Model: Applying Safety Engineering Frameworks to Text-to-Image Development,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604685,"Rismani, Shalaleh; Shelby, Renee; Smart, Andrew; Delos Santos, Renelito; Moon, AJung; Rostamzadeh, Negar",2023.0,https://doi.org/10.1145/3600211.3604685,conferencePaper,"Identifying potential social and ethical risks in emerging machine learning (ML) models and their applications remains challenging. In this work, we applied two well-established safety engineering frameworks (FMEA, STPA) to a case study involving text-to-image models at three stages of the ML product development pipeline: data processing, integration of a T2I model with other models, and use. Results of our analysis demonstrate the safety frameworks – both of which are not designed explicitly examine social and ethical risks – can uncover failure and hazards that pose social and ethical risks. We discovered a broad range of failures and hazards (i.e., functional, social, and ethical) by analyzing interactions (i.e., between different ML models in the product, between the ML product and user, and between development teams) and processes (i.e., preparation of training data or workflows for using an ML service/product). Our findings underscore the value and importance of examining beyond an ML model in examining social and ethical risks, especially when we have minimal information about an ML model.",Art; Responsible ML; Safety engineering; T2I generative models,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,70–83,AIES '23,Association for Computing Machinery,U79KQDXC,0.1011904761904761,0.0,0.0,0.0891726049089469
156,156,A Multi-Agent Approach to Combine Reasoning and Learning for an Ethical Behavior,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462515,"Chaput, Rémy; Duval, Jérémy; Boissier, Olivier; Guillermin, Mathieu; Hassas, Salima",2021.0,https://doi.org/10.1145/3461702.3462515,conferencePaper,"The recent field of Machine Ethics is experiencing rapid growth to answer the societal need for Artificial Intelligence (AI) algorithms imbued with ethical considerations, such as benevolence toward human users and actors. Several approaches already exist for this purpose, mostly either by reasoning over a set of predefined ethical principles (Top-Down), or by learning new principles (Bottom-Up). While both methods have their own advantages and drawbacks, only few works have explored hybrid approaches, such as using symbolic rules to guide the learning process for instance, combining the advantages of each. This paper draws upon existing works to propose a novel hybrid method using symbolic judging agents to evaluate the ethics of learning agents' behaviors, and accordingly improve their ability to ethically behave in dynamic multi-agent environments. Multiple benefits ensue from this separation between judging and learning agents: agents can evolve (or be updated by human designers) separately, benefiting from co-construction processes; judging agents can act as accessible proxies for non-expert human stakeholders or regulators; and finally, multiple points of view (one per judging agent) can be adopted to judge the behavior of the same agent, which produces a richer feedback. Our proposed approach is applied to an energy distribution problem, in the context of a Smart Grid simulator, with continuous and multi-dimensional states and actions. The experiments and results show the ability of learning agents to correctly adapt their behaviors to comply with the judging agents' rules, including when rules evolve over time.",ethics; machine ethics; ethical judgment; hybrid neural-symbolic learning; multi-agent learning; reinforcement learning,978-1-4503-8473-5,,2021,2023-11-06 01:29:54,2023-11-06 01:29:54,13–23,AIES '21,Association for Computing Machinery,3ZZ3MVFU,0.0452674897119341,0.5833333333333334,0.25,0.0888492979580001
157,157,"Have a Break from Making Decisions, Have a MARS: The Multi-Valued Action Reasoning System","Artificial Intelligence XXXIX: 42nd SGAI International Conference on Artificial Intelligence, AI 2022, Cambridge, UK, December 13–15, 2022, Proceedings",,,10.1007/978-3-031-21441-7_31,"Badea, Cosmin",2022.0,https://doi.org/10.1007/978-3-031-21441-7_31,conferencePaper,"The Multi-valued Action Reasoning System (MARS) is an automated value-based ethical decision-making model for agents in Artificial Intelligence (AI). Given a set of available actions and an underlying moral paradigm, by employing MARS one can identify the ethically preferred action. It can be used to implement and model different ethical theories, different moral paradigms, as well as combinations of such, in the context of automated practical reasoning and normative decision analysis. It can also be used to model moral dilemmas and discover the moral paradigms that result in the desired outcomes therein. In this paper we give a condensed description of MARS, explain its uses, and comparatively place it in the existing literature.",AI ethics; Value alignment; Expert and knowledge-based systems; Intelligent agents; Logics for AI; Mathematical models for AI; Multi-criteria decision-making,978-3-031-21440-0,,2022,2023-11-06 01:30:03,2023-11-06 01:30:03,359–366,,Springer-Verlag,YRYEXWCX,0.0619469026548672,0.2631578947368421,0.0,0.0883940136290932
158,158,Hard Choices in Artificial Intelligence,Artif. Intell.,300.0,C,10.1016/j.artint.2021.103555,"Dobbe, Roel; Krendl Gilbert, Thomas; Mintz, Yonatan",2021.0,https://doi.org/10.1016/j.artint.2021.103555,journalArticle,"As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured.",AI ethics; AI governance; AI regulation; AI safety; Philosophy of artificial intelligence; Sociotechnical systems,,0004-3702,2021-11,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,3VQXTGY2,0.0591397849462365,0.4285714285714285,0.0,0.0878856480050875
159,159,Are We Nearly There Yet? A Desires &amp; Realities Framework for Europe’s AI Strategy,Information Systems Frontiers,25.0,1,10.1007/s10796-022-10285-2,"Polyviou, Ariana; Zamani, Efpraxia D.",2022.0,https://doi.org/10.1007/s10796-022-10285-2,journalArticle,"Of all emerging technologies, Artificial Intelligence (AI) is perhaps the most debated topic in contemporary society because it promises to redefine and disrupt several sectors. At the same time, AI poses challenges for policymakers and decision-makers, particularly regarding formulating strategies and regulations to address their stakeholders’ needs and perceptions. This paper explores stakeholder perceptions as expressed through their participation in the formulation of Europe’s AI strategy and sheds light on the challenges of AI in Europe and the expectations for the future. Our analysis reveals six dimensions towards an AI strategy; ecosystems, education, liability, data availability sufficiency &amp; protection, governance and autonomy. It draws on these dimensions to construct a desires-realities framework for AI strategy in Europe and provide a research agenda for addressing existing realities. Our findings contribute to understanding stakeholder desires on AI and hold important implications for research, practice and policymaking.",AI ethics; Artificial Intelligence; AI strategy; Technology policy; Technology regulation,,1387-3326,2022-06,2023-11-06 01:30:01,2023-11-06 01:30:01,143–159,,,XM8J97V3,0.0625,0.4,0.0714285714285714,0.0875785248953001
160,160,"Examining the Ethics of Brain-Computer Interfaces: Ensuring Safety, the Rights and Dignity of Personhood","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604733,"Mchia, Terkura Thomas",2023.0,https://doi.org/10.1145/3600211.3604733,conferencePaper,"Brain-Computer interfacing is one of the most interesting, digital health devices. It includes wearable, implantable, and injectable medical systems to improve or restore movement. It also includes machine-learning algorithms to help neurologically deficient persons to communicate and make decisions. All BCI applications connect the human brain to a machine that is external to the brain and the source of the self. Research and interest in Brain-Computer Interfaces have been developing at a rapid rate, with neuroscientists using BCI technology in an increasing range of applications. There are ethical questions that are lacking from the application of AI medically supported tools: safety assurance; human rights; the autonomy and dignity of the person who uses BCIs.",,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,982,AIES '23,Association for Computing Machinery,MZ64XZGB,0.0701754385964912,0.0,0.2142857142857142,0.0872649125664789
161,161,Towards Formalizing and Assessing AI Fairness,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604762,"Schmitz, Anna",2023.0,https://doi.org/10.1145/3600211.3604762,conferencePaper,"At the beginning of my PhD, I studied various AI ethics and trust- worthiness guidelines, including the HLEG Ethics Guidelines for Trustworthy AI [6] which I consider especially relevant from a European perspective. In [12], I summarize the motivation for trust- worthy Al and a set of trustworthiness dimensions that are con- sistently mentioned in these guidelines (ie., fairness, reliability, safety & security, transparency, data protection, autonomy & con- trol). While this set focuses on those requirements and risks which can be addressed by technical means in the AI system itself, it is noteworthy that various guidelines also refer to the way the or ganization (e.g., provider or operator) handles its Al applications (eg, post-market monitoring [2], »AI Ethics Review Board<< [6]). Therefore, in [12], I highlight two perspectives on trustworthy Al« and argue that an interplay of both is necessary in order to achieve and assure >trustworthy Al« in practice: the product and organiza- tional perspectives. I describe the essence of these perspectives as follows: i) high technical quality of Al systems is required, ii) the organization should make appropriate preparations (e.g., establish structures, processes and roles) to handle its Al applications and their development in a trustworthy manner. For each perspective, I",Trustworthy AI; AI Assessment; AI Fairness; AI Risk,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,999–1001,AIES '23,Association for Computing Machinery,UZK8SQDY,0.0686274509803921,0.5,0.1666666666666666,0.0870585498679583
162,162,Lean Privacy Review: Collecting Users’ Privacy Concerns of Data Practices at a Low Cost,ACM Trans. Comput.-Hum. Interact.,28.0,5,10.1145/3463910,"Jin, Haojian; Shen, Hong; Jain, Mayank; Kumar, Swarun; Hong, Jason I.",2021.0,https://doi.org/10.1145/3463910,journalArticle,"Today, industry practitioners (e.g., data scientists, developers, product managers) rely on formal privacy reviews (a combination of user interviews, privacy risk assessments, etc.) in identifying potential customer acceptance issues with their organization’s data practices. However, this process is slow and expensive, and practitioners often have to make ad-hoc privacy-related decisions with little actual feedback from users. We introduce Lean Privacy Review (LPR), a fast, cheap, and easy-to-access method to help practitioners collect direct feedback from users through the proxy of crowd workers in the early stages of design. LPR takes a proposed data practice, quickly breaks it down into smaller parts, generates a set of questionnaire surveys, solicits users’ opinions, and summarizes those opinions in a compact form for practitioners to use. By doing so, LPR can help uncover the range and magnitude of different privacy concerns actual people have at a small fraction of the cost and wait-time for a formal review. We evaluated LPR using 12 real-world data practices with 240 crowd users and 24 data practitioners. Our results show that (1) the discovery of privacy concerns saturates as the number of evaluators exceeds 14 participants, which takes around 5.5 hours to complete (i.e., latency) and costs 3.7 hours of total crowd work ( $80 in our experiments); and (2) LPR finds 89% of privacy concerns identified by data practitioners as well as 139% additional privacy concerns that practitioners are not aware of, at a 6% estimated false alarm rate.",data ethics; heuristic evaluation; Privacy concern; privacy engineering,,1073-0516,2021-08,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,ASW3V8FI,0.0537190082644628,0.625,0.2142857142857142,0.0853124345895335
163,163,The Development of Teaching Case Studies to Explore Ethical Issues Associated with Computer Programming: Four Case Studies on Programming Ethics.,Proceedings of the 2021 Conference on United Kingdom &amp; Ireland Computing Education Research,,,10.1145/3481282.3481293,"Gordon, Damian; Collins, Michael; O'Sullivan, Dympna",2021.0,https://doi.org/10.1145/3481282.3481293,conferencePaper,"In the past decade software products have become pervasive in many aspects of people's lives around the world. Unfortunately, the quality of the experience an individual has interacting with that software is dependent on the quality of the software itself, and it is becoming more and more evident that many large software products contain a range of issues and errors, and these issues are not known to the developers of these systems, and they are unaware of the deleterious impacts of those issues on the individuals who use these systems. The authors of this paper are developing a new digital ethics curriculum for the instruction of computer science students. In this paper we present case studies that were explored to demonstrate programming issues to First Year Computer Science students. Each case study outlines key issues associated with a particular scenario and is accompanied by specific questions to be used by the instructor to allow students to begin to reflect on, and evaluate, the implications of these issues. The objective of this teaching content is to ensure that the students are presented with, and engage with, ethical considerations early in their studies and well before they encounter them in an employment setting.",Ethical Case studies; Programming Ethics; Programming Issues,978-1-4503-8568-8,,2021,2023-11-06 01:30:04,2023-11-06 01:30:04,,UKICER '21,Association for Computing Machinery,29PWHRFY,0.0348258706467661,0.7142857142857143,0.25,0.0844246572591409
164,164,"The Theory, Practice, and Ethical Challenges of Designing a Diversity-Aware Platform for Social Relations","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462595,"Schelenz, Laura; Bison, Ivano; Busso, Matteo; de Götzen, Amalia; Gatica-Perez, Daniel; Giunchiglia, Fausto; Meegahapola, Lakmal; Ruiz-Correa, Salvador",2021.0,https://doi.org/10.1145/3461702.3462595,conferencePaper,"Diversity-aware platform design is a paradigm that responds to the ethical challenges of existing social media platforms. Available platforms have been criticized for minimizing users' autonomy, marginalizing minorities, and exploiting users' data for profit maximization. This paper presents a design solution that centers the well-being of users. It presents the theory and practice of designing a diversity-aware platform for social relations. In this approach, the diversity of users is leveraged in a way that allows like-minded individuals to pursue similar interests or diverse individuals to complement each other in a complex activity. The end users of the envisioned platform are students, who participate in the design process. Diversity-aware platform design involves numerous steps, of which two are highlighted in this paper: 1) defining a framework and operationalizing the ""diversity"" of students, 2) collecting ""diversity"" data to build diversity-aware algorithms. The paper further reflects on the ethical challenges encountered during the design of a diversity-aware platform.",diversity; diversity-aware technology; ethics; platform; social media,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,905–915,AIES '21,Association for Computing Machinery,G26DK5AR,0.0580645161290322,0.2857142857142857,0.2142857142857142,0.0833687398546698
165,165,Artificial Concepts of Artificial Intelligence: Institutional Compliance and Resistance in AI Startups,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534138,"Winecoff, Amy A.; Watkins, Elizabeth Anne",2022.0,https://doi.org/10.1145/3514094.3534138,conferencePaper,"Scholars and industry practitioners have debated how to best develop interventions for ethical artificial intelligence (AI). Such interventions recommend that companies building and using AI tools change their technical practices, but fail to wrangle with critical questions about the organizational and institutional context in which AI is developed. In this paper, we contribute descriptive research around the life of ""AI"" as a discursive concept and organizational practice in an understudied sphere–emerging AI startups–and with a focus on extra-organizational pressures faced by entrepreneurs. Leveraging a theoretical lens for how organizations change, we conducted semi-structured interviews with 23 entrepreneurs working at early-stage AI startups. We find that actors within startups both conform to and resist institutional pressures. Our analysis identifies a central tension for AI entrepreneurs: they often valued scientific integrity and methodological rigor; however, influential external stakeholders either lacked the technical knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused on business priorities. As a result, entrepreneurs adopted hyped marketing messages about AI that diverged from their scientific values, but attempted to preserve their legitimacy internally. Institutional pressures and organizational constraints also influenced entrepreneurs' modeling practices and their response to actual or impending regulation. We conclude with a discussion for how such pressures could be used as leverage for effective interventions towards building ethical AI.",artificial intelligence; ethical systems; industry practice; organizational theory; qualitative methods,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,788–799,AIES '22,Association for Computing Machinery,42PVP9CT,0.0694444444444444,0.3,0.0833333333333333,0.0833088460361187
166,166,"Blind-Sided by Privacy? Digital Contact Tracing, the Apple/Google API and Big Tech’s Newfound Role as Global Health Policy Makers",Ethics and Inf. Technol.,23.0,Suppl 1,10.1007/s10676-020-09547-x,"Sharon, Tamar",2021.0,https://doi.org/10.1007/s10676-020-09547-x,journalArticle,"Since the outbreak of COVID-19, governments have turned their attention to digital contact tracing. In many countries, public debate has focused on the risks this technology poses to privacy, with advocates and experts sounding alarm bells about surveillance and mission creep reminiscent of the post 9/11 era. Yet, when Apple and Google launched their contact tracing API in April 2020, some of the world’s leading privacy experts applauded this initiative for its privacy-preserving technical specifications. In an interesting twist, the tech giants came to be portrayed as greater champions of privacy than some democratic governments. This article proposes to view the Apple/Google API in terms of a broader phenomenon whereby tech corporations are encroaching into ever new spheres of social life. From this perspective, the (legitimate) advantage these actors have accrued in the sphere of the production of digital goods provides them with (illegitimate) access to the spheres of health and medicine, and more worrisome, to the sphere of politics. These sphere transgressions raise numerous risks that are not captured by the focus on privacy harms. Namely, a crowding out of essential spherical expertise, new dependencies on corporate actors for the delivery of essential, public goods, the shaping of (global) public policy by non-representative, private actors and ultimately, the accumulation of decision-making power across multiple spheres. While privacy is certainly an important value, its centrality in the debate on digital contact tracing may blind us to these broader societal harms and unwittingly pave the way for ever more sphere transgressions.",Digital ethics; Privacy; Justice; COVID-19; Digital contact tracing; Big tech,,1388-1957,2021-11,2023-11-06 01:30:01,2023-11-06 01:30:01,45–57,,,FSXKX84H,0.044,0.8,0.1578947368421052,0.0829031062180531
167,167,Ethical Obligations to Provide Novelty,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462555,"Golden, Paige; Danks, David",2021.0,https://doi.org/10.1145/3461702.3462555,conferencePaper,"TikTok is a popular platform that enables users to see tailored content feeds, particularly short videos with novel content. In recent years, TikTok has been criticized at times for presenting users with overly homogenous feeds, thereby reducing the diversity of content with which each user engages. In this paper, we consider whether TikTok has an ethical obligation to employ a novelty bias in its content recommendation engine. We explicate the principal morally relevant values and interests of key stakeholders, and observe that key empirical questions must be answered before a precise recommendation can be provided. We argue that TikTok's own values and interests mean that its actions should be largely driven by the values and interests of its users and creators. Unlike some other content platforms, TikTok's ethical obligations are not at odds with the values of its users, and so whether it is obligated to include a novelty bias depends on what will actually advance its users' interests.",recommender systems; ethical obligations; interest-based analysis; tiktok,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,502–508,AIES '21,Association for Computing Machinery,5CHCLXSZ,0.0377358490566037,0.4285714285714285,0.6,0.082526724317314
168,168,Learning in the Panopticon: Examining the Potential Impacts of AI Monitoring on Students,Proceedings of the 34th Australian Conference on Human-Computer Interaction,,,10.1145/3572921.3572937,"Han, Bingyi; Buchanan, George; Mckay, Dana",2023.0,https://doi.org/10.1145/3572921.3572937,conferencePaper,"In a panopticon, people are intrusively monitored across all areas of their lives. AI monitoring has been ever more widely adopted in education, with increasingly intrusive monitoring of students. These changes potentially create ethical harms, but current ethical discussions predominantly focus on legal and governance issues. The concerns of the majority of users—namely students—are neglected. Overlooking students’ concerns further increases their vulnerability. We use a student-centred and speculative approach through the Story Completion Method (SCM) to explore how students would potentially respond to intrusive AI monitoring in a higher education setting. Our study included 71 participants who elaborated on the story stems we provided to them. Through a blending of thematic analysis coding and the techniques of developing grounded theory, we reveal that the common responses of students to extensive AI monitoring included impacts on personal psychology, changed behaviour, and cognition. There are likely major disruptions to personal autonomy, identity and educational relationships. If we are to avoid a future ‘big brother’ classroom, further investigations using HCI methods are critical to understanding how to protect students in AI-dominated learning.",Surveillance; Privacy; AI Ethics; AI Monitoring; Learning Analytics; Story Completion Method,9798400700248,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,9–21,OzCHI '22,Association for Computing Machinery,DJHKYEUY,0.0558659217877094,0.4545454545454545,0.0769230769230769,0.0823480844444963
169,169,Why We Need to Know More: Exploring the State of AI Incident Documentation Practices,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604700,"Turri, Violet; Dzombak, Rachel",2023.0,https://doi.org/10.1145/3600211.3604700,conferencePaper,"To enable the development and use of safe and equitable artificial intelligence (AI) systems, AI engineers must monitor deployed AI systems and learn from past AI incidents where failures have occurred. Around the world, public databases for cataloging AI systems and resulting harms are instrumental in promoting awareness of potential AI harms among policymakers, researchers, and the public. However, despite growing recognition of the potential of AI systems to produce harms, causes of AI systems failure remain elusive and AI incidents continue to occur. For example, incidents of AI bias are frequently reported and discussed, yet biased systems continue to be developed and deployed. This raises the question – how are we learning from documented incidents? What information do we need to analyze AI incidents and develop new AI engineering best practices? This paper examines reporting techniques from a variety of AI stakeholders and across different industries, identifies requirements towards the design of effective AI incident documentation, and proposes policy recommendations for augmenting current practice.",Explainable Artificial Intelligence,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,576–583,AIES '23,Association for Computing Machinery,NYIJZ5Y6,0.0848484848484848,0.0,0.0714285714285714,0.0815524513030546
170,170,Anything New under the Sun? Insights from a History of Institutionalized AI Ethics,Ethics and Inf. Technol.,25.0,2,10.1007/s10676-023-09702-0,"Casiraghi, Simone",2023.0,https://doi.org/10.1007/s10676-023-09702-0,journalArticle,"Scholars, policymakers and organizations in the EU, especially at the level of the European Commission, have turned their attention to the ethics of (trustworthy and human-centric) Artificial Intelligence (AI). However, there has been little reflexivity on (1) the history of the ethics of AI as an institutionalized phenomenon and (2) the comparison to similar episodes of “ethification” in other fields, to highlight common (unresolved) challenges.Contrary to some mainstream narratives, which stress how the increasing attention to ethical aspects of AI is due to the fast pace and increasing risks of technological developments, Science and Technology Studies(STS)-informed perspectives highlight that the rise of institutionalized assessment methods indicates a need for governments to gain more control of scientific research and to bring EU institutions closer to the public on controversies related to emerging technologies.This article analyzes how different approaches of the recent past (i.e. bioethics, technology assessment (TA) and ethical, legal and social (ELS) research, Responsible Research and Innovation (RRI)) followed one another, often “in the name of ethics”, to address previous criticisms and/or to legitimate certain scientific and technological research programs. The focus is on how a brief history of the institutionalization of these approaches can provide insights into present challenges to the ethics of AI related to methodological issues, mobilization of expertise and public participation.",Artificial Intelligence; Bioethics; European Commission; Institutionalization; Public participation; Responsible Research and Innovation; Science and Technology Studies; Technology Assessment,,1388-1957,2023-04,2023-11-06 01:29:49,2023-11-06 01:29:49,,,,IENTRS5R,0.0837209302325581,0.0,0.2307692307692307,0.0815348618443361
171,171,Human-Centered Responsible Artificial Intelligence: Current&nbsp;&amp;&nbsp;Future&nbsp;Trends,Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems,,,10.1145/3544549.3583178,"Tahaei, Mohammad; Constantinides, Marios; Quercia, Daniele; Kennedy, Sean; Muller, Michael; Stumpf, Simone; Liao, Q. Vera; Baeza-Yates, Ricardo; Aroyo, Lora; Holbrook, Jess; Luger, Ewa; Madaio, Michael; Blumenfeld, Ilana Golbin; De-Arteaga, Maria; Vitak, Jessica; Olteanu, Alexandra",2023.0,https://doi.org/10.1145/3544549.3583178,conferencePaper,"In recent years, the CHI community has seen significant growth in research on Human-Centered Responsible Artificial Intelligence. While different research communities may use different terminology to discuss similar topics, all of this work is ultimately aimed at developing AI that benefits humanity while being grounded in human rights and ethics, and reducing the potential harms of AI. In this special interest group, we aim to bring together researchers from academia and industry interested in these topics to map current and future research trends to advance this important area of research by fostering collaboration and sharing ideas.",AI ethics; responsible AI; human-centered AI,978-1-4503-9422-2,,2023,2023-11-06 01:30:00,2023-11-06 01:30:00,,CHI EA '23,Association for Computing Machinery,QPYD92A9,0.0416666666666666,0.8333333333333334,0.0,0.0812526675202731
172,172,A Principle-Based Approach to AI: The Case for European Union and Italy,AI Soc.,38.0,2,10.1007/s00146-022-01453-8,"Corea, Francesco; Fossa, Fabio; Loreggia, Andrea; Quintarelli, Stefano; Sapienza, Salvatore",2022.0,https://doi.org/10.1007/s00146-022-01453-8,journalArticle,"As Artificial Intelligence (AI) becomes more and more pervasive in our everyday life, new questions arise about its ethical and social impacts. Such issues concern all stakeholders involved in or committed to the design, implementation, deployment, and use of the technology. The present document addresses these preoccupations by introducing and discussing a set of practical obligations and recommendations for the development of applications and systems based on AI techniques. With this work we hope to contribute to spreading awareness on the many social challenges posed by AI and encouraging the establishment of good practices throughout the relevant social areas. As points of novelty, the paper elaborates on an integrated view that combines both human rights and ethical concepts to reap the benefits of the two approaches. Moreover, it proposes innovative recommendations, such as those on redress and governance, which add further insight to the debate. Finally, it incorporates a specific focus on the Italian Constitution, thus offering an example of how core legislations of Member States might contribute to further specify and enrich the EU normative framework on AI.",AI Ethics; Principles; AI Governance; Recommendations; Values,,0951-5666,2022-05,2023-11-06 01:30:00,2023-11-06 01:30:00,521–535,,,DNGKP9RH,0.0558659217877094,0.5714285714285714,0.0833333333333333,0.0812262162811277
173,173,Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462589,"Afnan, Michael Anis Mihdi; Rudin, Cynthia; Conitzer, Vincent; Savulescu, Julian; Mishra, Abhishek; Liu, Yanhe; Afnan, Masoud",2021.0,https://doi.org/10.1145/3461702.3462589,conferencePaper,"AI has the potential to revolutionize many areas of healthcare. Radiology, dermatology, and ophthalmology are some of the areas most likely to be impacted in the near future, and they have received significant attention from the broader research community. But AI techniques are now also starting to be used in in vitro fertilization (IVF), in particular for selecting which embryos to transfer to the woman. The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people. We first give a brief introduction to IVF and review the use of AI for embryo selection. We discuss concerns with the interpretation of the reported results from scientific and practical perspectives. We then consider the broader ethical issues involved. We discuss in detail the problems that result from the use of black-box methods in this context and advocate strongly for the use of interpretable models. Importantly, there have been no published trials of clinical effectiveness, a problem in both the AI and IVF communities, and we therefore argue that clinical implementation at this point would be premature. Finally, we discuss ways for the broader AI community to become involved to ensure scientifically sound and ethically responsible development of AI in IVF.",artificial intelligence; ethics; AI; machine learning; black-box; embryo selection; in vitro fertilization; interpretable; IVF; randomised controlled trials; RCT,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,316–326,AIES '21,Association for Computing Machinery,Z4LQ5DRG,0.0596330275229357,0.1666666666666666,0.25,0.0809878834780138
174,174,Beyond Bias and Discrimination: Redefining the AI Ethics Principle of Fairness in Healthcare Machine-Learning Algorithms,AI Soc.,38.0,2,10.1007/s00146-022-01455-6,"Giovanola, Benedetta; Tiribelli, Simona",2022.0,https://doi.org/10.1007/s00146-022-01455-6,journalArticle,"The increasing implementation of and reliance on machine-learning (ML) algorithms to perform tasks, deliver services and make decisions in health and healthcare have made the need for fairness in ML, and more specifically in healthcare ML algorithms (HMLA), a very important and urgent task. However, while the debate on fairness in the ethics of artificial intelligence (AI) and in HMLA has grown significantly over the last decade, the very concept of fairness as an ethical value has not yet been sufficiently explored. Our paper aims to fill this gap and address the AI ethics principle of fairness from a conceptual standpoint, drawing insights from accounts of fairness elaborated in moral philosophy and using them to conceptualise fairness as an ethical value and to redefine fairness in HMLA accordingly. To achieve our goal, following a first section aimed at clarifying the background, methodology and structure of the paper, in the second section, we provide an overview of the discussion of the AI ethics principle of fairness in HMLA and show that the concept of fairness underlying this debate is framed in purely distributive terms and overlaps with non-discrimination, which is defined in turn as the absence of biases. After showing that this framing is inadequate, in the third section, we pursue an ethical inquiry into the concept of fairness and argue that fairness ought to be conceived of as an ethical value. Following a clarification of the relationship between fairness and non-discrimination, we show that the two do not overlap and that fairness requires much more than just non-discrimination. Moreover, we highlight that fairness not only has a distributive but also a socio-relational dimension. Finally, we pinpoint the constitutive components of fairness. In doing so, we base our arguments on a renewed reflection on the concept of respect, which goes beyond the idea of equal respect to include respect for individual persons. In the fourth section, we analyse the implications of our conceptual redefinition of fairness as an ethical value in the discussion of fairness in HMLA. Here, we claim that fairness requires more than non-discrimination and the absence of biases as well as more than just distribution; it needs to ensure that HMLA respects persons both as persons and as particular individuals. Finally, in the fifth section, we sketch some broader implications and show how our inquiry can contribute to making HMLA and, more generally, AI promote the social good and a fairer society.",Bias; Discrimination; Ethics of algorithms; Fairness; Healthcare machine-learning algorithms; Respect,,0951-5666,2022-05,2023-11-06 01:29:49,2023-11-06 01:29:49,549–563,,,FVE4E4VS,0.0618811881188118,0.3,0.2666666666666666,0.0795370550126908
175,175,Unpicking Epistemic Injustices in Digital Health: On the Implications of Designing Data-Driven Technologies for the Management of Long-Term Conditions,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604684,"Bennett, SJ; Claisse, Caroline; Luger, Ewa; Durrant, Abigail C.",2023.0,https://doi.org/10.1145/3600211.3604684,conferencePaper,"Applications of Artificial Intelligence (AI) in the domain of Personal Health Informatics (PHI) offer potential avenues for personalised treatment and support for people living with long-term conditions, however, they also present a number of ethical challenges. Whilst participatory approaches can help mitigate concerns by actively involving healthcare professionals, patients, and other stakeholders in design and development, these are constrained by the limits of epistemic standpoints and the risks posed by extrapolation from individuals to groups. In this paper we draw upon interviews with stakeholders involved in Human Immunodeficiency Virus (HIV) care, including clinicians, insurance providers and pharmaceutical industry representatives, to map intentions and ethical considerations for developing PHI tools for people living with HIV. Whilst treatment efficacy for HIV has improved patient quality of life and life expectancy, management and care is complicated by knowledge gaps about what living and ageing with HIV entails. We investigate how the critical concept of epistemic injustice can inform the design of data-driven technologies intended to address these gaps, helping orient expert perspectives within the broader structures and socio-historical influences that shape them. This is of particular importance when designing for marginalized populations such as people with HIV (i.e. who may experience social stigma and be under-resourced, managing multiple conditions), helping to identify and better account for fundamental ethical considerations such as equity.",AI Ethics; Critical Digital Health; Data Justice; Epistemic Injustice; Personal Health Informatics,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,322–332,AIES '23,Association for Computing Machinery,ZAZ446ZA,0.045662100456621,0.5833333333333334,0.1052631578947368,0.0792124213263189
176,176,Principles for AI Education for Elementary Grades Students,Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 2,,,10.1145/3502717.3532143,"Ottenbreit-Leftwich, Anne; Glazewski, Krista; Jeon, Minji; Jantaraweragul, Katie; Hmelo-Silver, Cindy; Scribner, Adam; Lee, Seung; Mott, Bradford; Lester, James",2022.0,https://doi.org/10.1145/3502717.3532143,conferencePaper,"AI is beginning to transform every aspect of society. With the dramatic increases in AI, K-12 students need to be prepared to understand AI. To succeed as the workers, creators, and innovators of the future, students must be introduced to core concepts of AI as early as elementary school. However, building a curriculum that introduces AI content to K-12 students present significant challenges, such as connecting to prior knowledge, and developing curricula that are meaningful for students and possible for teachers to teach. To lay the groundwork for elementary AI education, we conducted a qualitative study into the design of AI curricular approaches with elementary teachers and students. Interviews with elementary teachers and students suggests four design principles for creating an effective elementary AI curriculum to promote uptake by teachers. This example will present the co-designed curriculum with teachers (PRIMARYAI) and describe how these four elements were incorporated into real-world problem-based learning scenarios.",ai ethics; elementary education; k-12 ai education; teacher co-design,978-1-4503-9200-6,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,627,ITiCSE '22,Association for Computing Machinery,5C3EIB44,0.0522875816993464,0.4444444444444444,0.125,0.0790193285291324
177,177,Detecting Discriminatory Risk through Data Annotation Based on Bayesian Inferences,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445940,"Beretta, Elena; Vetrò, Antonio; Lepri, Bruno; Martin, Juan Carlos De",2021.0,https://doi.org/10.1145/3442188.3445940,conferencePaper,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination.",data ethics; machine learning; data labeling; human annotation; race discrimination; sampling bias,978-1-4503-8309-7,,2021,2023-11-06 01:30:02,2023-11-06 01:30:02,794–804,FAccT '21,Association for Computing Machinery,PA5F7H23,0.0523809523809523,0.4166666666666667,0.1,0.0784195861118938
178,178,AI Meets AI: Artificial Intelligence and Academic Integrity - A Survey on Mitigating AI-Assisted Cheating in Computing Education,Proceedings of the 24th Annual Conference on Information Technology Education,,,10.1145/3585059.3611449,"Xie, Ying; Wu, Shaoen; Chakravarty, Sumit",2023.0,https://doi.org/10.1145/3585059.3611449,conferencePaper,"This paper discusses pressing issues in the area where Artificial Intelligence (AI) meets Academic Integrity (AI). It starts by outlining the potential consequences of AI-assisted cheating, including the risks posed to education quality, fairness, and the credibility of academic institutions. After reviewing an array of strategies reported in the literature to counteract such cheating, this paper calls for rigorous research to assess the effectiveness of those strategies. It further suggests a range of research topics in detecting AI-generated content and highlights a promising research direction focusing on motivating students’ interest in learning through innovative AI applications that divert their efforts away from misuse of technology. Lastly, the paper suggests that addressing AI cheating requires ethical education, academia-industry collaboration, integration into AI ethics, and an international consortium. One of the unique contributions of this paper is outlining a range of potential research directions, both technical and non-technical, in this area where AI meets AI.",Artificial Intelligence; Academic Integrity; Cheating; Computing Education,9798400701306,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,79–83,SIGITE '23,Association for Computing Machinery,NR3UBV9U,0.0784313725490196,0.0,0.1111111111111111,0.0771806665052852
179,179,A Systematic Review of Ethical Concerns with Voice Assistants,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604679,"Seymour, William; Zhan, Xiao; Coté, Mark; Such, Jose",2023.0,https://doi.org/10.1145/3600211.3604679,conferencePaper,"Since Siri’s release in 2011 there have been a growing number of AI-driven domestic voice assistants that are increasingly being integrated into devices such as smartphones and TVs. But as their presence has expanded, a range of ethical concerns have been identified around the use of voice assistants, such as the privacy implications of having devices that are always listening and the ways that these devices are integrated into the existing social order of the home. This has created a burgeoning area of research across a range of fields including computer science, social science, and psychology. This paper takes stock of the foundations and frontiers of this work through a systematic literature review of 117 papers on ethical concerns with voice assistants. In addition to analysis of nine specific areas of concern, the review measures the distribution of methods and participant demographics across the literature. We show how some concerns, such as privacy, are operationalized to a much greater extent than others like accessibility, and how study participants are overwhelmingly drawn from a small handful of Western nations. In so doing we hope to provide an outline of the rich tapestry of work around these concerns and highlight areas where current research efforts are lacking.",accountability; transparency; autonomy; misinformation; privacy; accessibility; agency; conflict of interest; ethical concerns; performance of gender; social interaction; social order; Voice assistants,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,131–145,AIES '23,Association for Computing Machinery,BZZVEW8A,0.0390243902439024,0.238095238095238,0.3333333333333333,0.0762059719928272
180,180,Do Humans Trust Advice More If It Comes from AI? An Analysis of Human-AI Interactions,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534150,"Vodrahalli, Kailas; Daneshjou, Roxana; Gerstenberg, Tobias; Zou, James",2022.0,https://doi.org/10.1145/3514094.3534150,conferencePaper,"In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user. The user may ignore this advice or take it into consideration to modify their decision. With the increasing prevalence of such human-AI interactions, it is important to understand how users react to AI advice. In this paper, we recruited over 1100 crowdworkers to characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants' beliefs about how human versus AI performance on a given task affects whether they heed the advice. When participants do heed the advice, they use it similarly for human and AI suggestions. Based on these results, we propose a two-stage, ""activation-integration"" model for human behavior and use it to characterize the factors that affect human-AI interactions.",artificial intelligence; human-in-the-loop; ai advice; ai trust; human interaction with ai,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,763–777,AIES '22,Association for Computing Machinery,PLNLF8Z9,0.0422535211267605,0.3636363636363636,0.1333333333333333,0.0762037432225225
181,181,Limits of Individual Consent and Models of Distributed Consent in Online Social Networks,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3534640,"Lovato, Juniper L.; Allard, Antoine; Harp, Randall; Onaolapo, Jeremiah; Hébert-Dufresne, Laurent",2022.0,https://doi.org/10.1145/3531146.3534640,conferencePaper,"Personal data are not discrete in socially-networked digital environments. A user who consents to allow access to their profile can expose the personal data of their network connections to non-consented access. Therefore, the traditional consent model (informed and individual) is not appropriate in social networks where informed consent may not be possible for all users affected by data processing and where information is distributed across users. Here, we outline the adequacy of consent for data transactions. Informed by the shortcomings of individual consent, we introduce both a platform-specific model of “distributed consent” and a cross-platform model of a “consent passport.” In both models, individuals and groups can coordinate by giving consent conditional on that of their network connections. We simulate the impact of these distributed consent models on the observability of social networks and find that low adoption would allow macroscopic subsets of networks to preserve their connectivity and privacy.",Data ethics; Data privacy; Limits of consent; Privacy models; Social networks,978-1-4503-9352-2,,2022,2023-11-06 01:30:02,2023-11-06 01:30:02,2251–2262,FAccT '22,Association for Computing Machinery,FJQ62GKG,0.0466666666666666,0.5454545454545454,0.0,0.0755144379029997
182,182,The Challenges of AI Implementation in the Public Sector. An in-Depth Case Studies Analysis,Proceedings of the 24th Annual International Conference on Digital Government Research,,,10.1145/3598469.3598516,"Tangi, Luca; van Noordt, Colin; Rodriguez Müller, A. Paula",2023.0,https://doi.org/10.1145/3598469.3598516,conferencePaper,"Time is now mature for researching AI implementation in the public sector, creating knowledge from real-life settings. The current paper goes in this direction, aiming to explore the challenges public organizations face in implementing AI. The research has been conducted through eight in-depth case studies of AI solutions. As a theoretical background, we relied on a framework proposed by Wirtz et al. [36] that identified four classes of challenges: AI Society, AI Ethics, AI Law and Regulations, and AI Technology Implementation. Our results first confirm the importance of the four classes of challenges. Second, they highlight the need to add a fifth class of challenges, i.e., AI Organizational change. In fact, public organizations are facing important challenges in settling AI solutions in daily operations, practices, tasks, etc. Finally, the five classes have been discussed, including more detailed insights extracted from the coding of the cases.",,9798400708374,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,414–422,DGO '23,Association for Computing Machinery,MXB2MD42,0.0758620689655172,0.0,0.0714285714285714,0.0754781981036123
183,183,Friendly AI,Ethics and Inf. Technol.,23.0,3,10.1007/s10676-020-09556-w,"Fröding, Barbro; Peterson, Martin",2021.0,https://doi.org/10.1007/s10676-020-09556-w,journalArticle,"In this paper we discuss what we believe to be one of the most important features of near-future AIs, namely their capacity to behave in a friendly manner to humans. Our analysis of what it means for an AI to behave in a friendly manner does not presuppose that proper friendships between humans and AI systems could exist. That would require reciprocity, which is beyond the reach of near-future AI systems. Rather, we defend the claim that social AIs should be programmed to behave in a manner that mimics a sufficient number of aspects of proper friendship. We call this “as-if friendship”. The main reason for why we believe that ‘as if friendship’ is an improvement on the current, highly submissive behavior displayed by AIs is the negative effects the latter can have on humans. We defend this view partly on virtue ethical grounds and we argue that the virtue-based approach to AI ethics outlined in this paper, which we call “virtue alignment”, is an improvement on the traditional “value alignment” approach.",AI; Value alignment; Friend; Friendly; Virtue ethics,,1388-1957,2021-09,2023-11-06 01:30:03,2023-11-06 01:30:03,207–214,,,BFP5EGX9,0.0523255813953488,0.4285714285714285,0.5,0.0749797929196849
184,184,"Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Data Management, and Ethical Challenges",Comput. Sci. Rev.,43.0,C,10.1016/j.cosrev.2021.100452,"Ahmad, Kashif; Maabreh, Majdi; Ghaly, Mohamed; Khan, Khalil; Qadir, Junaid; Al-Fuqaha, Ala",2022.0,https://doi.org/10.1016/j.cosrev.2021.100452,journalArticle,"As the globally increasing population drives rapid urbanization in various parts of the world, there is a great need to deliberate on the future of the cities worth living. In particular, as modern smart cities embrace more and more data-driven artificial intelligence services, it is worth remembering that (1) technology can facilitate prosperity, wellbeing, urban livability, or social justice, but only when it has the right analog complements (such as well-thought out policies, mature institutions, responsible governance); and (2) the ultimate objective of these smart cities is to facilitate and enhance human welfare and social flourishing. Researchers have shown that various technological business models and features can in fact contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In the light of these observations, addressing the philosophical and ethical questions involved in ensuring the security, safety, and interpretability of such AI algorithms that will form the technological bedrock of future cities assumes paramount importance. Globally there are calls for technology to be made more humane and human-centered. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical (data and algorithmic) challenges to a successful deployment of AI in human-centric applications, with a particular emphasis on the convergence of these concepts/challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions. We believe such rigorous analysis will provide a baseline for future research in the domain.",AI ethics; Privacy; Security; Smart cities; Explainability; Machine learning; Adversarial attacks; Data auditing; Data bias; Data management; Data ownership; Evasion attacks; Interpretability; Trojan attacks,,1574-0137,2022-02,2023-11-06 01:30:03,2023-11-06 01:30:03,,,,QTWVGVZY,0.0354609929078014,0.3333333333333333,0.25,0.0749647905034958
185,185,What Does It Mean to Be a Responsible AI Practitioner: An Ontology of Roles and Skills,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604702,"Rismani, Shalaleh; Moon, AJung",2023.0,https://doi.org/10.1145/3600211.3604702,conferencePaper,"With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.",Competency Framework; Education; Responsible AI Practitioner,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,584–595,AIES '23,Association for Computing Machinery,PQSJCRWD,0.0714285714285714,0.1666666666666666,0.0625,0.0749541798187557
186,186,“I Would Like to Design”: Black Girls Analyzing and Ideating Fair and Accountable AI,Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems,,,10.1145/3544548.3581378,"Solyst, Jaemarie; Xie, Shixian; Yang, Ellia; Stewart, Angela E.B.; Eslami, Motahhare; Hammer, Jessica; Ogan, Amy",2023.0,https://doi.org/10.1145/3544548.3581378,conferencePaper,"Artificial intelligence (AI) literacy is especially important for those who may not be well-represented in technology design. We worked with ten Black girls in fifth and sixth grade from a predominantly Black school to understand their perceptions around fair and accountable AI and how they can have an empowered role in the creation of AI. Thematic analysis of discussions and activity artifacts from a summer camp and after-school session revealed a number of findings around how Black girls: perceive AI, primarily consider fairness as niceness and equality (but may need support considering other notions, such as equity), consider accountability, and envision a just future. We also discuss how the learners can be positioned as decision-making designers in creating AI technology, as well as how AI literacy learning experiences can be empowering.",AI ethics; artificial intelligence; AI literacy; Black girls; design,978-1-4503-9421-5,,2023,2023-11-06 01:30:00,2023-11-06 01:30:00,,CHI '23,Association for Computing Machinery,4S52CKYA,0.0458015267175572,0.4444444444444444,0.0714285714285714,0.0748874016292406
187,187,Complexity Management as an Ethical Challenge for AI-Based Age Tech,Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments,,,10.1145/3529190.3534752,"Rubeis, Giovanni",2022.0,https://doi.org/10.1145/3529190.3534752,conferencePaper,"Assistive technologies for older adults (age tech) may support individuals in living a mostly independent life in their own home environment for as long as possible. Especially age tech based on artificial intelligence (AI)-applications may enable a personalization of health and care services and facilitate quality of life. These technologies collect and process large amounts of individual data generated in the daily life and home environment of older adults through various sensors, monitor technologies, and smart wearables. Due to their focus on individual data, these technologies may be tools for tailoring health and care services to the individual needs and benefits of older adults. However, AI-based age tech comes with ethical issues linked to complexity management, i.e. the set of technical means and strategies for dealing with complex or ambiguous data, e.g. user characteristics or behavioral data. Algorithmic standardization might thus conflict with the goal of personalization. In this paper, I identify the crucial ethics issues related to AI-based age tech. I also discuss strategies for dealing with these issues and the fundamental trade-off between complexity management and the personalization of care services through AI-based age tech",artificial intelligence; assistive technology; ethics; gerontechnology,978-1-4503-9631-8,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,542–544,PETRA '22,Association for Computing Machinery,Q3MH2LUW,0.0481283422459893,0.3333333333333333,0.3,0.074423667332094
188,188,Examining Religion Bias in AI Text Generators,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462469,"Muralidhar, Deepa",2021.0,https://doi.org/10.1145/3461702.3462469,conferencePaper,"One of the biggest reasons artificial intelligence (AI) gets a backlash is because of inherent biases in AI software. Deep learning algorithms use data fed into the systems to find patterns to draw conclusions used to make application decisions. Patterns in data fed into machine learning algorithms have revealed that the AI software decisions have biases embedded within them. Algorithmic audits can certify that the software is making responsible decisions. These audits verify the standards centered around the various AI principles such as explainability, accountability, human-centered values, such as, fairness and transparency, to increase the trust in the algorithm and the software systems that implement AI algorithms.",algorithm; audit; NLP; religious-bias; tool-kit,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,273–274,AIES '21,Association for Computing Machinery,VIM3UD44,0.0747663551401869,0.0,0.1428571428571428,0.0742183243919216
189,189,Does a Compromise on Fairness Exist in Using AI Models?,"AI 2022: Advances in Artificial Intelligence: 35th Australasian Joint Conference, AI 2022, Perth, WA, Australia, December 5–8, 2022, Proceedings",,,10.1007/978-3-031-22695-3_14,"Zhou, Jianlong; Li, Zhidong; Xiao, Chun; Chen, Fang",2022.0,https://doi.org/10.1007/978-3-031-22695-3_14,conferencePaper,"Artificial Intelligence (AI) has been increasingly used to assist decision making in different domains. Multiple parties are usually affected by decisions in decision making, e.g. decision-maker and people affected by decisions. While various parties of users may have different responses to decisions regarding ethical concerns such as fairness, it is important to understand whether a compromise on fairness exists in using AI models. This paper takes AI-assisted talent shortlisting as a case study and investigates perception of fairness, trust, and satisfaction with decisions of both recruiters and applicants in AI-informed decision making. The compromises on fairness between decision-maker and people affected by decisions are identified which are then explained by social and psychological theories. The findings can be used to help find compromising points between decision-maker and people affected by decisions so that both parties can reach for a balanced state in decision making.",AI ethics; Fairness; Trust; Compromise; Satisfaction,978-3-031-22694-6,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,191–204,,Springer-Verlag,MC8ZWCKA,0.0416666666666666,0.6666666666666666,0.1,0.0740698729582577
190,190,From Human Resources to Human Rights: Impact Assessments for Hiring Algorithms,Ethics and Inf. Technol.,23.0,4,10.1007/s10676-021-09599-7,"Yam, Josephine; Skorburg, Joshua August",2021.0,https://doi.org/10.1007/s10676-021-09599-7,journalArticle,"Over the years, companies have adopted hiring algorithms because they promise wider job candidate pools, lower recruitment costs and less human bias. Despite these promises, they also bring perils. Using them can inflict unintentional harms on individual human rights. These include the five human rights to work, equality and nondiscrimination, privacy, free expression and free association. Despite the human rights harms of hiring algorithms, the AI ethics literature has predominantly focused on abstract ethical principles. This is problematic for two reasons. First, AI principles have been criticized for being vague and not actionable. Second, the use of vague ethical principles to discuss algorithmic risks does not provide any accountability. This lack of accountability creates an algorithmic accountability gap. Closing this gap is crucial because, without accountability, the use of hiring algorithms can lead to discrimination and unequal access to employment opportunities. This paper makes two contributions to the AI ethics literature. First, it frames the ethical risks of hiring algorithms using international human rights law as a universal standard for determining algorithmic accountability. Second, it evaluates four types of algorithmic impact assessments in terms of how effectively they address the five human rights of job applicants implicated in hiring algorithms. It determines which of the assessments can help companies audit their hiring algorithms and close the algorithmic accountability gap.",Artificial Intelligence; Machine Learning; Algorithmic Audits; Human Resources; Impact Assessments,,1388-1957,2021-12,2023-11-06 01:30:01,2023-11-06 01:30:01,611–623,,,44G8FJTN,0.0821917808219178,0.0,0.0,0.0737024889060389
191,191,The Forgotten Margins of AI Ethics,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533157,"Birhane, Abeba; Ruane, Elayne; Laurent, Thomas; S. Brown, Matthew; Flowers, Johnathan; Ventresque, Anthony; L. Dancy, Christopher",2022.0,https://doi.org/10.1145/3531146.3533157,conferencePaper,"How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued social and structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods, and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella of AI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences: FAccT and AIES. We organize the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications, use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of the majority of FAccT and AIES papers were often commendable, their consideration of the negative impacts of AI on traditionally marginalized groups remained shallow. Taken together, our conceptual analysis and the data from annotated papers indicate that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people’s experiences, and applications as well as from approaches that are sensitive to structural and historical power asymmetries.",AI Ethics; AIES; FAccT; Justice; Trends,978-1-4503-9352-2,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,948–958,FAccT '22,Association for Computing Machinery,XZGESDIQ,0.05,0.6666666666666666,0.5,0.0732290436835891
192,192,Shared Privacy Concerns of the Visually Impaired and Sighted Bystanders with Camera-Based Assistive Technologies,ACM Trans. Access. Comput.,15.0,2,10.1145/3506857,"Akter, Taslima; Ahmed, Tousif; Kapadia, Apu; Swaminathan, Manohar",2022.0,https://doi.org/10.1145/3506857,journalArticle,"Camera-based assistive technologies can provide people with visual impairments (PVIs) visually derived information about people in their vicinity. Furthermore, the advent of smart glasses offers the possibility of not only analyzing visual information in front of the wearers but also behind them through an extended field of view. Although such ‘visually available’ information can enhance one’s social interactions, the privacy and ethical implications for automated judgments about bystanders, especially from the perspective of PVIs, remains underexplored. To study the concerns of both bystanders and PVIs with such technologies, we conducted two online surveys with visually impaired participants as wearers (N = 128) and sighted participants as bystanders (N = 136). Although PVIs found some types of information to be improper or impolite (such as someone’s weight), our overarching finding is the shared ethical concern between PVIs and bystanders related to the fallibility of AI, in which bystanders can be misrepresented (algorithmically) by the devices. These mischaracterizations can range from occasional unexpected algorithmic errors (e.g., errors in facial recognition) to the questionable use of AI for determining subjective social constructs (such as gender). Based on our findings, we discuss the design implications and directions for future work in the development of camera-based assistive technologies while mitigating the ethical concerns of PVIs and bystanders.",AI ethics; Privacy; augmented reality; fairness and bias; visually impaired,,1936-7228,2022-05,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,BT5FLRFS,0.0566037735849056,0.4,0.0714285714285714,0.0729709940861729
193,193,Aegis: An Agent for Multi-Party Privacy Preservation,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534134,"Ben Salem, Rim; Aïmeur, Esma; Hage, Hicham",2022.0,https://doi.org/10.1145/3514094.3534134,conferencePaper,"The proliferation of social media set the foundation for the culture of over-disclosure where many people document every single event, incident, trip, etc. for everyone to see. Raising the individual's awareness of the privacy issues that they are subjecting themselves to can be challenging. This becomes more complex when the post being shared includes data ""owned"" by others. The existing approaches aiming to assist users in multi-party disclosure situations need to be revised to go beyond preferences to the ""good"" of the collective.This paper proposes an agent called Aegis to calculate the potential risk incurred by multi-party members in order to push privacy-preserving nudges to the sharer. Aegis is inspired by the consequentialist approach in normative ethical problem-solving techniques. The main contribution is the introduction of a social media-specific risk equation based on data valuation and the propagation of the post from intended to unintended audience. The proof-of-concept reports on how Aegis performs based on real-world data from the SNAP dataset and synthetically generated networks.",aegis; consequentialist approach; data valuation; normative ethical problem solving; nudges.,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,68–77,AIES '22,Association for Computing Machinery,2KEAEV6B,0.0424242424242424,0.4,0.1428571428571428,0.0727018282220823
194,194,The Ethical Implications of Generative Audio Models: A Systematic Literature Review,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604686,"Barnett, Julia",2023.0,https://doi.org/10.1145/3600211.3604686,conferencePaper,"Generative audio models typically focus their applications in music and speech generation, with recent models having human-like quality in their audio output. This paper conducts a systematic literature review of 884 papers in the area of generative audio models in order to both quantify the degree to which researchers in the field are considering potential negative impacts and identify the types of ethical implications researchers in this area need to consider. Though 65% of generative audio research papers note positive potential impacts of their work, less than 10% discuss any negative impacts. This jarringly small percentage of papers considering negative impact is particularly worrying because the issues brought to light by the few papers doing so are raising serious ethical implications and concerns relevant to the broader field such as the potential for fraud, deep-fakes, and copyright infringement. By quantifying this lack of ethical consideration in generative audio research and identifying key areas of potential harm, this paper lays the groundwork for future work in the field at a critical point in time in order to guide more conscientious research as this field progresses.",algorithmic ethics; audio; broader impacts; generative models; literature review,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,146–161,AIES '23,Association for Computing Machinery,IRUQIIK4,0.0489130434782608,0.2222222222222222,0.2727272727272727,0.0726621762214416
195,195,International Prospects and Trends of Artificial Intelligence Education: A Content Analysis of Top-Level AI Curriculum across Countries,Proceedings of the 6th International Conference on Digital Technology in Education,,,10.1145/3568739.3568796,"Zhou, Yujun; Zhan, Zehui; Liu, Lu; Wan, Jiayi; Liu, Simai; Zou, Xuanxuan",2023.0,https://doi.org/10.1145/3568739.3568796,conferencePaper,"This study intends to investigate the present situation of AI curriculum offered for grades K-12. We screened 11 representative countries and areas from six continents and assessed the content of their top K-12 AI courses in terms of teaching content and teaching implementation in order to comprehend the current state of K-12 AI courses in diverse nations. Provide some ideas and suggestions for the development of AI courses for students in grades K-12. (1) Countries may choose AI applications, AI influences in various aspects, AI ethics, machine learning, data, classification, reasoning, Identify, and other content to establish independent AI teaching content standards; or choose programming as the core teaching content/starting point; or integrate programming, data, AI, and other content related to improving students' computational thinking into computer/science/technology courses. (2) Project-based learning is still the primary way of instruction, along with a range of other approaches. There are four categories of available instructional resources, and there is an abundance of them. Most countries emphasize the evaluation of students' abilities and the results achieved in the learning process.",,978-1-4503-9809-1,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,337–343,ICDTE '22,Association for Computing Machinery,A48NSE2H,0.0738636363636363,0.0,0.0588235294117647,0.0723596256684492
196,196,The Ethics of Datasets: Moving Forward Requires Stepping Back,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462643,"Narayanan, Arvind",2021.0,https://doi.org/10.1145/3461702.3462643,conferencePaper,"Machine learning research culture is driven by benchmark datasets to a greater degree than most other research fields. But the centrality of datasets also amplifies the harms associated with data, including privacy violation and underrepresentation or erasure of some populations. This has stirred a much-needed debate on the ethical responsibilities of dataset creators and users. I argue that clarity on this debate requires taking a step back to better understand the benefits of the dataset-driven approach. I show that benchmark datasets play at least six different roles and that the potential harms depend on the roles a dataset plays. By understanding this relationship, we can mitigate the harms while preserving what is scientifically valuable about the prevailing approach.",ethics; machine learning; dataset,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,1,AIES '21,Association for Computing Machinery,R3P8DZSK,0.0423728813559322,0.5,0.2222222222222222,0.072068217014358
197,197,Towards a Seamful Ethics of Covid-19 Contact Tracing Apps?,Ethics and Inf. Technol.,23.0,Suppl 1,10.1007/s10676-020-09559-7,"Hoffman, Andrew S.; Jacobs, Bart; van Gastel, Bernard; Schraffenberger, Hanna; Sharon, Tamar; Pas, Berber",2021.0,https://doi.org/10.1007/s10676-020-09559-7,journalArticle,"In the early months of 2020, the deadly Covid-19 disease spread rapidly around the world. In response, national and regional governments implemented a range of emergency lockdown measures, curtailing citizens’ movements and greatly limiting economic activity. More recently, as restrictions begin to be loosened or lifted entirely, the use of so-called contact tracing apps has figured prominently in many jurisdictions’ plans to reopen society. Critics have questioned the utility of such technologies on a number of fronts, both practical and ethical. However, little has been said about the ways in which the normative design choices of app developers, and the products that result therefrom, might contribute to ethical reflection and wider political debate. Drawing from scholarship in critical design and human–computer interaction, this paper examines the development of a QR code-based tracking app called Zwaai (‘Wave’ in Dutch), where its designers explicitly positioned the app as an alternative to the predominant Bluetooth and GPS-based approaches. Through analyzing these designers’ choices, this paper argues that QR code infrastructures can work to surface a set of ethical–political seams, two of which are discussed here—responsibilization and networked (im)permanence—that more ‘seamless’ protocols like Bluetooth actively aim to bypass, and which may go otherwise unnoticed by existing ethical frameworks.",Digital ethics; Contact tracing; Covid-19; Critical design; Seamful infrastructure,,1388-1957,2021-11,2023-11-06 01:30:01,2023-11-06 01:30:01,105–115,,,82JQ9B9D,0.0441176470588235,0.4444444444444444,0.2222222222222222,0.0716939171834868
198,198,Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula,J. Artif. Int. Res.,73.0,,10.1613/jair.1.13550,"Javed, Rana Tallal; Nasir, Osama; Borit, Melania; Vanhée, Loïs; Zea, Elias; Gupta, Shivam; Vinuesa, Ricardo; Qadir, Junaid",2022.0,https://doi.org/10.1613/jair.1.13550,journalArticle,"The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom’s taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how. This article appears in the AI &amp; Society track.",data mining; discourse modelling; philosophical foundations; scientific discovery,,1076-9757,2022-05,2023-11-06 01:29:49,2023-11-06 01:29:49,,,,T2GYSM9D,0.0580912863070539,0.125,0.2222222222222222,0.0712384255196213
199,199,Statistically Responsible Artificial Intelligences,Ethics and Inf. Technol.,23.0,3,10.1007/s10676-021-09591-1,"Smith, Nicholas; Vickers, Darby",2021.0,https://doi.org/10.1007/s10676-021-09591-1,journalArticle,"As artificial intelligence (AI) becomes ubiquitous, it will be increasingly involved in novel, morally significant situations. Thus, understanding what it means for a machine to be morally responsible is important for machine ethics. Any method for ascribing moral responsibility to AI must be intelligible and intuitive to the humans who interact with it. We argue that the appropriate approach is to determine how AIs might fare on a standard account of human moral responsibility: a Strawsonian account. We make no claim that our Strawsonian approach is either the only one worthy of consideration or the obviously correct approach, but we think it is preferable to trying to marry fundamentally different ideas of moral responsibility (i.e. one for AI, one for humans) into a single cohesive account. Under a Strawsonian framework, people are morally responsible when they are appropriately subject to a particular set of attitudes—reactive attitudes—and determine under what conditions it might be appropriate to subject machines to this same set of attitudes. Although the Strawsonian account traditionally applies to individual humans, it is plausible that entities that are not individual humans but possess these attitudes are candidates for moral responsibility under a Strawsonian framework. We conclude that weak AI is never morally responsible, while a strong AI with the right emotional capacities may be morally responsible.",AI ethics; Artificial intelligence; Moral responsibility; Reactive attitude; Strawson,,1388-1957,2021-09,2023-11-06 01:30:01,2023-11-06 01:30:01,483–493,,,B8Z9STDB,0.0506912442396313,0.4444444444444444,0.0,0.0703446494527365
200,200,AI for the Public. How Public Interest Theory Shifts the Discourse on AI,AI Soc.,38.0,2,10.1007/s00146-022-01480-5,"Züger, Theresa; Asghari, Hadi",2022.0,https://doi.org/10.1007/s00146-022-01480-5,journalArticle,"AI for social good is a thriving research topic and a frequently declared goal of AI strategies and regulation. This article investigates the requirements necessary in order for AI to actually serve a public interest, and hence be socially good. The authors propose shifting the focus of the discourse towards democratic governance processes when developing and deploying AI systems. The article draws from the rich history of public interest theory in political philosophy and law, and develops a framework for ‘public interest AI’. The framework consists of (1) public justification for the AI system, (2) an emphasis on equality, (3) deliberation/ co-design process, (4) technical safeguards, and (5) openness to validation. This framework is then applied to two case studies, namely SyRI, the Dutch welfare fraud detection project, and UNICEF’s Project Connect, that maps schools worldwide. Through the analysis of these cases, the authors conclude that public interest is a helpful and practical guide for the development and governance of AI for the people.",AI ethics; Artificial intelligence; Deliberation; Democratic governance; Public interest,,0951-5666,2022-06,2023-11-06 01:30:01,2023-11-06 01:30:01,815–828,,,G6FR3LCZ,0.0426829268292682,0.3333333333333333,0.1538461538461538,0.0701408075468263
201,201,Literacy and STEM Teachers Adapt AI Ethics Curriculum,Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence,,,10.1609/aaai.v37i13.26906,"Walsh, Benjamin; Dalton, Bridget; Forsyth, Stacey; Yeh, Tom",2023.0,https://doi.org/10.1609/aaai.v37i13.26906,conferencePaper,"This article examines the ways secondary computer science and English Language Arts teachers in urban, suburban, and semi-rural schools adapted a project-based AI ethics curriculum to make it better fit their local contexts. AI ethics is an urgent topic with tangible consequences for youths' current and future lives, but one that is rarely taught in schools. Few teachers have formal training in this area as it is an emerging field even at the university level. Exploring AI ethics involves examining biases related to race, gender, and social class, a challenging task for all teachers, and an unfamiliar one for most computer science teachers. It also requires teaching technical content which falls outside the comfort zone of most humanities teachers. Although none of our partner teachers had previously taught an AI ethics project, this study demonstrates that their expertise and experience in other domains played an essential role in providing high quality instruction. Teachers designed and redesigned tasks and incorporated texts and apps to ensure the AI ethics project would adhere to district and department level requirements; they led equity-focused inquiry in a way that both protected vulnerable students and accounted for local cultures and politics; and they adjusted technical content and developed hands-on computer science experiences to better challenge and engage their students. We use Mishra and Kohler's TPACK framework to highlight the ways teachers leveraged their own expertise in some areas, while relying on materials and support from our research team in others, to create stronger learning experiences.",,978-1-57735-880-0,,2023,2023-11-06 01:29:48,2023-11-06 01:29:48,,AAAI'23/IAAI'23/EAAI'23,AAAI Press,7BLRMF5T,0.0602409638554216,0.0,0.375,0.070094613539806
202,202,"A Survey of the Potential Long-Term Impacts of AI: How AI Could Lead to Long-Term Changes in Science, Cooperation, Power, Epistemics and Values","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534131,"Clarke, Sam; Whittlestone, Jess",2022.0,https://doi.org/10.1145/3514094.3534131,conferencePaper,"It is increasingly recognised that advances in artificial intelligence could have large and long-lasting impacts on society. However, what form those impacts will take, just how large and long-lasting they will be, and whether they will ultimately be positive or negative for humanity, is far from clear. Based on surveying literature on the societal impacts of AI, we identify and discuss five potential long-term impacts of AI: how AI could lead to long-term chances in science, cooperation, power, epistemics, and values. We review the state of existing research in each of these areas and highlight priority questions for future research.",societal impacts of ai; ai alignment; conflict; cooperation; epistemic processes; foresight; power and inequality; scientific progress; transformative ai,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,192–202,AIES '22,Association for Computing Machinery,GTHZT6E6,0.03,0.2222222222222222,0.0869565217391304,0.0700370816723834
203,203,"More Similar Values, More Trust? - The Effect of Value Similarity on Trust in Human-Agent Interaction","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462576,"Mehrotra, Siddharth; Jonker, Catholijn M.; Tielman, Myrthe L.",2021.0,https://doi.org/10.1145/3461702.3462576,conferencePaper,"As AI systems are increasingly involved in decision making, it also becomes important that they elicit appropriate levels of trust from their users. To achieve this, it is first important to understand which factors influence trust in AI. We identify that a research gap exists regarding the role of personal values in trust in AI. Therefore, this paper studies how human and agent Value Similarity (VS) influences a human's trust in that agent. To explore this, 89 participants teamed up with five different agents, which were designed with varying levels of value similarity to that of the participants. In a within-subjects, scenario-based experiment, agents gave suggestions on what to do when entering the building to save a hostage. We analyzed the agent's scores on subjective value similarity, trust and qualitative data from open-ended questions. Our results show that agents rated as having more similar values also scored higher on trust, indicating a positive effect between the two. With this result, we add to the existing understanding of human-agent trust by providing insight into the role of value-similarity.",human-AI interaction; values; trust; artificial agents; human-computer interaction; intelligent agents; value similarity,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,777–783,AIES '21,Association for Computing Machinery,MDH5W8U9,0.0621468926553672,0.0833333333333333,0.125,0.0687422463075659
204,204,"Examining Trust in Conversational Systems: Conceptual and Empirical Findings on User Trust, Related Behavior, and System Trustworthiness","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539525,"Schmitt, Anuschka",2022.0,https://doi.org/10.1145/3514094.3539525,conferencePaper,"Machine learning (ML)-based conversational systems represent a value enabler for human-machine interaction. Simultaneously, the opacity, complexity, and humanness accompanied by such systems introduce their own issues, including trust misalignment. While trust is viewed as a prerequisite for effective system use, few studies have considered calibrating for appropriate trust, and empirically testing the relationship between trust and related behavior. Moreover, the desired implications of transparency-enhancing design cues are ambiguous. My research aims to explore the impact of system performance on trust, the dichotomy between trust and behavior, and how transparency might help attenuate the effects caused by low system performance in the specific context of decision-making tasks assisted by ML-based conversational systems.",transparency; conversational system; experiment; reliance; system trustworthiness; trust,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,912,AIES '22,Association for Computing Machinery,3F9KNS7Y,0.054054054054054,0.125,0.1176470588235294,0.0681030086690464
205,205,Challenges of Spatio-Temporal Trajectory Data Use: Focus Group Findings from the 1st International Summer School on Data Science for Mobility,Proceedings of the 27th International Database Engineered Applications Symposium,,,10.1145/3589462.3589478,"Arslan, Muhammad; Cruz, Christophe",2023.0,https://doi.org/10.1145/3589462.3589478,conferencePaper,"The fast development of wireless location acquisition technologies has led to a significant increase in the availability of mobility data, specifically spatio-temporal trajectory data, which includes information about the movements (locations) of objects over time. This data has proven valuable for a wide range of applications, including predicting travel patterns, discovering routes, analyzing social interactions, and managing resources in urban environments. However, using trajectory datasets can also be challenging, particularly in different countries. This article aims to explore the challenges of using trajectory datasets, specifically using focus group discussions. Focus groups are a qualitative method of gaining a deeper understanding of a particular topic and have not been previously used to examine the challenges related to trajectory datasets. The information gathered through these discussions is augmented by a review of existing literature for a comprehensive understanding of data challenges.",Data ethics; Data privacy; Focus group study; Spatio-temporal datasets; Trajectory analysis,9798400707445,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,51–58,IDEAS '23,Association for Computing Machinery,CQ48F2HP,0.0287769784172661,0.4545454545454545,0.1,0.0678068803139306
206,206,Making Human-Like Moral Decisions,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534174,"Loreggia, Andrea; Mattei, Nicholas; Rahgooy, Taher; Rossi, Francesca; Srivastava, Biplav; Venable, Kristen Brent",2022.0,https://doi.org/10.1145/3514094.3534174,conferencePaper,"Many real-life scenarios require humans to make difficult trade-offs: do we always follow all the traffic rules or do we violate the speed limit in an emergency? In general, how should we account for and balance the ethical values, safety recommendations, and societal norms, when we are trying to achieve a certain objective? To enable effective AI-human collaboration, we must equip AI agents with a model of how humans make such trade-offs in environments where there is not only a goal to be reached, but there are also ethical constraints to be considered and to possibly align with. These ethical constraints could be both deontological rules on actions that should not be performed, or also consequentialist policies that recommend avoiding reaching certain states of the world. Our purpose is to build AI agents that can mimic human behavior in these ethically constrained decision environments, with a long term research goal to use AI to help humans in making better moral judgments and actions. To this end, we propose a computational approach where competing objectives and ethical constraints are orchestrated through a method that leverages a cognitive model of human decision making, called multi-alternative decision field theory (MDFT). Using MDFT, we build an orchestrator, called MDFT-Orchestrator (MDFT-O), that is both general and flexible. We also show experimentally that MDFT-O both generates better decisions than using a heuristic that takes a weighted average of competing policies (WA-O), but also performs better in terms of mimicking human decisions as collected through Amazon Mechanical Turk (AMT). Our methodology is therefore able to faithfully model human decision in ethically constrained decision environments.",cognitive model; ethical constraints; human decision-making process; markov decision processes; orchestration,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,447–454,AIES '22,Association for Computing Machinery,LJNWK5ZI,0.0561797752808988,0.2727272727272727,0.0,0.067693668683432
207,207,EAAI-22 Blue Sky Ideas in Artificial Intelligence Education from the AAAI/ACM SIGAI New and Future AI Educator Program,AI Matters,8.0,2,10.1145/3557785.3557789,"Guerzhoy, Michael; Neumann, Marion; Johnson*, Emmanuel; Johnson, David; Chai, Henry; Garijo, Daniel; Lyu, Zhuoyue; MacLellan, Christopher J.",2022.0,https://doi.org/10.1145/3557785.3557789,journalArticle,"The 12th Symposium on Educational Advances in Artificial Intelligence (EAAI-22, cochaired by Michael Guerzhoy and Marion Neumann) continued the AAAI/ACM SIGAI New and Future AI Educator Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia). As part of the program, awardees were asked to address one of the following ""blue sky"" questions:•How could/should AI courses incorporate AI Ethics into the curriculum?•How could we teach AI topics at an early undergraduate or a secondary school level?•AI has the potential for broad impact to numerous disciplines. How could we make AI education more interdisciplinary, specifically to benefit non-engineering fields?•How should standard AI courses evolve?•How could we leverage AI education to promote diversity in the field?This paper is a collection of their responses, intended to help motivate discussion around these issues in AI education.",,,,2022-11,2023-11-06 01:30:03,2023-11-06 01:30:03,16–21,,,T9N3LTBI,0.0680272108843537,0.0,0.0555555555555555,0.0667225519049227
208,208,Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers,J. Artif. Int. Res.,71.0,,10.1613/jair.1.12895,"Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan",2021.0,https://doi.org/10.1613/jair.1.12895,journalArticle,"Machine learning (ML) and artificial intelligence (AI) researchers play an important role in the ethics and governance of AI, including through their work, advocacy, and choice of employment. Nevertheless, this influential group's attitudes are not well understood, undermining our ability to discern consensuses or disagreements between AI/ML researchers. To examine these researchers' views, we conducted a survey of those who published in two top AI/ML conferences (N = 524). We compare these results with those from a 2016 survey of AI/ML researchers (Grace et al., 2018) and a 2018 survey of the US public (Zhang &amp; Dafoe, 2020). We find that AI/ML researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of AI in the public interest; moderate trust in most Western tech companies; and low trust in national militaries, Chinese tech companies, and Facebook. While the respondents were overwhelmingly opposed to AI/ML researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of AI, particularly logistics algorithms. A strong majority of respondents think that AI safety research should be prioritized and that ML institutions should conduct pre-publication review to assess potential harms. Being closer to the technology itself, AI/ML researchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. The findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for AI. This article appears in the special track on AI &amp; Society.",AI ethics; AI governance; AI policy; scientific community,,1076-9757,2021-09,2023-11-06 01:30:00,2023-11-06 01:30:00,591–666,,,A5XKLVSR,0.044776119402985,0.625,0.1428571428571428,0.066499607170687
209,209,Automated Emotion Recognition in the Workplace: How Proposed Technologies Reveal Potential Futures of Work,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579528,"Boyd, Karen L.; Andalibi, Nazanin",2023.0,https://doi.org/10.1145/3579528,journalArticle,"Emotion recognition technologies, while critiqued for bias, validity, and privacy invasion, continue to be developed and applied in a range of domains including in high-stakes settings like the workplace. We set out to examine emotion recognition technologies proposed for use in the workplace, describing the input data and training, outputs, and actions that these systems take or prompt. We use these design features to reflect on these technologies' implications using the ethical speculation lens. We analyzed patent applications that developed emotion recognition technologies to be used in the workplace (N=86). We found that these technologies scope data collection broadly; claim to reveal not only targets' emotional expressions, but also their internal states; and take or prompt a wide range of actions, many of which impact workers' employment and livelihoods. Technologies described in patent applications frequently violated existing guidelines for ethical automated emotion recognition technology. We demonstrate the utility of using patent applications for ethical speculation. In doing so, we suggest that 1) increasing the visibility of claimed emotional states has the potential to create additional emotional labor for workers (a burden that is disproportionately distributed to low-power and marginalized workers) and contribute to a larger pattern of blurring boundaries between expectations of the workplace and a worker's autonomy, and more broadly to the data colonialism regime; 2) Emotion recognition technology's failures can be invisible, may inappropriately influence high-stakes workplace decisions and can exacerbate inequity. We discuss the implications of making emotions and emotional data visible in the workplace and submit for consideration implications for designers of emotion recognition, employers who use them, and policymakers.",AI ethics; AI and the future of work; emotion AI; emotion recognition; patents,,,2023-04,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,FRB5DVY9,0.0568181818181818,0.3846153846153846,0.0,0.0663247258577997
210,210,Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462561,"Pandey, Akshat; Caliskan, Aylin",2021.0,https://doi.org/10.1145/3461702.3462561,conferencePaper,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.",AI ethics; algorithmic bias; disparate impact; geolocation; prediction; price discrimination,978-1-4503-8473-5,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,822–833,AIES '21,Association for Computing Machinery,JWGSS5NE,0.048,0.4,0.0833333333333333,0.066155935613682
211,211,From Explanation to Recommendation: Ethical Standards for Algorithmic Recourse,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534185,"Sullivan, Emily; Verreault-Julien, Philippe",2022.0,https://doi.org/10.1145/3514094.3534185,conferencePaper,"People are increasingly subject to algorithmic decisions, and it is generally agreed that end-users should be provided an explanation or rationale for these decisions. There are different purposes that explanations can have, such as increasing user trust in the system or allowing users to contest the decision. One specific purpose that is gaining more traction is algorithmic recourse. We first propose that recourse should be viewed as a recommendation problem, not an explanation problem. Then, we argue that the capability approach provides plausible and fruitful ethical standards for recourse. We illustrate by considering the case of diversity constraints on algorithmic recourse. Finally, we discuss the significance and implications of adopting the capability approach for algorithmic recourse research.",diversity; explainable AI; counterfactuals; algorithmic recourse; capability approach; recommendations,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,712–722,AIES '22,Association for Computing Machinery,4X4PQVL5,0.0341880341880341,0.1111111111111111,0.3333333333333333,0.0655873163426033
212,212,Judging Instinct Exploitation in Statistical Data Explanations Based on Word Embedding,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534171,"Zhang, Kang; Shinden, Hiroaki; Mutsuro, Tatsuki; Suzuki, Einoshin",2022.0,https://doi.org/10.1145/3514094.3534171,conferencePaper,"This paper proposes 18 types of statistical data explanations and three kinds of procedures to investigate credibility in unethical and biased explanations due to exploitation of the 10 instincts proposed by Rosling et al. The explanation ""women have lower math scores than men” accompanied with the averages and the distributions of their scores is an example of such an explanation, as it exploits the gap instinct, i.e., our tendency to divide all kinds of things into two distinct and often conflicting groups. It becomes much less credible if we replace the word ""math” with ""English”, even if we keep the data as they are, as the exploitation seems to fail. Our judging procedures are based on phrase embedding and carefully designed comparisons to judge the credibility. The results of our experiments comparing the 18 types with their variants show promising results and clues for further developments.",ai and ethics; exploitation of thinking traits; text classification; word embedding,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,867–879,AIES '22,Association for Computing Machinery,N3VBHBVU,0.0273972602739726,0.3636363636363636,0.1818181818181818,0.0655696836584771
213,213,Exploring the Moral Value of Explainable Artificial Intelligence Through Public Service Postal Banks,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604741,"Brand, Joshua",2023.0,https://doi.org/10.1145/3600211.3604741,conferencePaper,This research examines Explainable AI (XAI) from a duty-based perspective in the context of public service postal banks. I argue that XAI is a strict obligation for these banks whenever they implement advanced AI-recommendation systems which flows from the Kantian principle to respect humanity that is integral to their public service identity.,Deontology; Ethics; Explainability; Explainable AI; Public Service Banks,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,990–992,AIES '23,Association for Computing Machinery,2Q4RWP8E,0.0192307692307692,0.375,0.0,0.0650572831423895
214,214,Is Elementary AI Education Possible?,Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2,,,10.1145/3545947.3576308,"Ottenbreit-Leftwich, Anne; Glazewski, Krista; Hmelo-Silver, Cindy; Jantaraweragul, Katie; Jeon, Minji; Chakraburty, Srijita; Scribner, Adam; Lee, Seung; Mott, Bradford; Lester, James",2023.0,https://doi.org/10.1145/3545947.3576308,conferencePaper,"As artificial intelligence (AI) technology becomes increasingly pervasive, it is critical that students recognize AI and how it can be used. There is little research exploring learning capabilities of elementary students and the pedagogical supports necessary to facilitate students' learning. PrimaryAI was created as a 3rd-5th grade AI curriculum that utilizes problem-based and immersive learning within an authentic life science context through four units that cover machine learning, computer vision, AI planning, and AI ethics. The curriculum was implemented by two upper elementary teachers during Spring 2022. Based on pre-test/post-test results, students were able to conceptualize AI concepts related to machine learning and computer vision. Results showed no significant differences based on gender. Teachers indicated the curriculum engaged students and provided teachers with sufficient scaffolding to teach the content in their classrooms. Recommendations for future implementations include greater alignment between the AI and life science concepts, alterations to the immersive problem-based learning environment, and enhanced connections to local animal populations.",elementary education; teacher co-design; ai education; integration,978-1-4503-9433-8,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,1364,SIGCSE 2023,Association for Computing Machinery,3R4GQFMI,0.05625,0.1428571428571428,0.2,0.0647255419677598
215,215,"AI, Explainability and Public Reason: The Argument from the Limitations of the Human Mind",Minds Mach.,31.0,3,10.1007/s11023-021-09570-x,"Maclure, Jocelyn",2021.0,https://doi.org/10.1007/s11023-021-09570-x,journalArticle,"Machine learning-based AI algorithms lack transparency. In this article, I offer an interpretation of AI’s explainability problem and highlight its ethical saliency. I try to make the case for the legal enforcement of a strong explainability requirement: human organizations which decide to automate decision-making should be legally obliged to demonstrate the capacity to explain and justify the algorithmic decisions that have an impact on the wellbeing, rights, and opportunities of those affected by the decisions. This legal duty can be derived from the demands of Rawlsian public reason. In the second part of the paper, I try to show that the argument from the limitations of human cognition fails to get AI off the hook of public reason. Against a growing trend in AI ethics, my main argument is that the analogy between human minds and artificial neural networks fails because it suffers from an atomistic bias which makes it blind to the social and institutional dimension of human reasoning processes. I suggest that developing interpretive AI algorithms is not the only possible answer to the explainability problem; social and institutional answers are also available and in many cases more trustworthy than techno-scientific ones.",AI ethics; Artificial intelligence; Explainability; Cognitive biases; Machine learning; Public reason,,0924-6495,2021-09,2023-11-06 01:29:59,2023-11-06 01:29:59,421–438,,,8MT3MA57,0.0463917525773195,0.2727272727272727,0.0714285714285714,0.0638869857801834
216,216,Understanding the Representation and Representativeness of Age in AI Data Sets,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462590,"Park, Joon Sung; Bernstein, Michael S.; Brewer, Robin N.; Kamar, Ece; Morris, Meredith Ringel",2021.0,https://doi.org/10.1145/3461702.3462590,conferencePaper,"A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.",inclusion; datasets; representation; accessibility; aging; AI fate; older adults,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,834–842,AIES '21,Association for Computing Machinery,DXMZVJER,0.0550847457627118,0.1111111111111111,0.1818181818181818,0.0638346344595441
217,217,Learning Design to Support Student-AI Collaboration: Perspectives of Leading Teachers for AI in Education,Education and Information Technologies,27.0,5,10.1007/s10639-021-10831-6,"Kim, Jinhee; Lee, Hyunkyung; Cho, Young Hoan",2022.0,https://doi.org/10.1007/s10639-021-10831-6,journalArticle,"Preparing students to collaborate with AI remains a challenging goal. As AI technologies are new to K-12 schools, there is a lack of studies that inform how to design learning when AI is introduced as a collaborative learning agent to classrooms. The present study, therefore, aimed to explore teachers’ perspectives on what (1) curriculum design, (2) student-AI interaction,&nbsp;and (3) learning environments are required to design student-AI collaboration (SAC) in learning and (4) how SAC would evolve. Through in-depth interviews with 10 Korean leading teachers in AI in Education (AIED), the study found that teachers perceived capacity and subject-matter knowledge building as the optimal learning goals for SAC. SAC can be facilitated through interdisciplinary learning, authentic problem solving, and creative tasks in tandem with process-oriented assessment and collaboration performance assessment. While teachers expressed instruction on AI principles, data literacy, error analysis, AI ethics, and AI experiences in daily life were crucial support, AI needs to offer an instructional scaffolding and possess attributes as a learning mate to enhance student-AI interaction. In addition, teachers highlighted systematic AIED policy, flexible school system, the culture of collaborative learning, and a&nbsp;safe to fail&nbsp;environment are significant. Teachers further anticipated students would develop collaboration with AI through three stages: (1) learn about AI, (2) learn from AI, and (3) learn together. These findings can provide a&nbsp;more holistic understanding of the AIED and implications for the educational policies, educational AI design as well as instructional design that are aimed at enhancing SAC in learning.",AI in education; Distributed cognition; Learning design; Student-AI collaboration,,1360-2357,2022-06,2023-11-06 01:30:04,2023-11-06 01:30:04,6069–6104,,,L45UJSHX,0.0609756097560975,0.1111111111111111,0.0714285714285714,0.0636498122368471
218,218,Mimetic Models: Ethical Implications of AI That Acts Like You,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534177,"McIlroy-Young, Reid; Kleinberg, Jon; Sen, Siddhartha; Barocas, Solon; Anderson, Ashton",2022.0,https://doi.org/10.1145/3514094.3534177,conferencePaper,"An emerging theme in artificial intelligence research is the creation of models to simulate the decisions and behavior of specific people, in domains including game-playing, text generation, and artistic expression. These models go beyond earlier approaches in the way they are tailored to individuals, and the way they are designed for interaction rather than simply the reproduction of fixed, pre-computed behaviors. We refer to these as mimetic models, and in this paper we develop a framework for characterizing the ethical and social issues raised by their growing availability. Our framework includes a number of distinct scenarios for the use of such models, and considers the impacts on a range of different participants, including the target being modeled, the operator who deploys the model, and the entities that interact with it.",artificial intelligence; ethics; machine learning; generative models; mimetic models,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,479–490,AIES '22,Association for Computing Machinery,L6JZNMM5,0.023076923076923,0.2222222222222222,0.4,0.0633614188380922
219,219,Causal Framework of Artificial Autonomous Agent Responsibility,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534140,"Franklin, Matija; Ashton, Hal; Awad, Edmond; Lagnado, David",2022.0,https://doi.org/10.1145/3514094.3534140,conferencePaper,"Recent empirical work on people's attributions of responsibility toward artificial autonomous agents (such as Artificial Intelligence agents or robots) has delivered mixed findings. The conflicting results reflect differences in context, the roles of AI and human agents, and the domain of application. In this article, we outline a causal framework of responsibility attribution which integrates these findings. It outlines nine factors that influence responsibility attribution - causality, role, knowledge, objective foreseeability, capability, intent, desire, autonomy, and character. We propose a framework of responsibility that outlines the causal relationships between the nine factors and responsibility. To empirically test the framework we discuss some initial findings and outline an approach to using serious games for causal cognitive research on responsibility attribution. Specifically, we propose a game that uses a generative approach to creating different scenarios, in which participants can freely inspect different sources of information to make judgments about human and artificial autonomous agents.",attribution; blame; causal cognition; responsibility,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,276–284,AIES '22,Association for Computing Machinery,E39RLJ3F,0.0526315789473684,0.2,0.1428571428571428,0.0633487518310732
220,220,Governing Artificial Intelligence and Algorithmic Decision Making: Human Rights and Beyond,"Responsible AI and Analytics for an Ethical and Inclusive Digitized Society: 20th IFIP WG 6.11 Conference on e-Business, e-Services and e-Society, I3E 2021, Galway, Ireland, September 1–3, 2021, Proceedings",,,10.1007/978-3-030-85447-8_16,"Koniakou, Vasiliki",2021.0,https://doi.org/10.1007/978-3-030-85447-8_16,conferencePaper,"In the context of Artificial Intelligence Ethics, human rights have been commonly invoked as a promising basis for an ethical framework. They have been also promoted as guidelines for Artificial Intelligence and Automatic Decision-making governance, or as engineering principles that may be turned into design requirements. Since literature so far engages only partially with the relevance and suitability of the extension of human rights in the realm of proprietary algorithms and privately owned Artificial Intelligence systems, this paper offers the necessary background and justification, building upon international human rights law theory and the concept of radiance of human rights. It aims to contribute to the scholarship promoting the human rights not only as ethical values but also as governance principles for Artificial Intelligence and algorithms. It also stresses the significance of concretizing and implementing the values of transparency, accountability, and explicability. Moreover, it suggests that for the ethically sound and societally beneficial employment of Artificial Intelligence and algorithms, useful insights may be derived from the field of technology governance. Stemming from that, it emphasizes the necessity to embrace the role of designers, and the need of conscious democratic control.",AI ethics; AI governance; Algorithms; Algorithmic decision-making (ADM); Artificial intelligence (AI); Human rights; Science and technology studies (STS); Technology theory,978-3-030-85446-1,,2021,2023-11-06 01:30:02,2023-11-06 01:30:02,173–184,,Springer-Verlag,UPLMJK5L,0.0423280423280423,0.25,0.0,0.0624605678233438
221,221,"No Justice, No Robots: From the Dispositions of Policing to an Abolitionist Robotics","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604663,"Williams, Tom; Haring, Kerstin Sophie",2023.0,https://doi.org/10.1145/3600211.3604663,conferencePaper,"In this paper, we examine the risks posed by roboticists’ collaboration with law enforcement agencies in the U.S. Using Trust frameworks from AI Ethics, we argue that collaborations with law enforcement present not only risks of technology misuse, but also risks of legitimizing bad actors, and of exacerbating our field’s challenges of representation. We discuss evidence of bad dispositions justifying these risks, grounded in the behavior, origins, and incentivization of American policing, and suggest courses of action for American roboticists seeking to pursue research projects that currently require collaboration with law enforcement agencies, closing with a call for abolitionist robotics.",Abolition; Policing; Robot Ethics,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,566–575,AIES '23,Association for Computing Machinery,Z9KEVFD5,0.04,0.5,0.0769230769230769,0.0624038461538461
222,222,Detecting Risk of Biased Output with Balance Measures,J. Data and Information Quality,14.0,4,10.1145/3530787,"Mecati, Mariachiara; Vetrò, Antonio; Torchiano, Marco",2022.0,https://doi.org/10.1145/3530787,journalArticle,"Data have become a fundamental element of the management and productive infrastructures of our society, fuelling digitization of organizational and decision-making processes at an impressive speed. This transition shows lights and shadows, and the “bias in-bias out” problem is one of the most relevant issues, which encompasses technical, ethical, and social perspectives. We address this field of research by investigating how the balance of protected attributes in training data can be used to assess the risk of algorithmic unfairness. We identify four balance measures and test their ability to detect the risk of discriminatory classification by applying them to the training set. The results of this proof of concept show that the indexes can properly detect unfairness of software output. However, we found the choice of the balance measure has a relevant impact on the threshold to consider as risky; further work is necessary to deepen knowledge on this aspect.",data ethics; algorithm fairness; data bias; Data quality,,1936-1955,2022-11,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,8975AGLM,0.0333333333333333,0.625,0.0,0.0623765432098765
223,223,Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579467,"Ehsan, Upol; Saha, Koustuv; De Choudhury, Munmun; Riedl, Mark O.",2023.0,https://doi.org/10.1145/3579467,journalArticle,"Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap-divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.",AI ethics; AI governance; responsible ai; explainable ai; fate; framework; human-AI interaction; human-centered explainable ai; organizational dynamics; participatory design; sociotechnical gap; user study,,,2023-04,2023-11-06 01:30:03,2023-11-06 01:30:03,,,,U52G6WJN,0.0132450331125827,0.3043478260869565,0.0666666666666666,0.0620156212281176
224,224,The Profile: Unleashing Your Deepfake Self,Multimedia Tools Appl.,82.0,20,10.1007/s11042-023-14568-x,"Cheres, Ioana; Groza, Adrian",2023.0,https://doi.org/10.1007/s11042-023-14568-x,journalArticle,"From the way we perceive society to the way we perceive ourselves, the virtual environment is changing the fundamental mechanisms of living. There is a significant gap in understanding the impact of current AI technologies on society. For instance, deepfake creates realistic forgeries that make people unable to distinguish reality from fiction, while filtering algorithms distort the perception by amplifying the interests of the user. The speed and scale of the virtual world and its hazy moral context put pressure on regulators to build the groundwork of ethical codes and regulatory frameworks. The multifaceted nature of such regulatory frameworks - legal, technological, societal, political, ethical - make it a challenging task, in which the public perception remains a key issue.The study presents a multimedia art installation - “The Profile”- based on deepfake technologies, in which the users movements are gradually learned and an alter ego is generated. The user becomes confused when this alter ego starts to deviate from its initial mirroring behaviour. “The Profile” artwork aims to raise awareness of the main threats of current online technologies and dark patterns employed online. The users experience ethical dilemmas regarding the impact of the latest technologies in a metaphoric approach. The interaction is structured in three parts that match the steps of creating an online identity: (i) filling personal data, (ii) agreeing to terms and conditions, and (iii) receiving personalised content. Our work extends the current state-of-the-art by using the decomposition of the frame along with the image simplification, real time rendering and the combination of ethics and art for human-centred technology. We also discuss some of the risks of AI, along with the proposal of art as a way to approach technology ethics. With the “The Profile”, we aim to raise awareness about the power of deepfakes, build new technology, understand human behaviour, and present a fresh perspective on intertwining art and AI.",AI ethics; AI and art; Full body puppetry,,1380-7501,2023-03,2023-11-06 01:30:00,2023-11-06 01:30:00,31839–31854,,,2I6YB4BM,0.0543130990415335,0.5,0.0,0.061870761035651
225,225,AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539531,"Chen, Pei-Yu",2022.0,https://doi.org/10.1145/3514094.3539531,conferencePaper,"This project proposes a different way of looking at AI alignment, namely by introducing AI Alignment Dialogues. We argue that alignment dialogues have a number of advantages in comparison to data-driven approaches, especially for behaviour support agents, which aim to support users in achieving their desired future behaviours rather than their current behaviours. The advantages of alignment dialogues include allowing the users to directly convey higher-level concepts to the agent and making the agent more transparent and trusted.",responsible AI; AI alignment; behaviour support agent; conversational agent; human values,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,894,AIES '22,Association for Computing Machinery,VE5Q7R3C,0.0256410256410256,0.1818181818181818,0.1666666666666666,0.0615599530493147
226,226,Ground Truth Or Dare: Factors Affecting The Creation Of Medical Datasets For Training AI,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604766,"Zając, Hubert Dariusz; Avlona, Natalia Rozalia; Kensing, Finn; Andersen, Tariq Osman; Shklovski, Irina",2023.0,https://doi.org/10.1145/3600211.3604766,conferencePaper,"One of the core goals of responsible AI development is ensuring high-quality training datasets. Many researchers have pointed to the importance of the annotation step in the creation of high-quality data, but less attention has been paid to the work that enables data annotation. We define this work as the design of ground truth schema and explore the challenges involved in the creation of datasets in the medical domain even before any annotations are made. Based on extensive work in three health-tech organisations, we describe five external and internal factors that condition medical dataset creation processes. Three external factors include regulatory constraints, the context of creation and use, and commercial and operational pressures. These factors condition medical data collection and shape the ground truth schema design. Two internal factors include epistemic differences and limits of labelling. These directly shape the design of the ground truth schema. Discussions of what constitutes high-quality data need to pay attention to the factors that shape and constrain what is possible to be created, to ensure responsible AI design.",Data Creation; Medical Datasets; Responsible Artificial Intelligence and Machine Learning,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,351–362,AIES '23,Association for Computing Machinery,57IURNWH,0.0517241379310344,0.1,0.1428571428571428,0.0609984272063623
227,227,AI and Shared Prosperity,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462619,"Klinova, Katya; Korinek, Anton",2021.0,https://doi.org/10.1145/3461702.3462619,conferencePaper,"Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",automation; inequality; job displacement; shared prosperity; socio-technical systems; steering technological progress,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,645–651,AIES '21,Association for Computing Machinery,YJPH7SU2,0.0504201680672268,0.0909090909090909,0.25,0.0609941080659889
228,228,"Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices","Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445874,"Cheong, Marc; Leins, Kobi; Coghlan, Simon",2021.0,https://doi.org/10.1145/3442188.3445874,conferencePaper,"Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.",diversity; gender; computer science; gender representation; publishing; research; sex equality,978-1-4503-8309-7,,2021,2023-11-06 01:30:04,2023-11-06 01:30:04,106–115,FAccT '21,Association for Computing Machinery,GHDK25MA,0.0625,0.0,0.0909090909090909,0.0608511415842962
229,229,The Future of Cognitive Personal Informatics,Proceedings of the 25th International Conference on Mobile Human-Computer Interaction,,,10.1145/3565066.3609790,"Schneegass, Christina; Wilson, Max L; Maior, Horia A.; Chiossi, Francesco; Cox, Anna L; Wiese, Jason",2023.0,https://doi.org/10.1145/3565066.3609790,conferencePaper,"While Human-Computer Interaction (HCI) has contributed to demonstrating that physiological measures can be used to detect cognitive changes, engineering and machine learning will bring these to application in consumer wearable technology. For HCI, many open questions remain, such as: What happens when this becomes a cognitive form of personal informatics? What goals do we have for our daily cognitive activity? How should such a complex concept be conveyed to users to be useful in their everyday life? How can we mitigate potential ethical concerns? These issues are different from physiologically controlled interactions, such as BCIs, to a time when we have new data about ourselves. This workshop will be the first to directly address the future of Cognitive Personal Informatics (CPI), by bringing together design, BCI and physiological data, ethics, and personal informatics researchers to discuss and set the research agenda in this inevitable future before it arrives.",digital health; neurotechnology; personal informatics; well-being; work-life balance,978-1-4503-9924-1,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,,MobileHCI '23 Companion,Association for Computing Machinery,KW3FMGBQ,0.0472972972972973,0.25,0.0,0.0607986034815303
230,230,What Could Safety Research Contribute to Technology Design?,"Culture and Computing. Design Thinking and Cultural Computing: 9th International Conference, C&amp;C 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II",,,10.1007/978-3-030-77431-8_4,"Hallamaa, Jaana",2021.0,https://doi.org/10.1007/978-3-030-77431-8_4,conferencePaper,"There is already evidence of several mishaps, accidents, and even major catastrophes in which AI technologies have played a part. Among them are traditional types of organizational accidents but also complex misadventures where the effects of the use of AI technology intertwine with human action in the context of intricate socio-technical settings. The origins of problems range from making errors during a planned course of action to accidents due to unintended and unpredictable outcomes of the use of AI. A variety of ethical principles for the use and design of AI have been formulated to remedy the ills. The article sums up the main concepts of the AI ethical codes and discusses their role in attempts to prevent AI-induced problems. According to recent empirical studies, the impact of ethics principles is weak in terms of real-life AI design and use. As an alternative, the article suggests the findings of safety research as a source for a practice-oriented approach to tackle the problems. A century-long study of safety provides both conceptual models and practical tools for the prevention of accidents and creating the means to improve safety. The article introduces the main findings of this line of research and suggests how the Normal Accident Theory, High Reliability Organizations studies, and Just Culture approach could contribute to a safer AI design.",AI-Ethics; High Reliability Organizations; Just culture; Normal Accident Theory; Organizational accidents; Safety culture,978-3-030-77430-1,,2021,2023-11-06 01:30:03,2023-11-06 01:30:03,56–79,,Springer-Verlag,EFU5ZUDF,0.0684931506849315,0.0,0.0,0.0605697784977286
231,231,Designing Interfaces to Elicit Data Issues for Data Workers,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604756,"Bryson, Kevin",2023.0,https://doi.org/10.1145/3600211.3604756,conferencePaper,"Goals of objectivity and neutrality drive the usage of algorithmic systems, however most efforts have produced similar or more harm than their human counterparts. Post-hoc analyses of these flawed systems often reveal systemic issues underlying the data and flawed assumptions in preparation stages that affect the models produced. To date, the algorithmic fairness community has had a myopic focus on optimizing and evaluating algorithmic systems at static decision points and mathematical definitions of fairness, neglecting efforts towards critically understanding the data being used prior to modeling. My research aims to support data workers’ sociotechnical understanding of data by creating new interfaces and methods to elicit and utilize their prior knowledge and open information (such as census data), as means to discover and augment harmful patterns in data.",,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,957–958,AIES '23,Association for Computing Machinery,PRLLA3PS,0.0472440944881889,0.0,0.2222222222222222,0.0583448574304555
232,232,Bias in Hate Speech and Toxicity Detection,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539519,"Reyero Lobo, Paula",2022.0,https://doi.org/10.1145/3514094.3539519,conferencePaper,"Many Artificial Intelligence (AI) systems rely on finding patterns in large datasets, which are prone to bias and exacerbate existing segregation and inequalities of marginalised communities. Due to their socio-technical impact, bias in AI has become a pressing issue. In this work, we investigate discrimination prevention methods on the assumption that disparities of specific populations in the training samples are reproduced or even amplified in the AI system outcomes. We aim to identify the information from vulnerable groups in the training data, uncover potential inequalities in how data capture these groups and provide additional information about them to alleviate inequalities, e.g., stereotypical and generalised views that lead to learning discriminatory associations. We develop data preprocessing techniques in automated moderation (AI systems to flag or filter online abuse) due to its substantial social implications and existing challenges common to many AI applications.",artificial intelligence; bias; semantic web; toxic speech,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,910,AIES '22,Association for Computing Machinery,MGR4T53G,0.0638297872340425,0.0,0.0,0.0580377167150964
233,233,Machine Learning and Power Relations,AI Soc.,38.0,4,10.1007/s00146-022-01400-7,"Maas, Jonne",2022.0,https://doi.org/10.1007/s00146-022-01400-7,journalArticle,"There has been an increased focus within the AI ethics literature on questions of power, reflected in the ideal of accountability supported by many Responsible AI guidelines. While this recent debate points towards the power asymmetry between those who shape AI systems and those affected by them, the literature lacks normative grounding and misses conceptual clarity on how these power dynamics take shape. In this paper, I develop a workable conceptualization of said power dynamics according to Cristiano Castelfranchi’s conceptual framework of power and argue that end-users depend on a system’s developers and users, because end-users rely on these systems to satisfy their goals, constituting a power asymmetry between developers, users and end-users. I ground my analysis in the neo-republican moral wrong of domination, drawing attention to legitimacy concerns of the power-dependence relation following from the current lack of accountability mechanisms. I illustrate my claims on the basis of a risk-prediction machine learning system, and propose institutional (external auditing) and project-specific solutions (increase contestability through design-for-values approaches) to mitigate domination.",Responsible AI; Machine learning; AI design; Design-for-values; Domination; Power relations,,0951-5666,2022-02,2023-11-06 01:30:03,2023-11-06 01:30:03,1493–1500,,,PS88WJ39,0.0411764705882352,0.3,0.0,0.0576840160147828
234,234,Fostering AI Literacy with Embodiment &amp; Creativity: From Activity Boxes to Museum Exhibits,Proceedings of the 22nd Annual ACM Interaction Design and Children Conference,,,10.1145/3585088.3594495,"Long, Duri; Rollins, Sophie; Ali-Diaz, Jasmin; Hancock, Katherine; Nuonsinoeun, Samnang; Roberts, Jessica; Magerko, Brian",2023.0,https://doi.org/10.1145/3585088.3594495,conferencePaper,"Fostering young learners’ literacy surrounding AI technologies is becoming increasingly important as AI is becoming integrated in many aspects of our lives and is having far-reaching impacts on society. We have developed Knowledge Net and Creature Features, two activity boxes for family groups to engage with in their homes that communicate AI literacy competencies such as understanding knowledge representations, the steps of machine learning, and AI ethics. Our current work is exploring how to transform these activity boxes into museum exhibits for middle-school age learners, focusing on three key considerations: centering learner interests, generating personally meaningful outputs, and incorporating embodiment and collaboration on a larger scale. Our demonstration will feature the existing Knowledge Net and Creature Features activity boxes alongside early-stage prototypes adapting these activities into larger-scale museum exhibits. This paper contributes an exploration into how to design AI literacy learning interventions for varied informal learning contexts.",AI literacy; AI education; at-home learning; design research; informal learning; museum exhibit; prototyping; tangible user interfaces,9798400701313,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,727–731,IDC '23,Association for Computing Machinery,PYXMI4DD,0.0476190476190476,0.125,0.0769230769230769,0.0576754662961559
235,235,Data Subjects' Conceptualizations of and Attitudes Toward Automatic Emotion Recognition-Enabled Wellbeing Interventions on Social Media,Proc. ACM Hum.-Comput. Interact.,5.0,CSCW2,10.1145/3476049,"Roemmich, Kat; Andalibi, Nazanin",2021.0,https://doi.org/10.1145/3476049,journalArticle,"Automatic emotion recognition (ER)-enabled wellbeing interventions use ER algorithms to infer the emotions of a data subject (i.e., a person about whom data is collected or processed to enable ER) based on data generated from their online interactions, such as social media activity, and intervene accordingly. The potential commercial applications of this technology are widely acknowledged, particularly in the context of social media. Yet, little is known about data subjects' conceptualizations of and attitudes toward automatic ER-enabled wellbeing interventions. To address this gap, we interviewed 13 US adult social media data subjects regarding social media-based automatic ER-enabled wellbeing interventions. We found that participants' attitudes toward automatic ER-enabled wellbeing interventions were predominantly negative. Negative attitudes were largely shaped by how participants compared their conceptualizations of Artificial Intelligence (AI) to the humans that traditionally deliver wellbeing support. Comparisons between AI and human wellbeing interventions were based upon human attributes participants doubted AI could hold: 1) helpfulness and authentic care; 2) personal and professional expertise; 3) morality; and 4) benevolence through shared humanity. In some cases, participants' attitudes toward automatic ER-enabled wellbeing interventions shifted when participants conceptualized automatic ER-enabled wellbeing interventions' impact on others, rather than themselves. Though with reluctance, a minority of participants held more positive attitudes toward their conceptualizations of automatic ER-enabled wellbeing interventions, citing their potential to benefit others: 1) by supporting academic research; 2) by increasing access to wellbeing support; and 3) through egregious harm prevention. However, most participants anticipated harms associated with their conceptualizations of automatic ER-enabled wellbeing interventions for others, such as re-traumatization, the spread of inaccurate health information, inappropriate surveillance, and interventions informed by inaccurate predictions. Lastly, while participants had qualms about automatic ER-enabled wellbeing interventions, we identified three development and delivery qualities of automatic ER-enabled wellbeing interventions upon which their attitudes toward them depended: 1) accuracy; 2) contextual sensitivity; and 3) positive outcome. Our study is not motivated to make normative statements about whether or how automatic ER-enabled wellbeing interventions should exist, but to center voices of the data subjects affected by this technology. We argue for the inclusion of data subjects in the development of requirements for ethical and trustworthy ER applications. To that end, we discuss ethical, social, and policy implications of our findings, suggesting that automatic ER-enabled wellbeing interventions imagined by participants are incompatible with aims to promote trustworthy, socially aware, and responsible AI technologies in the current practical and regulatory landscape in the US.",AI ethics; ethics; fairness; emotion recognition; affect recognition; affective computing; algorithmic accountability; artificial emotional intelligence; emotion ai; social media; wellbeing interventions,,,2021-10,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,YS9GKVKD,0.0421836228287841,0.2857142857142857,0.0666666666666666,0.0575557987203233
236,236,COVID-19 Brings Data Equity Challenges to the Fore,Digit. Gov.: Res. Pract.,2.0,2,10.1145/3440889,"Jagadish, H. V.; Stoyanovich, Julia; Howe, Bill",2021.0,https://doi.org/10.1145/3440889,journalArticle,"The COVID-19 pandemic is compelling us to make crucial data-driven decisions quickly, bringing together diverse and unreliable sources of information without the usual quality control mechanisms we may employ. These decisions are consequential at multiple levels: They can inform local, state, and national government policy, be used to schedule access to physical resources such as elevators and workspaces within an organization, and inform contact tracing and quarantine actions for individuals. In all these cases, significant inequities are likely to arise and to be propagated and reinforced by data-driven decision systems. In this article, we propose a framework, called FIDES, for surfacing and reasoning about data equity in these systems.",data ethics; Data equity; responsible data science,,,2021-03,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,8V5AUBG3,0.0091743119266055,0.7142857142857143,0.125,0.0575216081829366
237,237,Hard Choices and Hard Limits in Artificial Intelligence,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462539,"Goodman, Bryce",2021.0,https://doi.org/10.1145/3461702.3462539,conferencePaper,"Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve.",AI ethics; fairness; hard choices; value alignment,978-1-4503-8473-5,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,112–121,AIES '21,Association for Computing Machinery,UDPALJ2W,0.0336134453781512,0.4285714285714285,0.0,0.0574789915966386
238,238,Applying a Principle of Explicability to AI Research in Africa: Should We Do It?,Ethics and Inf. Technol.,23.0,2,10.1007/s10676-020-09534-2,"Carman, Mary; Rosman, Benjamin",2021.0,https://doi.org/10.1007/s10676-020-09534-2,journalArticle,"Developing and implementing artificial intelligence (AI) systems in an ethical manner faces several challenges specific to the kind of technology at hand, including ensuring that decision-making systems making use of machine learning are just, fair, and intelligible, and are aligned with our human values. Given that values vary across cultures, an additional ethical challenge is to ensure that these AI systems are not developed according to some unquestioned but questionable assumption of universal norms but are in fact compatible with the societies in which they operate. This is particularly pertinent for AI research and implementation across Africa, a ground where AI systems are and will be used but also a place with a history of imposition of outside values. In this paper, we thus critically examine one proposal for ensuring that decision-making systems are just, fair, and intelligible—that we adopt a principle of explicability to generate specific recommendations—to assess whether the principle should be adopted in an African research context. We argue that a principle of explicability not only can contribute to responsible and thoughtful development of AI that is sensitive to African interests and values, but can also advance tackling some of the computational challenges in machine learning research. In this way, the motivation for ensuring that a machine learning-based system is just, fair, and intelligible is not only to meet ethical requirements, but also to make effective progress in the field itself.",Machine learning; Accountability; Africa; Intelligibility; Principle of explicability,,1388-1957,2021-06,2023-11-06 01:30:04,2023-11-06 01:30:04,107–117,,,MD4DNB2F,0.0598290598290598,0.0,0.0714285714285714,0.0573834662069956
239,239,Against “Democratizing AI”,AI Soc.,38.0,4,10.1007/s00146-021-01357-z,"Himmelreich, Johannes",2022.0,https://doi.org/10.1007/s00146-021-01357-z,journalArticle,"This paper argues against the call to democratize artificial intelligence (AI). Several authors demand to reap purported benefits that rest in direct and broad participation: In the governance of AI, more people should be more involved in more decisions about AI—from development and design to deployment. This paper opposes this call. The paper presents five objections against broadening and deepening public participation in the governance of AI. The paper begins by reviewing the literature and carving out a set of claims that are associated with the call to “democratize AI”. It then argues that such a democratization of AI (1) rests on weak grounds, because it does not answer to a demand of legitimization, (2) is redundant in that it overlaps with existing governance structures, (3) is resource intensive, which leads to injustices, (4) is morally myopic and thereby creates popular oversights and moral problems of its own, and finally, (5) is neither theoretically nor practically the right kind of response to the injustices that animate the call. The paper concludes by suggesting that AI should be democratized not by broadening and deepening participation but by increasing the democratic quality of the administrative and executive elements of collective decision making. In a slogan: The question is not so much whether AI should be democratized but how.",AI Ethics; Artificial Intelligence; Governance; Participation; Democracy; Public administration,,0951-5666,2022-01,2023-11-06 01:30:00,2023-11-06 01:30:00,1333–1346,,,SL7JI7VX,0.0324074074074074,0.3333333333333333,0.3333333333333333,0.0567795874857454
240,240,"Flickr Africa: Examining Geo-Diversity in Large-Scale, Human-Centric Visual Data","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604659,"Naggita, Keziah; LaChance, Julienne; Xiang, Alice",2023.0,https://doi.org/10.1145/3600211.3604659,conferencePaper,"Biases in large-scale image datasets are known to influence the performance of computer vision models as a function of geographic context. To investigate the limitations of standard Internet data collection methods in low- and middle-income countries, we analyze human-centric image geo-diversity on a massive scale using geotagged Flickr images associated with each nation in Africa. We report the quantity and content of available data with comparisons to population-matched nations in Europe as well as the distribution of data according to fine-grained intra-national wealth estimates. Temporal analyses are performed at two-year intervals to expose emerging data trends. Furthermore, we present findings for an “othering” phenomenon as evidenced by a substantial number of images from Africa being taken by non-local photographers. The results of our study suggest that further work is required to capture image data representative of African people and their environments and, ultimately, to improve the applicability of computer vision models in a global context.",Africa; AI Ethics; Computer Vision; Datasets; Geo-diversity; Machine Learning,9798400702310,,2023,2023-11-06 01:29:49,2023-11-06 01:29:49,520–530,AIES '23,Association for Computing Machinery,VFT6G42K,0.032258064516129,0.3333333333333333,0.1111111111111111,0.0562931414733607
241,241,Towards Pluralistic Value Alignment: Aggregating Value Systems Through ℓp-Regression,Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems,,,,"Lera-Leri, Roger; Bistaffa, Filippo; Serramia, Marc; Lopez-Sanchez, Maite; Rodriguez-Aguilar, Juan",2022.0,https://dl.acm.org/doi/10.5555/3535850.3535938,conferencePaper,"Dealing with the challenges of an interconnected globalised world requires to handle plurality. This is no exception when considering value-aligned intelligent systems, since the values to align with should capture this plurality. So far, most literature on value-alignment has just considered a single value system. Thus, this paper advances the state of the art by proposing a method for the aggregation of value systems. By exploiting recent results in the social choice literature, we formalise our aggregation problem as an optimisation problem. We then cast such problem as an ℓp-regression problem. By doing so, we provide a general theoretical framework that allows us to model and solve the above-mentioned problem. Our aggregation method allows us to consider a range of ethical principles, from utilitarian (maximum utility) to egalitarian (maximum fairness). We illustrate the aggregation of value systems by considering real-world data from the European Value Study. Specifically, we show how different consensus value systems can be obtained depending on the ethical principle of choice.",ethics; ai &amp; optimisation; value systems,978-1-4503-9213-6,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,780–788,AAMAS '22,International Foundation for Autonomous Agents and Multiagent Systems,8ZBMBQLA,0.0426829268292682,0.5,0.0,0.0561382444581158
242,242,Pedagogical Possibilities of Critical Engagement in Introductory CS Education,Proceedings of the 52nd ACM Technical Symposium on Computer Science Education,,,10.1145/3408877.3439659,"Yaghoobian, Hamed",2021.0,https://doi.org/10.1145/3408877.3439659,conferencePaper,"Computer science holds the unfortunate distinction, at least historically, as a pragmatic and often non-critical domain of practice - in terms of both theory and praxis. Nonetheless, we are witnessing a growing allegiance to AI and data ethics in both academia and industry, and purportedly, 200 university curricula claim to have complied with tech ethics [2]. This proliferation is a promising step forward and can be a useful corrective against discriminatory consequences concerning algorithmic systems, particularly in technical fields like CS and AI, where ethical issues are not discursively and normatively foregrounded. However, these efforts are more coordinated with conventional business ethics than more critical traditions of social justice [3] expected to prevail in educational settings [5]. It is required of educational systems to enable students to improve their technological sensibilities and skill sets to generate alternative possibilities that would challenge technocratic ideologies that are uncritical and limit innovation to the measures of efficiency and marketability.Nudged by these concerns and energized by the creative possibilities of a classroom, we realize the integration of the critical and social dimensions of computing technologies into introductory CS courses as a key area of attention and propose a pedagogical approach largely through concepts that have their legacies in intersectional feminist activism, collective organizing [1], and critical theory [4]. As reflected in the results from the surveys, this approach has resonated with %95.4 of the studied group of students.",computing technologies; critical theory; cs education; feminist ai,978-1-4503-8062-1,,2021,2023-11-06 01:30:04,2023-11-06 01:30:04,1267,SIGCSE '21,Association for Computing Machinery,VYG74Y6J,0.0555555555555555,0.125,0.0,0.0557287729906777
243,243,Managing Sustainability Tensions in Artificial Intelligence: Insights from Paradox Theory,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534175,"Mill, Eleanor; Garn, Wolfgang; Ryman-Tubb, Nick",2022.0,https://doi.org/10.1145/3514094.3534175,conferencePaper,"This paper offers preliminary reflections on the sustainability tensions present in Artificial Intelligence (AI) and suggests that Paradox Theory, an approach borrowed from the strategic management literature, may help guide scholars towards innovative solutions. The benefits of AI to our society are well documented. Yet those benefits come at environmental and sociological cost, a fact which is often overlooked by mainstream scholars and practitioners. After examining the nascent corpus of literature on the sustainability tensions present in AI, this paper introduces the Accuracy - Energy Paradox and suggests how the principles of paradox theory can guide the AI community to a more sustainable solution.",green ai; paradox theory; sustainability of ai; sustainable ai,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,491–498,AIES '22,Association for Computing Machinery,NE7BUA9D,0.0384615384615384,0.3333333333333333,0.0,0.0556594254510921
244,244,Ordinary People as Moral Heroes and Foes: Digital Role Model Narratives Propagate Social Norms in China's Social Credit System,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534180,"Chen, Mo; Engelmann, Severin; Grossklags, Jens",2022.0,https://doi.org/10.1145/3514094.3534180,conferencePaper,"The Chinese Social Credit System (SCS) is a digital sociotechnical credit system that rewards and sanctions economic and social behaviors of individuals and companies. As a complex and transformative digital credit system, the SCS uses digital communication channels to inform the Chinese public about behaviors that lead to reward or sanction. Since 2017, the Chinese government has been publishing ""blameworthy"" and ""praiseworthy"" role model narratives of ordinary Chinese citizens on its central SCS information platform creditchina.gov.cn. Across many cultures, role model narratives are a known instrument to convey ""appropriate"" and ""inappropriate"" social norms. Using a directed content analysis methodology, we study the SCS-specific social norms embedded in 100 ""praiseworthy"" and 100 ""blameworthy"" role model narratives published on creditchina.gov.cn. ""Blameworthy"" role model narratives stress social norms associated with an ""immoral"" SCS identity label termed ""Lao Lai"" - a ""moral foe"" that fails to repay debt. SCS role model narratives familiarize Chinese society with SCS-specific measures such as digital surveillance, public shaming, and disproportionate punishment. Our study makes progress towards understanding how a state-run sociotechnical credit system combines digital tools with culturally familiar customs to propagate ""blameworthy"" and ""praiseworthy"" identities.",china; moral education; narrative study; social credit system,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,181–191,AIES '22,Association for Computing Machinery,ABVY7SHZ,0.0531914893617021,0.0,0.1052631578947368,0.0552931049409787
245,245,The AI Mirror: Reclaiming Our Humanity in an Age of Machine Thinking,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539567,"Vallor, Shannon",2022.0,https://doi.org/10.1145/3514094.3539567,conferencePaper,"The new interdisciplinary field of AI ethics has revealed the extent to which AI systems tend to reflect back and amplify human vices: our unfair biases and discriminatory behaviours, our penchant for consuming and spreading misinformation, and our tendency to pursue narrow gains while losing sight of the bigger picture. While this is true, the mirror metaphor conveys the misleading and dangerous impression that AI merely captures and replicates our humanity in software. Yet we all know that a mirror does not capture the embodied human presence. Glass mirrors erase and occlude much of our material and conscious reality. Mirror images convey no smell, no depth, no softness, no fear, no hope, no imagination. What does the AI mirror occlude? In this talk I explore the dimensions of our humanity that AI's transformation of the socioeconomic and moral order makes it harder for us to see in ourselves and in one another, and why our futures depend upon bringing these vital aspects of our humanity back into view.",artificial intelligence; bias; consciousness; ethics; humanity,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,6,AIES '22,Association for Computing Machinery,7K88DR85,0.0357142857142857,0.3333333333333333,0.0833333333333333,0.054575569358178
246,246,Respect as a Lens for the Design of AI Systems,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534186,"Seymour, William; Van Kleek, Max; Binns, Reuben; Murray-Rust, Dave",2022.0,https://doi.org/10.1145/3514094.3534186,conferencePaper,"Critical examinations of AI systems often apply principles such as fairness, justice, accountability, and safety, which is reflected in AI regulations such as the EU AI Act. Are such principles sufficient to promote the design of systems that support human flourishing? Even if a system is in some sense fair, just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or otherwise conflict with cultural, individual, or social values. This paper proposes a dimension of interactional ethics thus far overlooked: the ways AI systems should treat human beings. For this purpose, we explore the philosophical concept of respect: if respect is something everyone needs and deserves, shouldn't technology aim to be respectful? Despite its intuitive simplicity, respect in philosophy is a complex concept with many disparate senses. Like fairness or justice, respect can characterise how people deserve to be treated; but rather than relating primarily to the distribution of benefits or punishments, respect relates to how people regard one another, and how this translates to perception, treatment, and behaviour. We explore respect broadly across several literatures, synthesising perspectives on respect from Kantian, post-Kantian, dramaturgical, and agential realist design perspectives with a goal of drawing together a view of what respect could mean for AI. In so doing, we identify ways that respect may guide us towards more sociable artefacts that ethically and inclusively honour and recognise humans using the rich social language that we have evolved to interact with one another every day.",AI systems; ethical design; respect,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,641–652,AIES '22,Association for Computing Machinery,FMMUL74V,0.037037037037037,0.8,0.1,0.0544693102996989
247,247,Disambiguating Algorithmic Bias: From Neutrality to Justice,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604695,"Edenberg, Elizabeth; Wood, Alexandra",2023.0,https://doi.org/10.1145/3600211.3604695,conferencePaper,"As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term ‘bias.’ Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination. In this paper, we examine the promises and challenges of different approaches to disambiguating bias and designing for justice. While both approaches aid in understanding and addressing clear algorithmic harms, we argue that they also risk being leveraged in ways that ultimately deflect accountability from those building and deploying these systems. Applying this analysis to recent examples of generative AI, our argument highlights unseen dangers in current methods of evaluating algorithmic bias and points to ways to redirect approaches to addressing bias in generative AI at its early stages in ways that can more robustly meet the demands of justice.",algorithms; bias; discrimination; fairness; generative AI; justice; large language models; law; philosophy; vision-language models,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,691–704,AIES '23,Association for Computing Machinery,SQ6SADEH,0.0368663594470046,0.2142857142857142,0.1428571428571428,0.0541427132185016
248,248,Governing AI Applications To Monitoring and Managing Our Global Environmental Commons,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539540,"Chapman, Melissa",2022.0,https://doi.org/10.1145/3514094.3539540,conferencePaper,"Artificial Intelligence (AI) is playing a rapidly growing role in our response to global environmental change, from how we collect and process ecological and earth system data to how we make and enforce management decisions. Importantly, AI permits the use of increasingly high-resolution data, promising more accurate monitoring of environmental change and more targeted interventions. But, while AI promises several potential contributions to addressing global environmental change, how do we ensure these technologies stand to enhance equity rather than exasperate existing environmental injustices and power asymmetries?",biodiversity; climate change; conservation by algorithm; environmental policy; social-environmental systems,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,893,AIES '22,Association for Computing Machinery,QI7T6W9A,0.0581395348837209,0.0,0.0909090909090909,0.0539293412925423
249,249,Algorithmic Fairness and Structural Injustice: Insights from Feminist Political Philosophy,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534188,"Kasirzadeh, Atoosa",2022.0,https://doi.org/10.1145/3514094.3534188,conferencePaper,"Data-driven predictive algorithms are widely used to automate and guide high-stake decision making such as bail and parole recommendation, medical resource distribution, and mortgage allocation. Nevertheless, harmful outcomes biased against vulnerable groups have been reported. The growing research field known as 'algorithmic fairness' aims to mitigate these harmful biases. Its primary methodology consists in proposing mathematical metrics to address the social harms resulting from an algorithm's biased outputs. The metrics are typically motivated by – or substantively rooted in – ideals of distributive justice, as formulated by political and legal philosophers. The perspectives of feminist political philosophers on social justice, by contrast, have been largely neglected. Some feminist philosophers have criticized the local scope of the paradigm of distributive justice and have proposed corrective amendments to surmount its limitations. The present paper brings some key insights of feminist political philosophy to algorithmic fairness. The paper has three goals. First, I show that algorithmic fairness does not accommodate structural injustices in its current scope. Second, I defend the relevance of structural injustices – as pioneered in the contemporary philosophical literature by Iris Marion Young – to algorithmic fairness. Third, I take some steps in developing the paradigm of 'responsible algorithmic fairness' to correct for errors in the current scope and implementation of algorithmic fairness. I close by some reflections of directions for future research.",feminist philosophy; algorithmic bias; algorithmic fairness; ethics of artificial intelligence; responsibility; distributive justice; ethical machine learning; algorithmic justice; political philosophy; structural injustice,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,349–356,AIES '22,Association for Computing Machinery,LMYQHZUK,0.0134529147982062,0.3636363636363636,0.0,0.0538948646977699
250,250,"Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462605,"Kelley, Patrick Gage; Yang, Yongwei; Heldreth, Courtney; Moessner, Christopher; Sedley, Aaron; Kramm, Andreas; Newman, David T.; Woodruff, Allison",2021.0,https://doi.org/10.1145/3461702.3462605,conferencePaper,"As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.",artificial intelligence; public perception,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,627–637,AIES '21,Association for Computing Machinery,R6ND4KGE,0.0620155038759689,0.0,0.0,0.0537428586096382
251,251,Typology of Risks of Generative Text-to-Image Models,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604722,"Bird, Charlotte; Ungless, Eddie; Kasirzadeh, Atoosa",2023.0,https://doi.org/10.1145/3600211.3604722,conferencePaper,"This paper investigates the direct risks and harms associated with modern text-to-image generative models, such as DALL-E and Midjourney, through a comprehensive literature review. While these models offer unprecedented capabilities for generating images, their development and use introduce new types of risk that require careful consideration. Our review reveals significant knowledge gaps concerning the understanding and treatment of these risks despite some already being addressed. We offer a taxonomy of risks across six key stakeholder groups, inclusive of unexplored issues, and suggest future research directions. We identify 22 distinct risk types, spanning issues from data bias to malicious use. The investigation presented here is intended to enhance the ongoing discourse on responsible model development and deployment. By highlighting previously overlooked risks and gaps, it aims to shape subsequent research and governance initiatives, guiding them toward the responsible, secure, and ethically conscious evolution of text-to-image models.",AI ethics; AI governance; AI risks; AI safety; Generative AI; Generative models; Responsible AI; Text-to-Image models,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,396–410,AIES '23,Association for Computing Machinery,3FJ6XTMF,0.0068965517241379,0.5,0.0,0.0536239215355647
252,252,Explainability in Process Mining: A Framework for Improved Decision-Making,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604729,"Nannini, Luca",2023.0,https://doi.org/10.1145/3600211.3604729,conferencePaper,"This research project aims to develop and validate explanatory facilities to enhance information reception of process mining solutions, which could inform and be translated to other business intelligence platforms. Process mining, a nascent field for analyzing event data stored in information systems, faces challenges in adoption, engagement, and comprehensive explainability frameworks. The research problem lies in the difficulties organizations face when understanding the return on investment and integration requirements associated with process mining operationalization. Furthermore, users often struggle to comprehend the elaboration and representation of process outputs. This issue is compounded by the limited application of Explainable AI (XAI) in process mining, which so far has been predominantly focused on prediction and monitoring activities without a holistic view of explainability trade-offs.",AI Ethics; AI policy; Explainable AI; Human-Computer Interaction; Responsible Process Mining,9798400702310,,2023,2023-11-06 01:29:49,2023-11-06 01:29:49,975–976,AIES '23,Association for Computing Machinery,BD24HYQE,0.0165289256198347,0.4545454545454545,0.0,0.0527089072543617
253,253,Measuring Lay Reactions to Personal Data Markets,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462582,"Nielsen, Aileen",2021.0,https://doi.org/10.1145/3461702.3462582,conferencePaper,"The recording, aggregation, and exchange of personal data is necessary to the development of socially-relevant machine learning applications. However, anecdotal and survey evidence show that ordinary people feel discontent and even anger regarding data collection practices that are currently typical and legal. This suggests that personal data markets in their current form do not adhere to the norms applied by ordinary people. The present study experimentally probes whether market transactions in a typical online scenario are accepted when evaluated by lay people. The results show that a high percentage of study participants refused to participate in a data pricing exercise, even in a commercial context where market rules would typically be expected to apply. For those participants who did price the data, the median price was an order of magnitude higher than the market price. These results call into question the notice and consent market paradigm that is used by technology firms and government regulators when evaluating data flows. The results also point to a conceptual mismatch between cultural and legal expectations regarding the use of personal data.",experimental law and economics; personal data markets; privacy law,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,807–813,AIES '21,Association for Computing Machinery,BU6E9XUC,0.0393258426966292,0.2222222222222222,0.1428571428571428,0.0525870593942119
255,255,Generating Deontic Obligations From Utility-Maximizing Systems,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534163,"Shea-Blymyer, Colin; Abbas, Houssam",2022.0,https://doi.org/10.1145/3514094.3534163,conferencePaper,"This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs - meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.",explainability; machine ethics; deontic logic; model checking; normative systems,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,653–663,AIES '22,Association for Computing Machinery,Z2D5FCXH,0.0458015267175572,0.2222222222222222,0.0,0.0519722771192456
256,256,A Framework for Understanding AI-Induced Field Change: How AI Technologies Are Legitimized and Institutionalized,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462591,"Larsen, Benjamin Cedric",2021.0,https://doi.org/10.1145/3461702.3462591,conferencePaper,"Artificial intelligence (AI) systems operate in increasingly diverse areas, from healthcare to facial recognition, the stock market, autonomous vehicles, and so on. While the underlying digital infrastructure of AI systems is developing rapidly, each area of implementation is subject to different degrees and processes of legitimization. By combining elements from institutional theory and information systems-theory, this paper presents a conceptual framework to analyze and understand AI-induced field-change. The introduction of novel AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape organizations and institutions while existing institutional infrastructures determine the scope and speed at which organizational change is allowed to occur. Where institutional infrastructure and governance arrangements, such as standards, rules, and regulations, still are unelaborate, the field can move fast but is also more likely to be contested. The institutional infrastructure surrounding AI-induced fields is generally little elaborated, which could be an obstacle to the broader institutionalization of AI-systems going forward.",AI; digital infrastructure; field change; institutional infrastructure; legitimization,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,683–694,AIES '21,Association for Computing Machinery,B7VBG9GV,0.0258064516129032,0.375,0.0714285714285714,0.0516069821420112
257,257,AI + Dance: Co-Designing Culturally Sustaining Curricular Resources for AI and Ethics Education Through Artistic Computing,Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 2,,,10.1145/3501709.3544275,"Castro, Francisco Enrique Vicente; DesPortes, Kayla; Payne, William; Bergner, Yoav; McDermott, Kathleen",2022.0,https://doi.org/10.1145/3501709.3544275,conferencePaper,"Artificial intelligence (AI) and machine learning (ML) systems are ubiquitous across many fields ranging from medicine (e.g., tumor detection), to natural language processing (e.g., digital home assistants, auto-translate tools), and personalization (e.g., social media recommendations). The rise of these systems has also seen a rise in the ethical challenges resulting from how some of these systems are designed and implemented. Most problematic is their propagation and amplification of problems such as racism and sexism [4, 6], among others. Further, these challenges exist within a computing discipline that is already burdened by exclusive, marginalizing cultures and practices that lead to low participation by women and Black, Indigenous, and People of Color (BIPOC) [3]. Our project, AI + Dance, attends to these dimensions of inequity within AI/ML education through developing our understanding of how we can equip learners to recognize and rectify issues of AI and ML systems within an inclusive and culturally sustaining experience. We explore this in collaboration with STEM From Dance1 (SFD), a non-profit organization that supports girls of color in creative production with dance, CS, and STEM. In our prior work with SFD, we developed danceON, an open-access creative coding environment that enables learners to create code to engage authentically with dance and body motion [5]. With danceON, learners can code animations that can bind and respond to body positions and be statically and dynamically positioned in space. The system provides two ways to explore AI and ML. First, it integrates a pose detection machine learning model (PoseNet [8]) that consists of body points (i.e., Left Ear, Nose, etc.) and confidence scores for these key points, which are all accessible within the code. Second, danceON enables learners to import a model trained in Google’s Teachable Machine [2] to access the probabilistic classifier to manipulate and trigger animations. While we have explored the use of danceON to enable learners to build culturally relevant artistic artifacts [5], we have yet to explore how we can teach AI and ML concepts and ethics through the use of the system. We investigate two research questions through the co-design of culturally sustaining AI/ML resources with teachers: RQ1) How can we authentically build on learners’ identities, cultural knowledge, and practices with dance as they build, explore, and critique ML and AI models and systems? RQ2) How can we leverage learners’ creative, embodied experiences with ML and AI to facilitate reflection and critique of CS and AI/ML within their communities and society more broadly? We focus on co-design as a method that facilitates sustainability through centering teachers’ values, ownership, and authentic contexts in the realization, implementation, and evaluation of learning designs [7]. Through co-designing curricular resources, we will develop a set of modules aligned with the AI4K12 guidelines [1] that builds our understanding of creating culturally sustaining educational resources to teach about AI and ethical design in ways that leverage the culturally situated, collaborative, and embodied nature of dance.",AI ethics; Artificial intelligence; Machine learning; Artistic computing; Creative computing; Dance; danceON,978-1-4503-9195-5,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,26–27,ICER '22,Association for Computing Machinery,QBTM5LLM,0.0371900826446281,0.25,0.25,0.0514103850826686
258,258,Where is the Human in Human-Centered AI? Insights from Developer Priorities and User Experiences,Comput. Hum. Behav.,141.0,C,10.1016/j.chb.2022.107617,"Bingley, William J.; Curtis, Caitlin; Lockey, Steven; Bialkowski, Alina; Gillespie, Nicole; Haslam, S. Alexander; Ko, Ryan K.L.; Steffens, Niklas; Wiles, Janet; Worthy, Peter",2023.0,https://doi.org/10.1016/j.chb.2022.107617,journalArticle,"Human-centered artificial intelligence (HCAI) seeks to shift the focus in AI development from technology to people. However, it is not clear whether existing HCAI principles and practices adequately accomplish this goal. To explore whether HCAI is sufficiently focused on people, we conducted a qualitative survey of AI developers (N = 75) and users (N = 130) and performed a thematic content analysis on their responses to gain insight into their differing priorities and experiences. Through this, we were able to compare HCAI in principle (guidelines and frameworks) and practice (developer priorities) with user experiences. We found that the social impact of AI was a defining feature of positive user experiences, but this was less of a priority for developers. Furthermore, our results indicated that improving AI functionality from the perspective of the user is an important part of making it human-centered. Indeed, users were more concerned about being understood by AI than about understanding AI. In line with HCAI guidelines, developers were concerned with issues such as ethics, privacy, and security, demonstrating an ‘avoidance of harm’ perspective. However, our results suggest that an increased focus on what people need in their lives is required for HCAI to be truly human-centered.",Artificial intelligence; Human-centered AI; Developers; Human needs; User experience,,0747-5632,2023-04,2023-11-06 01:30:04,2023-11-06 01:30:04,,,,I6YVSNPH,0.0459183673469387,0.1111111111111111,0.0714285714285714,0.0512623795303691
259,259,Governing Silicon Valley and Shenzhen: Assessing a New Era of Artificial Intelligence Governance in the US and China,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604746,"Hine, Emmie",2023.0,https://doi.org/10.1145/3600211.3604746,conferencePaper,"The United States and China are both striving to be the world leader in AI, with conflicting visions. However, the intensifying“clash of civilizations” narrative ignores factors integral to each country’s AI strategy. This project uses a philosophy-grounded framework and natural language processing (NLP) methods to analyze what policy differences exist and why they exist. It examines new developments and argues that while obstacles to cooperation still exist, ethical convergences offer hope.",artificial intelligence; China; geopolitics; governance; United States,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,947–949,AIES '23,Association for Computing Machinery,556P5C77,0.0704225352112676,0.0,0.0,0.0512163892445582
260,260,Are AI Ethics Conferences Different and More Diverse Compared to Traditional Computer Science Conferences?,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462616,"Acuna, Daniel E.; Liang, Lizhen",2021.0,https://doi.org/10.1145/3461702.3462616,conferencePaper,"Even though computer science (CS) has had a historical lack of gender and race representation, its AI research affects everybody eventually. Being partially rooted in CS conferences, ""AI ethics"" (AIE) conferences such as FAccT and AIES have quickly become distinct venues where AI's societal implications are discussed and solutions proposed. However, it is largely unknown if these conferences improve upon the historical representational issues of traditional CS venues. In this work, we explore AIE conferences' evolution and compare them across demographic characteristics, publication content, and citation patterns. We find that AIE conferences have increased their internal topical diversity and impact on other CS conferences. Importantly, AIE conferences are highly differentiable, covering topics not represented in other venues. However, and perhaps contrary to the field's aspirations, white authors are more common while seniority and black researchers are represented similarly to CS venues. Our results suggest that AIE conferences could increase efforts to attract more diverse authors, especially considering their sizable roots in CS.",artificial intelligence; content and citation analyses; ethics conferences; science of science,978-1-4503-8473-5,,2021,2023-11-06 01:29:48,2023-11-06 01:29:48,307–315,AIES '21,Association for Computing Machinery,9L277HJ8,0.0246913580246913,0.1818181818181818,0.2142857142857142,0.0504805024035793
261,261,Ethical Recommenders in the Public Library Sector,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539536,"Daniil, Savvina",2022.0,https://doi.org/10.1145/3514094.3539536,conferencePaper,"Recommender Systems as an algorithmic class hide lurking risks despite their prevalence in academic and commercial circles. My specific research revolves around tracking and mitigating potential risks specifically in the Public Library domain. In collaboration with the National Library of The Netherlands, I am working on investigating whether the incorporation of Recommenders in a library's loaning system serves their social responsibility and purpose, with securing inclusivity being the main point of interest.",,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,895,AIES '22,Association for Computing Machinery,3J549CYM,0.0138888888888888,0.0,0.4285714285714285,0.0498525073746312
262,262,The Bureaucratic Challenge to AI Governance: An Empirical Assessment of Implementation at U.S. Federal Agencies,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604701,"Lawrence, Christie; Cui, Isaac; Ho, Daniel",2023.0,https://doi.org/10.1145/3600211.3604701,conferencePaper,"Can government govern artificial intelligence (AI)? One of the central questions of AI governance surrounds state capacity, namely whether government has the ability to accomplish its policy goals. We study this question by assessing how well the U.S. federal government has implemented three binding laws around AI governance: two executive orders—concerning trustworthy AI in the public sector (E.O. 13,960) and AI leadership (E.O. 13,859)—and the AI in Government Act. We conduct the first systematic empirical assessment of the implementation status of these three laws, which have each been described as central to US AI innovation. First, we track, through extensive research, line-level adoption of each mandated action. Based on publicly available information, we find that fewer than 40 percent of 45 legal requirements could be verified as having been implemented. Second, we research the specific implementation of transparency requirements at up to 220 federal agencies. We find that nearly half of agencies failed to publicly issue AI use case inventories—even when these agencies have demonstrable use cases of machine learning. Even when agencies have complied with these requirements, efforts are inconsistent. Our work highlights the weakness of U.S. state capacity to carry out AI governance mandates and we discuss implications for how to address bureaucratic capacity challenges.",AI policy; bureaucracy; policy implementation; regulation,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,606–652,AIES '23,Association for Computing Machinery,75VZ8GI3,0.0434782608695652,0.1666666666666666,0.0666666666666666,0.0496018423462168
263,263,Notions Of&nbsp;Fairness In&nbsp;Automated Decision Making: An Interdisciplinary Approach To&nbsp;Open Issues,"Electronic Government and the Information Systems Perspective: 11th International Conference, EGOVIS 2022, Vienna, Austria, August 22–24, 2022, Proceedings",,,10.1007/978-3-031-12673-4_1,"Yousefi, Yasaman",2022.0,https://doi.org/10.1007/978-3-031-12673-4_1,conferencePaper,"Artificial Intelligence (AI) systems share complex characteristics including opacity, that often do not allow for transparent reasoning behind a given decision. As the use of Machine Leaning (ML) systems is exponentially increasing in decision-making contexts, not being able to understand why and how decisions were made, raises concerns regarding possible discriminatory outcomes that are not in line with the shared fundamental values. However, mitigating (human) discrimination through the application of the concept of fairness in ML systems leaves room for further studies in the field. This work gives an overview of the problem of discrimination in Automated Decision-Making (ADM) and assesses the existing literature for possible legal and technical solutions to defining fairness in ML systems.",AI ethics; Fairness; Automated Decision-Making; Machine Learning,978-3-031-12672-7,,2022,2023-11-06 01:30:00,2023-11-06 01:30:00,3–17,,Springer-Verlag,HYTPV4IN,0.0258620689655172,0.4285714285714285,0.0,0.0493568831902224
264,264,Welcome to AI Matters 7(1),AI Matters,7.0,1,10.1145/3465074.3465075,"Leite, Iolanda; Karpatne, Anuj",2021.0,https://doi.org/10.1145/3465074.3465075,journalArticle,"Welcome to the first issue of this year's AI Matters Newsletter!We start with a report on upcoming SIGAI Events by Dilini Samarasinghe and Conference reports by Louise Dennis, our conference coordination officers. In our regular Education column, Duri Long, Jonathan Moon, and Brian Magerko introduce two ""unplugged"" activities (i.e., no technology needed) to learn about AI focussed on K-12 AI Education. We then bring you our regular Policy column, where Larry Medsker covers several topics on AI policy, including the role of Big Tech on AI Ethics and an interview with Dr. Eric Daimler who is the CEO of the MIT-spinout Conexus.com.Finally, we close with four article contributions. The first article discusses emerging applications of AI in analyzing source code and its implications to several industries. The second article discusses topics in the area of physical scene understanding that are necessary for machines to perceive, interact, and reason about the physical world. The third article presents novel practices and highlights from the Fourth Workshop on Mechanism Design for Social Good. The fourth article provides a report on the ""Decoding AI"" event that was conducted online by ViSER for high school students and adults sponsored by ACM SIGAI.",,,,2021-07,2023-11-06 01:30:03,2023-11-06 01:30:03,4,,,SR6B5L8K,0.0456852791878172,0.0,0.2,0.0488003278998644
265,265,Fairness and Data Protection Impact Assessments,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462528,"Kasirzadeh, Atoosa; Clifford, Damian",2021.0,https://doi.org/10.1145/3461702.3462528,conferencePaper,"In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.",algorithmic fairness; ethics of artificial intelligence; data protection impact assessments; fairness principle; general data protection regulation; regulation of artificial intelligence,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,146–153,AIES '21,Association for Computing Machinery,CMPSIBR5,0.0211640211640211,0.2,0.1666666666666666,0.0487066835281121
266,266,Legitimacy and Automated Decisions: The Moral Limits of Algocracy,Ethics and Inf. Technol.,24.0,3,10.1007/s10676-022-09647-w,"Chomanski, Bartek",2022.0,https://doi.org/10.1007/s10676-022-09647-w,journalArticle,"With the advent of automated decision-making, governments have increasingly begun to rely on artificially intelligent algorithms to inform policy decisions across a range of domains of government interest and influence. The practice has not gone unnoticed among philosophers, worried about “algocracy” (rule by algorithm), and its ethical and political impacts. One of the chief issues of ethical and political significance raised by algocratic governance, so the argument goes, is the lack of transparency of algorithms.One of the best-known examples of philosophical analyses of algocracy is John Danaher’s “The threat of algocracy” (2016), arguing that government by algorithm undermines political legitimacy. In this paper, I will treat Danaher’s argument as a springboard for raising additional questions about the connections between algocracy, comprehensibility, and legitimacy, especially in light of empirical results about what we can expect the voters and policymakers to know.The paper has the following structure: in Sect.&nbsp;2, I introduce the basics of Danaher’s argument regarding algocracy. In Sect.&nbsp;3 I argue that the algocratic threat to legitimacy has troubling implications for social justice. In Sect.&nbsp;4, I argue that, nevertheless, there seem to be good reasons for governments to rely on algorithmic decision support systems. Lastly, I try to resolve the apparent tension between the findings of the two preceding Sections.",AI ethics; Algocracy; Algorithmic governance; Legitimacy,,1388-1957,2022-09,2023-11-06 01:30:00,2023-11-06 01:30:00,,,,NHDHIIJA,0.0334928229665071,0.5,0.0,0.0486493852583126
267,267,Differentially Private Normalizing Flows for Privacy-Preserving Density Estimation,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462625,"Waites, Chris; Cummings, Rachel",2021.0,https://doi.org/10.1145/3461702.3462625,conferencePaper,"Normalizing flow models have risen as a popular solution to the problem of density estimation, enabling high-quality synthetic data generation as well as exact probability density evaluation. However, in contexts where individuals are directly associated with the training data, releasing such a model raises privacy concerns. In this work, we propose the use of normalizing flow models that provide explicit differential privacy guarantees as a novel approach to the problem of privacy-preserving density estimation. We evaluate the efficacy of our approach empirically using benchmark datasets, and we demonstrate that our method substantially outperforms previous state-of-the-art approaches. We additionally show how our algorithm can be applied to the task of differentially private anomaly detection.",differential privacy; anomaly detection; density estimation; normalizing flows; synthetic data generation,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,1000–1009,AIES '21,Association for Computing Machinery,MLPUWEFA,0.0353982300884955,0.1818181818181818,0.0,0.0479575856417286
268,268,On the Privacy Risks of Model Explanations,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462533,"Shokri, Reza; Strobel, Martin; Zick, Yair",2021.0,https://doi.org/10.1145/3461702.3462533,conferencePaper,"Privacy and transparency are two key foundations of trustworthy machine learning. Model explanations offer insights into a model's decisions on input data, whereas privacy is primarily concerned with protecting information about the training data. We analyze connections between model explanations and the leakage of sensitive information about the model's training set. We investigate the privacy risks of feature-based model explanations using membership inference attacks: quantifying how much model predictions plus their explanations leak information about the presence of a datapoint in the training set of a model. We extensively evaluate membership inference attacks based on feature-based model explanations, over a variety of datasets. We show that backpropagation-based explanations can leak a significant amount of information about individual training datapoints. This is because they reveal statistical information about the decision boundaries of the model about an input, which can reveal its membership. We also empirically investigate the trade-off between privacy and explanation quality, by studying the perturbation-based model explanations.",privacy; membership inference; model explanations,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,231–241,AIES '21,Association for Computing Machinery,NDMJ7FHY,0.0379746835443038,0.2,0.1428571428571428,0.0478264822653479
269,269,Self-Determination through Explanation: An Ethical Perspective on the Implementation of the Transparency Requirements for Recommender Systems Set by the Digital Services Act of the European Union,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604717,"Fabbri, Matteo",2023.0,https://doi.org/10.1145/3600211.3604717,conferencePaper,"In the contemporary information age, recommender systems (RSs) play a critical role in influencing online behaviour: from social media to e-commerce, from music streaming to news aggregators, individuals are constantly targeted by personalized recommendations suggesting contents that may interest them. Despite such diffusion, the extent to which recommendations influence users’ decisions is still underexplored, given that independent audits on the structure and functioning of RSs deployed on online platforms are usually prevented by proprietary constraints. The nudging potential of RSs can represent a risk for vulnerable people: indeed, judicial cases involving platforms’ responsibility for displaying recommendations that may lead to political radicalization or endangerment of minors have recently caught public attention. The Digital Services Act of the European Union (DSA) is the first supranational regulation that sets specific transparency and auditing requirements for RSs implemented by online platforms with the aim of enhancing users’ self-determination: in particular, it allows users to modify the parameters on which recommendations rely so to let them choose autonomously which kind of content they want to see. This research focuses on whether and how the enforcement of this regulation can mitigate the unfair consequences of the power imbalance between online platforms and users. To this aim, I discuss the harms arising from digital nudging based on RSs and propose explanations as a tool that can reduce the impact of those harms by increasing users’ awareness. Through a comparative analysis of relevant articles of the DSA, the General Data Protection Regulation (GDPR) and the AI Act, I outline how the provisions of the DSA fill some of the gaps left by other relevant European regulations, while leaving the so-called right to explanation substantially unaddressed. As a result of this analysis, I argue that, in order for the implementation of the DSA provisions on recommender systems to be effective, policy-makers should: 1) enhance users’ awareness through clear and easily accessible explanations on how the recommendation process works and how they can be influenced by it; 2) grant users the possibility of intervening directly on the strategies through which RSs target them on the platform’s interface.",Digital Nudging; Digital Services Act; Recommender Systems; Regulation of AI; Transparency,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,653–661,AIES '23,Association for Computing Machinery,BJTF2VZG,0.0201149425287356,0.4545454545454545,0.1923076923076923,0.0477170436637152
270,270,"Benchmarked Ethics: A Roadmap to AI Alignment, Moral Knowledge, and Control","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604764,"Kierans, Aidan",2023.0,https://doi.org/10.1145/3600211.3604764,conferencePaper,"Today’s artificial intelligence (AI) systems rely heavily on Artificial Neural Networks (ANNs), yet their black box nature induces risk of catastrophic failure and harm. In order to promote verifiably safe AI, my research will determine constraints on incentives from a game-theoretic perspective, tie those constraints to moral knowledge as represented by a knowledge graph, and reveal how neural models meet those constraints with novel interpretability methods. Specifically, I will develop techniques for describing models’ decision-making processes by predicting and isolating their goals, especially in relation to values derived from knowledge graphs. My research will allow critical AI systems to be audited in service of effective regulation.",interpretability; auditing; alignment; control; knowledge graphs,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,964–965,AIES '23,Association for Computing Machinery,XF9KYZQA,0.0283018867924528,0.0,0.2727272727272727,0.046864339815368
271,271,"An Expert Interview Study of IoT Wearable Technologies for an Aging Population from Product, Data, and Society Dimensions","Human Aspects of IT for the Aged Population. Supporting Everyday Life Activities: 7th International Conference, ITAP 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part II",,,10.1007/978-3-030-78111-8_29,"Lee, Sheng-Hung; Zhu, Ziyuan; Lee, Chaiwoo; Duarte, Fabio; Coughlin, Joseph F.",2021.0,https://doi.org/10.1007/978-3-030-78111-8_29,conferencePaper,"This research focuses on investigating the product, data, and society dimensions around IoT (Internet of Things) wearable technologies with insights and empirical knowledge from exploratory expert interviews. The purpose is to find implications around future designs of IoT wearable technologies for the aging population. Quantitative and qualitative data were collected through in-depth expert interviews and pre-surveys to explore topics and insights related to the design and development of IoT wearable technologies. Through synthesizing findings from expert interviews and pre-surveys, insights and concerning issues were summarized into three dimensions: product, data, and society. The implications from this research can help overcome the obstacles that impede the inclusiveness and adaptability of IoT wearable technologies. This study concludes that it is essential for designers, engineers, and researchers to consider these non-technological issues when designing and developing future IoT wearable technologies for the aging population.",Data ethics; Aging population; Design with data; Expert interview; IoT wearable technology,978-3-030-78110-1,,2021,2023-11-06 01:30:02,2023-11-06 01:30:02,428–437,,Springer-Verlag,6UH2LGIT,0.0212765957446808,0.3333333333333333,0.0555555555555555,0.0468582310780892
272,272,Prototyping An&nbsp;End-User User Interface For&nbsp;the&nbsp;Solid Application Interoperability Specification Under GDPR,"The Semantic Web: 20th International Conference, ESWC 2023, Hersonissos, Crete, Greece, May 28–June 1, 2023, Proceedings",,,10.1007/978-3-031-33455-9_33,"Bailly, Hadrien; Papanna, Anoop; Brennan, Rob",2023.0,https://doi.org/10.1007/978-3-031-33455-9_33,conferencePaper,"This paper describes prototyping of the draft Solid application interoperability specification (INTEROP). We developed and evaluated a dynamic user interface (UI) for the new Solid application access request and authorization extended with the Data Privacy Vocabulary. Solid places responsibility on users to control their data. INTEROP adds new declarative access controls. Solid applications to date have provided few policy interfaces with high usability. GDPR controls on usage are rarely addressed. Implementation identified specification and Semantic Web tool issues and also in the understandability of declarative policies, a key concern under GDPR or data ethics best practices. The prototype was evaluated in a usability and task accuracy experiment, where the UI enabled users to create access and usage control policies with an accuracy of between 72 and 37%. Overall, the UI had a poor usability rating, with a median SUS (system usability scale) score of 37.67. Experimental participants were classified according to the Westin privacy scale to investigate the impact of user attitudes to privacy on the results. The paper discusses the findings of the study and their consequences for future data sovereignty access request and authorization UI designs.",Access Control; Consent; GDPR; Solid; User Interface,978-3-031-33454-2,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,557–573,,Springer-Verlag,7RYNYQF3,0.0531914893617021,0.0,0.0,0.0468055576062722
273,273,We Haven't Gone Paperless Yet: Why the Printing Press Can Help Us Understand Data and AI,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462604,"Posada, Julian; Weller, Nicholas; Wong, Wendy H.",2021.0,https://doi.org/10.1145/3461702.3462604,conferencePaper,"How should we understand the social and political effects of the datafication of human life? This paper argues that the effects of data should be understood as a constitutive shift in social and political relations. We explore how datafication, or quantification of human and non-human factors into binary code, affects the identity of individuals and groups. This fundamental shift goes beyond economic and ethical concerns, which has been the focus of other efforts to explore the effects of datafication and AI. We highlight that technologies such as datafication and AI (and previously, the printing press) both disrupted extant power arrangements, leading to decentralization, and triggered a recentralization of power by new actors better adapted to leveraging the new technology. We use the analogy of the printing press to provide a framework for understanding constitutive change. The printing press example gives us more clarity on 1) what can happen when the medium of communication drastically alters how information is communicated and stored; 2) the shift in power from state to private actors; and 3) the tension of simultaneously connecting individuals while driving them towards narrower communities through algorithmic analyses of data.",governance; corporations; politics of data; printing press,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,864–872,AIES '21,Association for Computing Machinery,FUGR92PF,0.0368421052631578,0.1428571428571428,0.125,0.0467755102040816
274,274,How Open Source Machine Learning Software Shapes AI,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534167,"Langenkamp, Max; Yue, Daniel N.",2022.0,https://doi.org/10.1145/3514094.3534167,conferencePaper,"If we want a future where AI serves a plurality of interests, then we should pay attention to the factors that drive its success. While others have studied the importance of data, hardware, and models in directing the trajectory of AI, we argue that open source software is a neglected factor shaping AI as a discipline. We start with the observation that almost all AI research and applications are built on machine learning open source software (MLOSS). This paper presents three contributions. First, it quantifies the outsized impact of MLOSS by using Github contributions data. By contrasting the costs of MLOSS and its economic benefits, we find that the average dollar of MLOSS investment corresponds to at least 100 of global economic value created, corresponding to 30B of economic value created this year. Second, we leverage interviews with AI researchers and developers to develop a causal model of the effect of open sourcing on economic value. We argue that open sourcing creates value through three primary mechanisms: standardization of MLOSS tools, increased experimentation in AI research, and creation of communities. Finally, we consider the incentives for developing MLOSS and the broader implications of these effects. We intend this paper to be useful for technologists and academics who want to analyze and critique AI, and policymakers who want to better understand and regulate AI systems.",machine learning; open source,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,385–395,AIES '22,Association for Computing Machinery,BTKLFEVW,0.0446428571428571,0.0,0.125,0.0465181557572862
275,275,From Posthumanism to Ethics of Artificial Intelligence,AI Soc.,38.0,1,10.1007/s00146-021-01274-1,"Nath, Rajakishore; Manna, Riya",2021.0,https://doi.org/10.1007/s00146-021-01274-1,journalArticle,"Posthumanism is one of the well-known and significant concepts in the present day. It impacted numerous contemporary fields like philosophy, literary theories, art, and culture for the last few decades. The movement has been concentrated around the technological development of present days due to industrial advancement in society and the current proliferated daily usage of technology. Posthumanism indicated a deconstruction of our radical conception of ‘human’, and it further shifts our societal value alignment system to a novel dimension. The majority of our population is getting deeply involved in virtual reality in daily life. Sooner or later, we shall get a different conception of ‘biological human being’ through the advancement of artificial intelligence (AI) technology. If an automated artificial system could replace the human brain and repair any physical loss of our biological body, it will certainly become a journey towards immortality for scientists. However, we must analyze whether posthumanism will consider ‘hybrid human beings’ as moral agents, similar to biological humans. This is why, in the future, the relation between biological human beings and posthumans will play an active role in designing artificial moral agents. Whether the future posthumans would overpower biological humanity or both of them would work as peers to form a digital utopian society and create new dimensions of rationality is still a case of anticipation. Our aim in this paper is to critically analyze the authenticity of the posthuman cyborg as an agent, their relations with humans and the emergence of ‘AI ethics’.",Ethics; Artificial intelligence (AI); Artificial moral agent; Moral agent; Posthumanism; Postmodernity,,0951-5666,2021-09,2023-11-06 01:30:04,2023-11-06 01:30:04,185–196,,,VY2UPXSP,0.0241935483870967,0.2727272727272727,0.2857142857142857,0.0463318320092875
276,276,K-12 Computing Education for the AI Era: From Data Literacy to Data Agency,Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1,,,10.1145/3587102.3593796,"Tedre, Matti; Vartiainen, Henriikka",2023.0,https://doi.org/10.1145/3587102.3593796,conferencePaper,"The question of how to teach classical, rule-based programming has been driving much of the computing education research since the 1950s. In the K–12 (school) context, a consensus has emerged over time on the paradigmatic elements of computing education, which implicitly assumes a von Neumann computer executing instruction sequences guided by imperative programs. Within this framework, many researchers have focused on how to facilitate learners to develop an accurate mental model of what the computer does when it executes a piece of code.However, the traditional programming approach in computing education is inadequate for understanding and developing machine learning (ML) driven technology. ML has already facilitated significant advancements in automation, ranging from speech and image recognition, autonomous cars, and deepfake videos to super-human performance in board and computer games, and more. Many data-driven approaches that power today's cutting edge services and apps significantly diverge from the central paradigmatic assumptions of traditional programming. Consequently, traditional views on computing education are increasingly being challenged to account for the changes that AI/ML brings.This keynote talk presents early results from a study on how to teach fundamental AI insights and techniques to 200 4–9 graders in 14 primary schools in Eastern Finland. It describes the learning environments, tools, and pedagogical approaches involved, and explores the paradigmatic and conceptual changes required in transitioning from teaching classical programming to teaching ML in K–12 computing education. It outlines the mindset shifts required for this transition and discusses the challenges posed to the development of curricula, educational technology, and learning environments. It further provides examples of how AI ethics concepts, such as algorithmic bias, privacy, misinformation, diversity, and accountability, can be integrated into ML education.The talk discusses the relationship between different literacies in computing and presents an active concept, data agency, that refers to people's volition and capacity for informed actions that make a difference in their digital world. It emphasizes not only the understanding of data (i.e., data literacy) but also the active control and manipulation of information flows and the ethical and wise use of them.",artificial intelligence; machine learning; ai education; computing education; k-12; school,9798400701382,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,1–2,ITiCSE 2023,Association for Computing Machinery,89IH8JUP,0.0383480825958702,0.1,0.2307692307692307,0.0460807271957922
277,277,"Does AI De-Bias Recruitment? Race, Gender, and AI's 'Eradication of Differences Between Groups'","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534151,"Drage, Eleanor; Mackereth, Kerry",2022.0,https://doi.org/10.1145/3514094.3534151,conferencePaper,"In this paper, we analyze two key claims offered by recruitment AI companies in relation to the development and deployment of AI-powered HR tools: 1) recruitment AI can objectively assess candidates by removing gender and race from their systems, and 2) this removal of gender and race will make recruitment fairer, help customers attain their DEI goals, and lay the foundations for a truly meritocratic culture to thrive within an organization. We argue that these claims are misleading for four reasons: First, attempts to 'strip' gender and race from AI systems often misunderstand what gender and race are, casting them as isolatable attributes rather than broader systems of power. Second, the attempted outsourcing of 'diversity work' to AI-powered hiring tools may unintentionally entrench cultures of inequality and discrimination by failing to address the systemic problems within organizations. Third, AI hiring tools' supposedly neutral assessment of candidates' traits belies the power relationship between the observer and the observed. Specifically, the racialized history of character analysis and its associated processes of classification and categorisation play into longer histories of taxonomical sorting and reflect the current demands and desires of the job market, even when not explicitly conducted along the lines of gender and race. Fourth, recruitment AI tools help produce the 'ideal candidate' that they supposedly identify through by constructing associations between words and people's bodies. From these four conclusions outlined above, we offer three key recommendations to AI HR firms, their customers, and policy makers going forward.",ai ethics; bias; gender; race; recruitment,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,237,AIES '22,Association for Computing Machinery,PCEG5HCT,0.032520325203252,0.5,0.0769230769230769,0.0457276384672868
278,278,Becoming Good at AI for Good,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462599,"Kshirsagar, Meghana; Robinson, Caleb; Yang, Siyu; Gholami, Shahrzad; Klyuzhin, Ivan; Mukherjee, Sumit; Nasir, Md; Ortiz, Anthony; Oviedo, Felipe; Tanner, Darren; Trivedi, Anusua; Xu, Yixi; Zhong, Ming; Dilkina, Bistra; Dodhia, Rahul; Lavista Ferres, Juan M.",2021.0,https://doi.org/10.1145/3461702.3462599,conferencePaper,"AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations.",sustainability; case study; collaboration; AI for good,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,664–673,AIES '21,Association for Computing Machinery,GIHD9HND,0.0344827586206896,0.1428571428571428,0.1666666666666666,0.0456172180310111
279,279,FaiR-N: Fair and Robust Neural Networks for Structured Data,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462559,"Sharma, Shubham; Gee, Alan H.; Paydarfar, David; Ghosh, Joydeep",2021.0,https://doi.org/10.1145/3461702.3462559,conferencePaper,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.",fairness; robustness; ethical artificial intelligence; neural networks,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,946–955,AIES '21,Association for Computing Machinery,UJUCKP4R,0.0240384615384615,0.4285714285714285,0.1111111111111111,0.0455388432975193
280,280,Artificial Moral Advisors: A New Perspective from Moral Psychology,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534139,"Liu, Yuxin; Moore, Adam; Webb, Jamie; Vallor, Shannon",2022.0,https://doi.org/10.1145/3514094.3534139,conferencePaper,"Philosophers have recently put forward the possibility of achieving moral enhancement through artificial intelligence (e.g., Giubilini and Savulescu's version [32]), proposing various forms of ""artificial moral advisor"" (AMA) to help people make moral decisions without the drawbacks of human cognitive limitations. In this paper, we provide a new perspective on the AMA, drawing on empirical evidence from moral psychology to point out several challenges to these proposals that have been largely neglected by AI ethicists. In particular, we suggest that the AMA at its current conception is fundamentally misaligned with human moral psychology - it incorrectly assumes a static moral values framework underpinning the AMA's attunement to individual users, and people's reactions and subsequent (in)actions in response to the AMA suggestions will likely diverge substantially from expectations. As such, we note the necessity for a coherent understanding of human moral psychology in the future development of AMAs.",ai ethics; ai moral enhancement; artificial moral advisor; moral psychology; normative ethics,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,436–445,AIES '22,Association for Computing Machinery,SEYR3X26,0.0068027210884353,0.5,0.0,0.0454566507454045
281,281,Learning from Discriminatory Training Data,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604710,"Grabowicz, Przemyslaw; Perello, Nicholas; Takatsu, Kenta",2023.0,https://doi.org/10.1145/3600211.3604710,conferencePaper,"Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups’ intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulations, and is computationally lightweight — it can be used with any supervised learning model to prevent direct and indirect discrimination via proxies while maximizing model accuracy for business necessity.",algorithmic fairness; concept shift; dataset shift; discrimination; evaluation; explainability; intersectionality; law; supervised learning,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,752–763,AIES '23,Association for Computing Machinery,9EY3YFJQ,0.036144578313253,0.0769230769230769,0.2,0.045192984237989
283,283,Action Guidance and AI Alignment,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604714,"Robinson, Pamela",2023.0,https://doi.org/10.1145/3600211.3604714,conferencePaper,"I offer a preliminary conceptual framework for evaluating AI alignment projects. It is based on the concept of action guidance. In §1 and §2, I explain action guidance and its importance to AI alignment. I introduce the ‘Guidance Framework’ in §3. In §4, I show how it can be applied to two different sorts of questions: the practical question of how to design a specific AI agent (my example is a fictional ocean-cleaning robot), and the theoretical question of how to evaluate a specific AI alignment proposal (my example is Stuart Russell's ‘binary approach’). In §5 I discuss limitations of the framework and opportunities for further research.",,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,387–395,AIES '23,Association for Computing Machinery,DKMPXTVG,0.0373831775700934,0.0,0.2,0.0450357339197361
284,284,Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534145,"Barnett, Julia; Diakopoulos, Nicholas",2022.0,https://doi.org/10.1145/3514094.3534145,conferencePaper,"With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.",ai ethics; anticipatory governance; broader impacts; thematic analysis,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,56–67,AIES '22,Association for Computing Machinery,8TKHKDAQ,0.0317460317460317,0.375,0.0,0.0449382141564552
285,285,Responsible Agency Through Answerability: Cultivating the Moral Ecology of Trustworthy Autonomous Systems,Proceedings of the First International Symposium on Trustworthy Autonomous Systems,,,10.1145/3597512.3597529,"Hatherall, Louise; Keküllüoğlu, Dilara; Kokciyan, Nadin; Rovatsos, Michael; Sethi, Nayha; Vierkant, Tillmann; Vallor, Shannon",2023.0,https://doi.org/10.1145/3597512.3597529,conferencePaper,"The decades-old debate over so-called ‘responsibility gaps’ in intelligent systems has recently been reinvigorated by rapid advances in machine learning techniques that are delivering many of the capabilities of machine autonomy that Matthias [1] originally anticipated. The emerging capabilities of intelligent learning systems highlight and exacerbate existing challenges with meaningful human control of, and accountability for, the actions and effects of such systems. The related challenge of human ‘answerability’ for system actions and harms has come into focus in recent literature on responsibility gaps [2, 3]. We describe a proposed interdisciplinary approach to designing for answerability in autonomous systems, grounded in an instrumentalist framework of ‘responsible agency cultivation’ drawn from moral philosophy and cognitive sciences as well as empirical results from structured interviews and focus groups in the application domains of health, finance and government. We outline a prototype dialogue agent informed by these emerging results and designed to help bridge the structural gaps in organisations that typically impede the human agents responsible for an autonomous sociotechnical system from answering to vulnerable patients of responsibility.",AI ethics; Agency; Answerability; Dialogue agents; Responsibility gaps; Sociotechnical Systems Design,9798400707346,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,,TAS '23,Association for Computing Machinery,XJWRVMWZ,0.0228571428571428,0.3636363636363636,0.0,0.0445060281165757
286,286,AI Ethics Statements: Analysis and Lessons Learnt from NeurIPS Broader Impact Statements,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533780,"Ashurst, Carolyn; Hine, Emmie; Sedille, Paul; Carlier, Alexis",2022.0,https://doi.org/10.1145/3531146.3533780,conferencePaper,"Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",AI governance; broader impacts; conference policies; ethics statements; NeurIPS policies; research ethics,978-1-4503-9352-2,,2022,2023-11-06 01:29:48,2023-11-06 01:29:48,2047–2056,FAccT '22,Association for Computing Machinery,AFYH6GJH,0.0141342756183745,0.4166666666666667,0.25,0.0440877379784761
287,287,Co-Design and Ethical Artificial Intelligence for Health: Myths and Misconceptions,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462537,"Donia, Joseph; Shaw, Jay",2021.0,https://doi.org/10.1145/3461702.3462537,conferencePaper,"Applications of artificial intelligence / machine learning (AI/ML) are dynamic and rapidly growing, and although multi-purpose, are particularly consequential in health care. One strategy for anticipating and addressing ethical challenges related to AI/ML for health care is co-design - or involvement of end users in design. Co-design has a diverse intellectual and practical history, however, and has been conceptualized in many different ways. Moreover, the unique features of AI/ML introduce challenges to co-design that are often underappreciated. This review summarizes the research literature on involvement in health care and design, and informed by critical data studies, examines the extent to which co-design as commonly conceptualized is capable of addressing the range of normative issues raised by AI/ML for health. We suggest that AI/ML technologies have amplified existing challenges related to co-design, and created entirely new challenges. We outline five co-design 'myths and misconceptions' related to AI/ML for health that form the basis for future research and practice. We conclude by suggesting that the normative strength of a co-design approach to AI/ML for health can be considered at three levels: technological, health care system, and societal. We also suggest research directions for a 'new era' of co-design capable of addressing these challenges.Link to full text: https://bit.ly/3yZrb3y",artificial intelligence; ethics; machine learning; co-design; participatory design; health care; health systems,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,77,AIES '21,Association for Computing Machinery,I8Z639RS,0.0195121951219512,0.1666666666666666,0.3,0.0438827938465258
288,288,AI Art and Misinformation: Approaches and Strategies for Media Literacy and Fact Checking,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604715,"Walker, Johanna; Thuermer, Gefion; Vicens, Julian; Simperl, Elena",2023.0,https://doi.org/10.1145/3600211.3604715,conferencePaper,"Misinformation in its many forms is a substantial and growing problem for society today. Whether financially or ideologically motivated, purveyors of misinformation do not abide by legal, technical or moral rules. Therefore new, ludic, narrative, gamified and artistic approaches are needed. In this paper we analyse the approaches taken in countering misinformation by 18 AI and machine learning works of art, developed in the MediaFutures project. We examine how these align with existing AI approaches to countering misinformation, and how they address some of the key challenges. We show that AI artists engage with existing debunking and inoculating strategies, including highly technical aspects such as deepfakes, while also utilizing focused strategies of data literacy and collective intelligence. We also find that they are able to integrate hard-to-refute strategies such as narrative and emotion. These findings suggest that data as an art material and AI techniques as art tools are worth of further investigation as to their effectiveness for countering misinformation within society.",AI art; disinformation; fact checking; fake news; media literacy; Misinformation,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,26–37,AIES '23,Association for Computing Machinery,P7K3NZPE,0.037037037037037,0.1,0.0769230769230769,0.0438143808309554
289,289,Welfarist Moral Grounding for Transparent AI,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3593977,"Narayanan, Devesh",2023.0,https://doi.org/10.1145/3593013.3593977,conferencePaper,"As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple – even simplistic – move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us.",AI Ethics; Transparency; Moral Theory; Welfarism,9798400701924,,2023,2023-11-06 01:30:00,2023-11-06 01:30:00,64–76,FAccT '23,Association for Computing Machinery,JUIRUT98,0.0254237288135593,0.5,0.1666666666666666,0.0434214633154127
290,290,Why is My System Biased?: Rating of AI Systems through a Causal Lens,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539556,"Lakkaraju, Kausik",2022.0,https://doi.org/10.1145/3514094.3539556,conferencePaper,"Artificial Intelligence (AI) systems like facial recognition systems and sentiment analyzers are known to exhibit model uncertainty which can be perceived as algorithmic bias in most cases. The aim of my Ph.D. is to examine and control the bias present in these AI systems by establishing causal relationships and also assigning a rating to these systems, which helps the user to make an informed selection when choosing from different systems for their application.",bias; fairness; rating AI systems,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,902,AIES '22,Association for Computing Machinery,XWKQAG5A,0.0273972602739726,0.2,0.0769230769230769,0.0433825264875493
291,291,Platform Power and AI: The Case of Content,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462443,"Lazar, Seth; Bucher, Taina; Korolova, Aleksandra; O'Connor, Cailin; Suzor, Nicolas",2021.0,https://doi.org/10.1145/3461702.3462443,conferencePaper,"This panel brings experts together from law, philosophy, computer science and media studies to explore how digital platforms exercise power over which content is visible online, and which content is promoted to users, with a special focus on the use of algorithmic systems to achieve these ends.",content moderation; content promotion; power; recommendation systems; speech,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,2,AIES '21,Association for Computing Machinery,UMQH8X2H,0.0425531914893617,0.0,0.125,0.0431070011848951
292,292,Varieties of Transparency: Exploring Agency within AI Systems,AI Soc.,38.0,4,10.1007/s00146-021-01326-6,"Andrada, Gloria; Clowes, Robert W.; Smart, Paul R.",2022.0,https://doi.org/10.1007/s00146-021-01326-6,journalArticle,"AI systems play an increasingly important role in shaping and regulating the lives of millions of human beings across the world. Calls for greater transparency from such systems have been widespread. However, there is considerable ambiguity concerning what “transparency” actually means, and therefore, what greater transparency might entail. While, according to some debates, transparency requires seeing through the artefact or device, widespread calls for transparency imply seeing into different aspects of AI systems. These two notions are in apparent tension with each other, and they are present in two lively but largely disconnected debates. In this paper, we aim to further analyse what these calls for transparency entail, and in so doing, clarify the sorts of transparency that we should want from AI systems. We do so by offering a taxonomy that classifies different notions of transparency. After a careful exploration of the different varieties of transparency, we show how this taxonomy can help us to navigate various domains of human–technology interactions, and more usefully discuss the relationship between technological transparency and human agency. We conclude by arguing that all of these different notions of transparency should be taken into account when designing more ethically adequate AI systems.",AI ethics; Transparency; Philosophy of technology; Agency; Philosophy of AI; Philosophy of mind,,0951-5666,2022-01,2023-11-06 01:30:01,2023-11-06 01:30:01,1321–1331,,,VKD9I2CX,0.0202020202020202,0.3076923076923077,0.125,0.0429290964523601
293,293,A Model for Governing Information Sharing in Smart Assistants,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534129,"Zhan, Xiao; Sarkadi, Stefan; Criado, Natalia; Such, Jose",2022.0,https://doi.org/10.1145/3514094.3534129,conferencePaper,"Smart Personal Assistants (SPAs), such as Amazon Alexa, Google Assistant and Apple Siri, leverage different AI techniques to provide convenient help and assistance to users. However, inappropriate information sharing decisions can lead SPAs to incorrectly disclose user information to undesired parties, or mistakenly block their reasonable access in specific scenarios to desired parties. In fact, reports about privacy violations in SPAs and associated user concerns are well known and understood in the related literature. It is difficult for SPAs to automatically decide how data should be shared with respect to the privacy preferences of the users. We argue norms, which are regarded as shared standards of acceptable behaviour of groups and/or individuals, can be used to govern and reason about the best course of action of SPAs with regards to information sharing, and our work is the first to propose a practical model to address the above issues and govern SPAs based on normative systems and the contextual integrity theory of privacy. We evaluated the performance of the model using a real dataset of user preferences for privacy in SPAs and the results showed a very marked and significant improvement in understanding user preferences and making the right decisions with respect to data sharing.",privacy; personal data; smart personal assistants; voice assistants,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,845–855,AIES '22,Association for Computing Machinery,ME7GVSW3,0.034313725490196,0.25,0.0,0.0429074967956584
294,294,Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604705,"Ovalle, Anaelia; Subramonian, Arjun; Gautam, Vagrant; Gee, Gilbert; Chang, Kai-Wei",2023.0,https://doi.org/10.1145/3600211.3604705,conferencePaper,"Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness’ raison d’être of “fairness,” we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and praxis, and 4) provide actionable recommendations for AI fairness researchers to engage with intersectionality in their work by grounding it in AI epistemology.",artificial intelligence; fairness; literature review; intersectionality,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,496–511,AIES '23,Association for Computing Machinery,K2IMHQRQ,0.037037037037037,0.0,0.1333333333333333,0.0428070175438596
295,295,Cybersecurity Education in the Age of AI: Integrating AI Learning into Cybersecurity High School Curricula,Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1,,,10.1145/3545945.3569750,"Grover, Shuchi; Broll, Brian; Babb, Derek",2023.0,https://doi.org/10.1145/3545945.3569750,conferencePaper,"Artificial Intelligence (AI) and cybersecurity are becoming increasingly intertwined, with AI and Machine Learning (AI/ML) being leveraged for cybersecurity, and cybersecurity helping address issues caused by AI. The goal in our exploratory curricular initiative is to dovetail the need to teach these two critical, emerging topics in highschool, and create a suite of novel activities, 'AI &amp; Cybersecurity for Teens' (ACT) that introduces AI/ML in the context of cybersecurity and prepares high school teachers to integrate them in their cybersecurity curricula. Additionally, ACT activities are designed such that teachers (and students) build a deeper understanding of how ML works and how the machine actually ""learns"". Such understanding will aid more meaningful interrogation of critical issues such as AI ethics and bias. ACT introduces core ML topics contextualized in cybersecurity topics through a range of programming activities and pre-programmed games in NetsBlox, an easy-to-use block-based programming environment. We conducted 2 pilot workshops with 12 high school cybersecurity teachers focused on ACT activities. Teachers' feedback was positive and encouraging but also highlighted potential challenges in implementing ACT in the classroom. This paper reports on our approach and activities design, and teachers' experiences and feedback on integrating AI into high school cybersecurity curricula.",artificial intelligence education; block-based programming; cybersecurity education; k-12 computer science education; teacher preparation,978-1-4503-9431-4,,2023,2023-11-06 01:30:03,2023-11-06 01:30:03,980–986,SIGCSE 2023,Association for Computing Machinery,XNUQXSIU,0.04,0.0,0.1333333333333333,0.0426554352186121
296,296,Effective Enforceability of EU Competition Law Under AI Development Scenarios: A Framework for Anticipatory Governance,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604694,"Hua, Shin-Shin; Belfield, Haydn",2023.0,https://doi.org/10.1145/3600211.3604694,conferencePaper,"This paper examines whether competition law enforcement can remain effective under different AI development scenarios over the coming years. Economic and political power has become increasingly concentrated into a few AI companies, such as Big Tech. The growth of generative AI could further reinforce this concentration of power in Big Tech. The market power of these companies, and increasingly their involvement in AI, is a major focus for regulators such as the European Commission. Recent EU antitrust fines on Google alone run in the billions. The dynamism of technology markets such as AI can make it difficult for regulators to take effective action. If AI continues to develop rapidly over the coming years, propelled by the proliferation of generative AI, this ability to effectively enforce antitrust law may be further challenged. To help ensure regulators remain effective, EU competition law has been bolstered by a new tech-tailored, ex ante competition regime. These are likely to be critical tools to shape the market power of Big Tech but are largely untested. Exploring how these regulatory tools can be most effective in governing future AI development is a timely question for regulators, lawyers, companies, and citizens. This paper examines this question by considering the ‘effective enforceability’ of EU competition law and the Digital Markets Act under different AI development scenarios. By ‘effective enforceability’ of EU competition law we mean how well it achieves its policy objectives. We consider four factors: jurisdictional scope, potential loopholes, effectiveness of detection, and ability to remedy/sanction breaches. However, there is significant uncertainty as to how AI will develop in the coming years. Considering this, we propose an analytical framework based on five variables: key inputs, speed of development, AI capability, number of actors, and the nature/relationship of actors. In some of these scenarios, we argue EU competition law would struggle to address the power of the largest AI companies; but in many other scenarios it remains a powerful tool. This is a critical juncture for competition regulators. They stand at the dawn of emerging challenges presented by generative AI. With this paper, we hope to contribute to anticipatory governance at this important intersection of legal governance and technology.Effective and future-proof competition law enforcement is crucial to ensuring this potentially transformative technology has widely distributed benefits, rather than concentrating power in a few hands.",anticipatory governance; AI development scenarios; Antitrust; competition law,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,596–605,AIES '23,Association for Computing Machinery,9YPIHTPG,0.038860103626943,0.125,0.0666666666666666,0.0424443214358413
297,297,Organizational Governance of Emerging Technologies: AI Adoption in Healthcare,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594089,"Kim, Jee Young; Boag, William; Gulamali, Freya; Hasan, Alifia; Hogg, Henry David Jeffry; Lifson, Mark; Mulligan, Deirdre; Patel, Manesh; Raji, Inioluwa Deborah; Sehgal, Ajai; Shaw, Keo; Tobey, Danny; Valladares, Alexandra; Vidal, David; Balu, Suresh; Sendak, Mark",2023.0,https://doi.org/10.1145/3593013.3594089,conferencePaper,"Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.",,9798400701924,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,1396–1417,FAccT '23,Association for Computing Machinery,BAFXK9PE,0.039568345323741,0.0,0.1111111111111111,0.0423104025490409
298,298,Current and Near-Term AI as a Potential Existential Risk Factor,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534146,"Bucknall, Benjamin S.; Dori-Hacohen, Shiri",2022.0,https://doi.org/10.1145/3514094.3534146,conferencePaper,"There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole. A separate, parallel body of work has explored existential risks to humanity, including but not limited to that stemming from unaligned Artificial General Intelligence (AGI). In this paper, we problematise the notion that current and near-term artificial intelligence technologies have the potential to contribute to existential risk by acting as intermediate risk factors, and that this potential is not limited to the unaligned AGI scenario. We propose the hypothesis that certain already-documented effects of AI can act as existential risk factors, magnifying the likelihood of previously identified sources of existential risk. Moreover, future developments in the coming decade hold the potential to significantly exacerbate these risk factors, even in the absence of artificial general intelligence. Our main contribution is a (non-exhaustive) exposition of potential AI risk factors and the causal relationships between them, focusing on how AI can affect power dynamics and information security. This exposition demonstrates that there exist causal pathways from AI systems to existential risks that do not presuppose hypothetical future AI capabilities.",ai safety; existential risk; societal impacts of ai,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,119–129,AIES '22,Association for Computing Machinery,KPZJTEH7,0.0310880829015544,0.25,0.1,0.0416723549488054
299,299,"Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: A UK Case Study","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534200,"Zilka, Miri; Sargeant, Holli; Weller, Adrian",2022.0,https://doi.org/10.1145/3514094.3534200,conferencePaper,"We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.",,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,880–889,AIES '22,Association for Computing Machinery,CHBAY267,0.0394736842105263,0.0,0.0588235294117647,0.0415114266844641
300,300,Skilled and Mobile: Survey Evidence of AI Researchers' Immigration Preferences,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462617,"Zwetsloot, Remco; Zhang, Baobao; Dreksler, Noemi; Kahn, Lauren; Anderljung, Markus; Dafoe, Allan; Horowitz, Michael C.",2021.0,https://doi.org/10.1145/3461702.3462617,conferencePaper,"Countries, companies, and universities are increasingly competing over top-tier artificial intelligence (AI) researchers. Where are these researchers likely to immigrate and what affects their immigration decisions? We conducted a survey (n = 524) of the immigration preferences and motivations of researchers that had papers accepted at one of two prestigious AI conferences: the Conference on Neural Information Processing Systems (NeurIPS) and the International Conference on Machine Learning (ICML). We find that the U.S. is the most popular destination for AI researchers, followed by the U.K., Canada, Switzerland, and France. A country's professional opportunities stood out as the most common factor that influences immigration decisions of AI researchers, followed by lifestyle and culture, the political climate, and personal relations. The destination country's immigration policies were important to just under half of the researchers surveyed, while around a quarter noted current immigration difficulties to be a deciding factor. Visa and immigration difficulties were perceived to be a particular impediment to conducting AI research in the U.S., the U.K., and Canada. Implications of the findings for the future of AI talent policies and governance are discussed.",ai researchers; immigration policy; survey research,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,1050–1059,AIES '21,Association for Computing Machinery,UXBGBDKR,0.0327868852459016,0.1666666666666666,0.1,0.0413596777198211
301,301,From Bias to Repair: Error as a Site of Collaboration and Negotiation in Applied Data Science Work,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579607,"Lin, Cindy Kaiying; Jackson, Steven J.",2023.0,https://doi.org/10.1145/3579607,journalArticle,"Managing error has become an increasingly central and contested arena within data science work. While recent scholarship in artificial intelligence and machine learning has focused on limiting and eliminating error, practitioners have long used error as a site of collaboration and learning vis-à-vis labelers, domain experts, and the worlds data scientists seek to model and understand. Drawing from work in CSCW, STS, HCML, and repair studies, as well as from multi-sited ethnographic fieldwork within a government institution and a non-profit organization, we move beyond the notion of error as an edge case or anomaly to make three basic arguments. First, error discloses or calls to attention existing structures of collaboration unseen or underappreciated under 'working' systems. Second, error calls into being new forms and sites of collaboration (including, sometimes, new actors). Third, error redeploys old sites and actors in new ways, often through restructuring relations of hierarchy and expertise which recenter or devalue the position of different actors. We conclude by discussing how an artful living with error can better support the creative strategies of negotiation and adjustment which data scientists and their collaborators engage in when faced with disruption, breakdown, and friction in their work.",AI ethics; machine learning; critical data studies; data science; error; repair,,,2023-04,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,JGDGDMR5,0.0153061224489795,0.4545454545454545,0.0588235294117647,0.0412306820750437
302,302,Decision Time: Normative Dimensions of Algorithmic Speed,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533198,"Susser, Daniel",2022.0,https://doi.org/10.1145/3531146.3533198,conferencePaper,"Existing discussions about automated decision-making focus primarily on its inputs and outputs, raising questions about data collection and privacy on one hand and accuracy and fairness on the other. Less attention has been devoted to critically examining the temporality of decision-making processes—the speed at which automated decisions are reached. In this paper, I identify four dimensions of algorithmic speed that merit closer analysis. Duration (how much time it takes to reach a judgment), timing (when automated systems intervene in the activity being evaluated), frequency (how often evaluations are performed), and lived time (the human experience of algorithmic speed) are interrelated, but distinct, features of automated decision-making. Choices about the temporal structure of automated decision-making systems have normative implications, which I describe in terms of ”disruption,” ”displacement,” ”re-calibration,” and ”temporal fairness,” with values such as accuracy, fairness, accountability, and legitimacy hanging in the balance. As computational tools are increasingly tasked with making judgments about human activities and practices, the designers of decision-making systems will have to reckon, I argue, with when—and how fast—judgments ought to be rendered. Though computers are capable of reaching decisions at incredible speeds, failing to account for the temporality of automated decision-making risks misapprehending the costs and benefits automation promises.",AI ethics; automated decision-making; automation; data ethics; speed; temporality; time,978-1-4503-9352-2,,2022,2023-11-06 01:29:59,2023-11-06 01:29:59,1410–1420,FAccT '22,Association for Computing Machinery,7NWQJYAP,0.0098522167487684,0.6,0.0,0.0409171299016284
303,303,AI and Legal Personhood: An African Perspective,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539548,"Naidoo, Meshandren",2022.0,https://doi.org/10.1145/3514094.3539548,conferencePaper,"In recent years, artificial intelligence (AI) has posed challenges to the law - be it self-driving cars [1], auto-diagnostics [2], or DABUS, the AI inventor [3]. The law has been slow to catch up, but some solutions have been presented in global governance initiatives. However, these solutions are not without their challenges. Thus, a potential solution examined in this research is that of legal personhood for AI systems. The central question being whether there are compelling moral and legal reasons to grant some AI systems a degree of legal personhood.Within the global regulatory landscape, this position is heavily critiqued and labelled as ethically undesirable. However, this research contends that: (1) there may be moral reasons to grant specific AI with a sui generis form of legal personhood; and (2) there are strong legal rationales to endow some AI with a form of legal personhood.In terms of argument (1), this research finds that: (a) the debate in this area is largely unrepresentative of African philosophical thought. African philosophical thought tends to prize relationality (Metz, Eze, and Menketi) at its core; and (b) within this communitarian framework, this research finds that African philosophical thought may allow for some AI systems to be both subjects and objects of relationships.In terms of argument (2), this research contends that there are other important legal values for endowing some AI systems with legal personhood - such as legal efficiency, market efficiency, legal certainty, and accountability. The examples referred to which support this contention are: (i) AI inventiveness and creativity in intellectual property (IP) law; and (ii) liability in the instance where AI is an actor in healthcare. Furthermore, the strict liability route, as suggested by the European Union (EU), is problematic [5], especially for lower- middle-income countries (LMICs), such as South Africa. LMICs have smaller economies and often struggle with healthcare services, thus it is imperative that there exists an enabling environment for innovation. For this, LMICs regulatory regimes need to be more 'friendly' to developers and deployers of AI. As such, there is an imperative for out-of-the box legal solutions to these novel problems.",artificial intelligence; ethics; law; governance; legal personhood,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,906,AIES '22,Association for Computing Machinery,JK6LR65K,0.0315186246418338,0.2857142857142857,0.1428571428571428,0.0407586850897353
304,304,Harnessing Design Thinking for Human-Centered Modelling: A Tutorial on How Designers Can Pair with Data Scientists and Engineers for Modelling,Companion Proceedings of the 28th International Conference on Intelligent User Interfaces,,,10.1145/3581754.3584168,"Pak, Lauren Saebin",2023.0,https://doi.org/10.1145/3581754.3584168,conferencePaper,"Data scientists often pair with analytical translators to inform feature selection for models. Although translators bring their domain expertise, this industry knowledge is primarily focused on business context and operational metrics. There is a lack of deep understanding of human behavior. This result is models built on reductive assumptions that do not fully capture human experience. Although features can be tweaked to improve accuracy or prevent overfitting, these tactics only further perpetuate criticisms of AI's bias and lack of inclusivity. Furthermore, human-in-the-loop approaches involving hyperparameter tuning are not accessible for business stakeholders. The current experience for business users to translate model recommendations into action is poor, let alone continue to track and tune performance. The purpose of this tutorial is to leverage design thinking methods including ethnography and explore interdisciplinary methods for feature co-creation and model sustainability. This workshop will look at lessons from industry and explore the ways in which design thinking and data science best practice can come together to develop more human-centered models.",AI ethics; Design research; Design thinking for AI; Human-centered AI,9798400701078,,2023,2023-11-06 01:30:01,2023-11-06 01:30:01,196–198,IUI '23 Companion,Association for Computing Machinery,PGZM9VVW,0.0120481927710843,0.5,0.05,0.0400868369385654
305,305,RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of Opportunity,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462618,"Liu, David; Shafi, Zohair; Fleisher, William; Eliassi-Rad, Tina; Alfeld, Scott",2021.0,https://doi.org/10.1145/3461702.3462618,conferencePaper,"We present RAWLSNET, a system for altering Bayesian Network (BN) models to satisfy the Rawlsian principle of fair equality of opportunity (FEO). RAWLSNET's BN models generate aspirational data distributions: data generated to reflect an ideally fair, FEO-satisfying society. FEO states that everyone with the same talent and willingness to use it should have the same chance of achieving advantageous social positions (e.g., employment), regardless of their background circumstances (e.g., socioeconomic status). Satisfying FEO requires alterations to social structures such as school assignments. Our paper describes RAWLSNET, a method which takes as input a BN representation of an FEO application and alters the BN's parameters so as to satisfy FEO when possible, and minimize deviation from FEO otherwise. We also offer guidance for applying RAWLSNET, including on recognizing proper applications of FEO. We demonstrate the use of RAWLSNET with publicly available data sets. RAWLSNET's altered BNs offer the novel capability of generating aspirational data for FEO-relevant tasks. Aspirational data are free from biases of real-world data, and thus are useful for recognizing and detecting sources of unfairness in machine learning algorithms besides biased data.",aspirational data; Bayesian networks; rawlsian fair equality of opportunity,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,745–755,AIES '21,Association for Computing Machinery,G92BH9JA,0.0382513661202185,0.1111111111111111,0.0,0.0398301642409889
306,306,"Blacklists and Redlists in the Chinese Social Credit System: Diversity, Flexibility, and Comprehensiveness","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462535,"Engelmann, Severin; Chen, Mo; Dang, Lorenz; Grossklags, Jens",2021.0,https://doi.org/10.1145/3461702.3462535,conferencePaper,"The Chinese Social Credit System (SCS) is a novel digital socio-technical credit system. The SCS aims to regulate societal behavior by reputational and material devices. Scholarship on the SCS has offered a variety of legal and theoretical perspectives. However, little is known about its actual implementation. Here, we provide the first comprehensive empirical study of digital blacklists (listing ""bad"" behavior) and redlists (listing ""good"" behavior) in the Chinese SCS. Based on a unique data set of reputational blacklists and redlists in 30 Chinese provincial-level administrative divisions (ADs), we show the diversity, flexibility, and comprehensiveness of the SCS listing infrastructure. First, our results demonstrate that the Chinese SCS unfolds in a highly diversified manner: we find differences in accessibility, interface design and credit information across provincial-level SCS blacklists and redlists. Second, SCS listings are flexible. During the COVID-19 outbreak, we observe a swift addition of blacklists and redlists that helps strengthen the compliance with coronavirus-related norms and regulations. Third, the SCS listing infrastructure is comprehensive. Overall, we identify 273 blacklists and 154 redlists across provincial-level ADs. Our blacklist and redlist taxonomy highlights that the SCS listing infrastructure prioritizes law enforcement and industry regulations. We also identify redlists that reward political and moral behavior. Our study substantiates the enormous scale and diversity of the Chinese SCS and puts the debate on its reach and societal impact on firmer ground. Finally, we initiate a discussion on the ethical dimensions of data-driven research on the SCS.",china; china's social credit systems; digital socio-technical systems; reputation systems,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,78–88,AIES '21,Association for Computing Machinery,ISZ6HXXY,0.0330578512396694,0.2,0.0,0.0390178087243226
307,307,Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604667,"Narayanan Venkit, Pranav; Gautam, Sanjana; Panchanadikar, Ruchi; Huang, Ting-Hao; Wilson, Shomir",2023.0,https://doi.org/10.1145/3600211.3604667,conferencePaper,"We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers insights into the experience readers have when encountering such articles, highlighting the potential to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems.",Ethics in AI; HCI; Nationality Bias; Natural Language Processing,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,554–565,AIES '23,Association for Computing Machinery,MEC4BFWU,0.0267379679144385,0.3333333333333333,0.0,0.0384655991121717
308,308,"Explainability of Artificial Intelligence Methods, Applications and Challenges: A Comprehensive Survey",Inf. Sci.,615.0,C,10.1016/j.ins.2022.10.013,"Ding, Weiping; Abdel-Basset, Mohamed; Hawash, Hossam; Ali, Ahmed M.",2022.0,https://doi.org/10.1016/j.ins.2022.10.013,journalArticle,"The continuous advancement of Artificial Intelligence (AI) has been revolutionizing the strategy of decision-making in different life domains. Regardless of this achievement, AI algorithms have been built as Black-Boxes, that is as they hide their internal rationality and learning methodology from the human leaving many unanswered questions about how and why the AI decisions are made. The absence of explanation results in a sensible and ethical challenge. Explainable Artificial Intelligence (XAI) is an evolving subfield of AI that emphasizes developing a plethora of tools and techniques for unboxing the Black-Box AI solutions by generating human-comprehensible, insightful, and transparent explanations of AI decisions. This study begins by discussing the primary principles of XAI research, Black-Box problems, the targeted audience, and the related notion of explainability over the historical timeline of the XAI studies and accordingly establishes an innovative definition of explainability that addresses the earlier theoretical proposals. According to an extensive analysis of the literature, this study contributes to the body of knowledge by driving a fine-grained, multi-level, and multi-dimension taxonomy for insightful categorization of XAI studies with the main aim to shed light on the variations and commonalities of existing algorithms paving the way for extra methodological developments. Then, an experimental comparative analysis is presented for the explanation generated by common XAI algorithms applied to different categories of data to highlight their properties, advantages, and flaws. Followingly, this study discusses and categorizes the evaluation metrics for the XAI-generated explanation and the findings show that there is no common consensus on how an explanation must be expressed, and how its quality and dependability should be evaluated. The findings show that XAI can contribute to realizing responsible and trustworthy AI, however, the advantages of interpretability should be technically demonstrated, and complementary procedures and regulations are required to give actionable information that can empower decision-making in real-world applications. Finally, the tutorial is crowned by discussing the open research questions, challenges, and future directions that serve as a roadmap for the AI community to advance the research in XAI and to inspire specialists and practitioners to take the advantage of XAI in different disciplines.",Responsible AI; Machine learning; Black-box; Deep learning; Explainable AI; White-box,,0020-0255,2022-11,2023-11-06 01:30:04,2023-11-06 01:30:04,238–292,,,RUI2L3FR,0.0342857142857142,0.2,0.0,0.0382499866288709
309,309,Diffusing the Creator: Attributing Credit for Generative AI Outputs,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604716,"Khosrowi, Donal; Finn, Finola; Clark, Elinor",2023.0,https://doi.org/10.1145/3600211.3604716,conferencePaper,"The recent wave of generative AI (GAI) systems like Stable Diffusion that can produce images from human prompts raises controversial issues about creatorship, originality, creativity and copyright. This paper focuses on creatorship: who creates and should be credited with the outputs made with the help of GAI? Existing views on creatorship are mixed: some insist that GAI systems are mere tools, and human prompters are creators proper; others are more open to acknowledging more significant roles for GAI, but most conceive of creatorship in an all-or-nothing fashion. We develop a novel view, called CCC (collective-centered creation), that improves on these existing positions. On CCC, GAI outputs are created by collectives in the first instance. Claims to creatorship come in degrees and depend on the nature and significance of individual contributions made by the various agents and entities involved, including users, GAI systems, developers, producers of training data and others. Importantly, CCC maintains that GAI systems can sometimes be part of a co-creating collective. We detail how CCC can advance existing debates and resolve controversies around creatorship involving GAI.",ethics; Generative AI; collective-centered; copyright; creatorship; credit attribution; image synthesis,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,890–900,AIES '23,Association for Computing Machinery,IUPJ5FE2,0.0112359550561797,0.3,0.1111111111111111,0.0379669397112743
310,310,Actual Causality and Responsibility Attribution in Decentralized Partially Observable Markov Decision Processes,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534133,"Triantafyllou, Stelios; Singla, Adish; Radanovic, Goran",2022.0,https://doi.org/10.1145/3514094.3534133,conferencePaper,"Actual causality and a closely related concept of responsibility attribution are central to accountable decision making. Actual causality focuses on specific outcomes and aims to identify decisions (actions) that were critical in realizing an outcome of interest. Responsibility attribution is complementary and aims to identify the extent to which decision makers (agents) are responsible for this outcome. In this paper, we study these concepts under a widely used framework for multi-agent sequential decision making under uncertainty: decentralized partially observable Markov decision processes (Dec-POMDPs). Following recent works in RL that show correspondence between POMDPs and Structural Causal Models (SCMs), we first establish a connection between Dec-POMDPs and SCMs. This connection enables us to utilize a language for describing actual causality from prior work and study existing definitions of actual causality in Dec-POMDPs. Given that some of the well-known definitions may lead to counter-intuitive actual causes, we introduce a novel definition that more explicitly accounts for causal dependencies between agents' actions. We then turn to responsibility attribution based on actual causality, where we argue that in ascribing responsibility to an agent it is important to consider both the number of actual causes in which the agent participates, as well as its ability to manipulate its own degree of responsibility. Motivated by these arguments we introduce a family of responsibility attribution methods that extends prior work, while accounting for the aforementioned considerations. Finally, through a simulation-based experiment, we compare different definitions of actual causality and responsibility attribution methods. The empirical results demonstrate the qualitative difference between the considered definitions of actual causality and their impact on attributed responsibility.",actual causality; multi-agent systems; responsibility attribution,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,739–752,AIES '22,Association for Computing Machinery,TUFCFVX2,0.030188679245283,0.1666666666666666,0.0833333333333333,0.0372557250759832
311,311,From Coded Bias to Existential Threat: Expert Frames and the Epistemic Politics of AI Governance,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534161,"Schopmans, Hendrik R.",2022.0,https://doi.org/10.1145/3514094.3534161,conferencePaper,"While the knowledge produced by experts has been widely recognized to play a salient role in shaping policy on technological issues, the interaction between AI expertise and the evolving AI governance landscape has received little attention thus far. To address this gap, the present paper leverages insights from STS and International Relations to explore how different expert communities have constructed AI as a governance problem. More specifically, it presents the preliminary results of a qualitative frame analysis of 90 policy documents published by experts from industry, civil society, and the research community. The analysis finds that AI expertise is a highly contested field, as experts not only disagree on why AI is problematic and what policies are required, but, more fundamentally, about which artifacts, ideas, and practices make up AI in the first place. The paper proposes that the epistemic disagreements concerning AI have political consequences, as they engender protracted ontological politics that jeopardize the development of effective governance interventions. Against this background, the findings raise critical questions about the prevailing tendency of governance interventions to target the elusive and contested object 'artificial intelligence.'",artificial intelligence; epistemic community; experts; frame analysis.; global governance; governance objects,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,627–640,AIES '22,Association for Computing Machinery,S2BVBKSA,0.0380434782608695,0.0,0.0666666666666666,0.0371020832114225
312,312,Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions,Proc. ACM Hum.-Comput. Interact.,5.0,CSCW1,10.1145/3449180,"Lyons, Henrietta; Velloso, Eduardo; Miller, Tim",2021.0,https://doi.org/10.1145/3449180,journalArticle,"As the use of algorithmic systems in high-stakes decision-making increases, the ability to contest algorithmic decisions is being recognised as an important safeguard for individuals. Yet, there is little guidance on what 'contestability'–the ability to contest decisions–in relation to algorithmic decision-making requires. Recent research presents different conceptualisations of contestability in algorithmic decision-making. We contribute to this growing body of work by describing and analysing the perspectives of people and organisations who made submissions in response to Australia's proposed 'AI Ethics Framework', the first framework of its kind to include 'contestability' as a core ethical principle. Our findings reveal that while the nature of contestability is disputed, it is seen as a way to protect individuals, and it resembles contestability in relation to human decision-making. We reflect on and discuss the implications of these findings.",artificial intelligence; accountability; algorithmic decision-making; algorithmic fairness; and transparency; contestability,,,2021-04,2023-11-06 01:30:02,2023-11-06 01:30:02,,,,T5IE5XAA,0.044776119402985,0.0,0.0,0.0369422185811935
313,313,Socially-Aware Artificial Intelligence for Fair Mobility,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539545,"Michailidis, Dimitris",2022.0,https://doi.org/10.1145/3514094.3539545,conferencePaper,"Mobility systems have a fundamental impact on well-being. It is thus crucial to address the disproportional benefits that their design can lead to. In my research, I explore the trade-off between utility and fairness in transport network design and argue that AI can be used to create networks that achieve different notions of fairness.",fairness; reinforcement learning; mobility; socially-aware ai,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,904,AIES '22,Association for Computing Machinery,FQY3JQYS,0.0185185185185185,0.1666666666666666,0.0,0.0361396638929678
315,315,A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle,"Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization",,,10.1145/3465416.3483305,"Suresh, Harini; Guttag, John",2021.0,https://doi.org/10.1145/3465416.3483305,conferencePaper,"As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.",AI ethics; algorithmic bias; allocative harm; fairness in machine learning; representational harm; societal implications of machine learning,978-1-4503-8553-4,,2021,2023-11-06 01:30:02,2023-11-06 01:30:02,,EAAMO '21,Association for Computing Machinery,FXCRF6KG,0.0106382978723404,0.1764705882352941,0.0,0.0359281394592275
317,317,Unpacking the Expressed Consequences of AI Research in Broader Impact Statements,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462608,"Nanayakkara, Priyanka; Hullman, Jessica; Diakopoulos, Nicholas",2021.0,https://doi.org/10.1145/3461702.3462608,conferencePaper,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals.",ai ethics; anticipatory governance; broader impacts; thematic analysis,978-1-4503-8473-5,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,795–806,AIES '21,Association for Computing Machinery,B99AMPIB,0.0071942446043165,0.375,0.0909090909090909,0.0355788382894257
318,318,DIME: Fine-Grained Interpretations of Multimodal Models via Disentangled Local Explanations,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534148,"Lyu, Yiwei; Liang, Paul Pu; Deng, Zihao; Salakhutdinov, Ruslan; Morency, Louis-Philippe",2022.0,https://doi.org/10.1145/3514094.3534148,conferencePaper,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.",explainability; interpretability; visualization; multimodal machine learning,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,455–467,AIES '22,Association for Computing Machinery,6V54SZW5,0.0392156862745098,0.0,0.0,0.0353746190558334
319,319,Envisioning Communities: A Participatory Approach Towards AI for Social Good,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462612,"Bondi, Elizabeth; Xu, Lily; Acosta-Navas, Diana; Killian, Jackson A.",2021.0,https://doi.org/10.1145/3461702.3462612,conferencePaper,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good.",participatory design; artificial intelligence for social good; capabilities approach,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,425–436,AIES '21,Association for Computing Machinery,78BGHUGF,0.0334928229665071,0.0,0.1,0.0349819580664133
320,320,Explainable AI and Adoption of Financial Algorithmic Advisors: An Experimental Study,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462565,"Ben David, Daniel; Resheff, Yehezkel S.; Tron, Talia",2021.0,https://doi.org/10.1145/3461702.3462565,conferencePaper,"We study whether receiving advice from either a human or algorithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with ""No-explanation"" alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.",explainable ai; experiment; trust; algorithm adoption; financial advice; hci,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,390–400,AIES '21,Association for Computing Machinery,7Q74DATL,0.0181818181818181,0.2222222222222222,0.0909090909090909,0.0349645523558567
321,321,User-Oriented Fairness in Recommendation,Proceedings of the Web Conference 2021,,,10.1145/3442381.3449866,"Li, Yunqi; Chen, Hanxiong; Fu, Zuohui; Ge, Yingqiang; Zhang, Yongfeng",2021.0,https://doi.org/10.1145/3442381.3449866,conferencePaper,"As a highly data-driven application, recommender systems could be affected by data bias, resulting in unfair results for different data groups, which could be a reason that affects the system performance. Therefore, it is important to identify and solve the unfairness issues in recommendation scenarios. In this paper, we address the unfairness problem in recommender systems from the user perspective. We group users into advantaged and disadvantaged groups according to their level of activity, and conduct experiments to show that current recommender systems will behave unfairly between two groups of users. Specifically, the advantaged users (active) who only account for a small proportion in data enjoy much higher recommendation quality than those disadvantaged users (inactive). Such bias can also affect the overall performance since the disadvantaged users are the majority. To solve this problem, we provide a re-ranking approach to mitigate this unfairness problem by adding constraints over evaluation metrics. The experiments we conducted on several real-world datasets with various recommendation algorithms show that our approach can not only improve group fairness of users in recommender systems, but also achieve better overall recommendation performance.",AI Ethics; Fairness; Re-ranking; Recommendation System,978-1-4503-8312-7,,2021,2023-11-06 01:30:00,2023-11-06 01:30:00,624–632,WWW '21,Association for Computing Machinery,5CZBLJP5,0.0163043478260869,0.5,0.0,0.034893684544589
322,322,CSR 2021: The 1st International Workshop on Causality in Search and Recommendation,Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,,,10.1145/3404835.3462817,"Zhang, Yongfeng; Chen, Xu; Zhang, Yi; Chen, Xianjie",2021.0,https://doi.org/10.1145/3404835.3462817,conferencePaper,"Most of the current machine learning approaches to IR—including search and recommendation tasks—are mostly designed based on the basic idea of matching, which work from the perceptual and similarity learning perspective. This include both the learning of features from data such as representation learning, and the learning of similarity matching functions from data such as neural function learning. Though many models have been widely used in practical ranking systems such as search and recommendation, their design philosophy limits the models to the correlative signals in data. However, advancing from correlative learning to causal learning in search and recommendation is an important problem, because causal modeling can help us to think outside of the observational data for representation learning and ranking. More specially, causal learning can bring benefits to the IR community on various dimensions, including but not limited to Explainable IR models, Unbiased IR models, Fairness-aware IR models, Robust IR models and Cognitive Reasoning IR models. This workshop focuses on the research and application of causal modeling in search, recommendation and a broader scope of IR tasks. The workshop will gather both researchers and practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent regulations on AI Ethics, to a broader community including but not limited to IR, machine learning, AI, Data Science, and beyond. Workshop homepage is available online at https://csr21.github.io/.",causal learning; causality; counterfactual learning; information retrieval; recommendation; search,978-1-4503-8037-9,,2021,2023-11-06 01:30:04,2023-11-06 01:30:04,2677–2680,SIGIR '21,Association for Computing Machinery,5QNCSNY9,0.0387931034482758,0.0,0.0,0.0348435793415509
323,323,A Profile on Twitter Shadowban: An AI Ethics Position Paper on Free-Speech,"Intelligent Data Engineering and Automated Learning – IDEAL 2021: 22nd International Conference, IDEAL 2021, Manchester, UK, November 25–27, 2021, Proceedings",,,10.1007/978-3-030-91608-4_39,"Marcondes, Francisco S.; Gala, Adelino; Durães, Dalila; Moreira, Fernando; Almeida, José João; Baldi, Vania; Novais, Paulo",2021.0,https://doi.org/10.1007/978-3-030-91608-4_39,conferencePaper,"Concerns have been expressed lately about content verification algorithms on social media platforms, resulting in data being presented to users or omitted from them. Twitter is one of those platforms that use this strategy, giving censorship strategies for specific content. The shadowban or practice of limiting content distribution without user acknowledgement is a current practice in online social networks, especially on Twitter. So, this paper is an AI Ethics position paper willing to expand the reflection about the impacts of programming artefacts on individual liberties in enterprise’s Online social networks (OSN) to the computer science public. The conclusion to be drawn is that the limits of speech are to be imposed by the state after a properly democratic process took place [4]. The concern then turns into an international relations issue as it would be a threat for national sovereignty that one state, even inadvertently, regulates the actions of a communication enterprise on foreign states. Finally, the use of smart contracts is suggested as an alternative to be used by the base state for enforcing regulations on foreign OSNs.",Free-speech; Smart-contract; Twitter shadowban,978-3-030-91607-7,,2021,2023-11-06 01:29:49,2023-11-06 01:29:49,397–405,,Springer-Verlag,KCR6H3ZY,0.0223463687150838,0.0,0.25,0.0347822458803274
324,324,Fairness in the Eyes of the Data: Certifying Machine-Learning Models,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462554,"Segal, Shahar; Adi, Yossi; Pinkas, Benny; Baum, Carsten; Ganesh, Chaya; Keshet, Joseph",2021.0,https://doi.org/10.1145/3461702.3462554,conferencePaper,"We present a framework that allows to certify the fairness degree of a model based on an interactive and privacy-preserving test. The framework verifies any trained model, regardless of its training process and architecture. Thus, it allows us to evaluate any deep learning model on multiple fairness definitions empirically. We tackle two scenarios, where either the test data is privately available only to the tester or is publicly known in advance, even to the model creator. We investigate the soundness of the proposed approach using theoretical analysis and present statistical guarantees for the interactive test. Finally, we provide a cryptographic technique to automate fairness testing and certified inference with only black-box access to the model at hand while hiding the participants' sensitive data.",fairness; privacy; cryptography; machine-learning,978-1-4503-8473-5,,2021,2023-11-06 01:29:54,2023-11-06 01:29:54,926–935,AIES '21,Association for Computing Machinery,9RQL7PD8,0.016260162601626,0.25,0.1,0.0346588331763146
325,325,Artificial Intelligence in the Government: Responses to Failures and Social Impact,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534125,"Longoni, Chiara; Cian, Luca; Kyung, Ellie",2022.0,https://doi.org/10.1145/3514094.3534125,conferencePaper,"Artificial Intelligence (AI) is pervading the government and transforming how public services are provided to consumers—from allocation of benefits to law enforcement, risk monitoring and the provision of services. Despite technological improvements, AI systems are fallible and may err. How do consumers respond when learning of AI's failures? In thirteen preregistered studies (N = 3,724), we document a robust effect of algorithmic transference: algorithmic failures are generalized more broadly than human failures. Rather than reflecting generalized algorithm aversion, algorithmic transference is rooted in social categorization: it stems from how people perceive a group of AI systems versus a group of humans—as outgroups characterized by greater homogeneity than ingroups of comparable humans. Because AI systems are perceived as more homogeneous than people, failure information about one AI algorithm is transferred to another algorithm at a higher rate than failure information about a person is transferred to another person. Assessing AI's impact on consumers and societies, we show how the premature or mismanaged deployment of faulty AI technologies may engender algorithmic transference and undermine the very institutions that AI systems are meant to modernize.",artificial intelligence; public policy; algorithm; government; social impact,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,446,AIES '22,Association for Computing Machinery,CYJF5W7T,0.0384615384615384,0.0,0.0,0.0342326427921417
326,326,Tech Worker Organizing for Power and Accountability,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533111,"Boag, William; Suresh, Harini; Lepe, Bianca; D'Ignazio, Catherine",2022.0,https://doi.org/10.1145/3531146.3533111,conferencePaper,"In recent years, there has been a growing interest in the field of “AI Ethics” and related areas. This field is purposefully broad, allowing for the intersection of numerous subfields and disciplines. However, a lot of work in this area thus far has centered computational methods, leading to a narrow lens where technical tools are framed as solutions for broader sociotechnical problems. In this work, we discuss a less-explored mode of what it can mean to “do” AI Ethics: tech worker collective action. Through collective action, the employees of powerful tech companies can act as a countervailing force against strong corporate impulses to grow or make a profit to the detriment of other values. In this work, we ground these efforts in existing scholarship of social movements and labor organizing. We characterize 150 documented collective actions, and explore several case studies of successful campaigns. Looking forward, we also identify under-explored types of actions, and provide conceptual frameworks and inspiration for how to utilize worker organizing as an effective lever for change.",,978-1-4503-9352-2,,2022,2023-11-06 01:30:01,2023-11-06 01:30:01,452–463,FAccT '22,Association for Computing Machinery,JFSD2QD5,0.0350877192982456,0.0,0.0,0.0335343567251461
327,327,Navigating the Limits of AI Explainability: Designing for Novice Technology Users in Low-Resource Settings,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604759,"Okolo, Chinasa T.",2023.0,https://doi.org/10.1145/3600211.3604759,conferencePaper,"As researchers and technology companies rush to develop artificial intelligence (AI) applications that aid the health of marginalized communities, it is critical to consider the needs of the community health workers (CHWs) who will be increasingly expected to oper ate tools that incorporate these technologies. My previous work has shown that these users have low levels of Al knowledge, form incor- rect mental models about how AI works, and at times, may trust al- gorithmic decisions more than their own. This is concerning, given that AI applications targeting the work of CHWs are already in active development, and early deployments in low-resource health- care settings have already reported failures that created additional workflow inefficiencies and inconvenienced patients. Explainable AI (XAI) can help avoid such pitfalls, but nearly all prior work has focused on users that live in relatively resource-rich settings (e.g., the US and Europe) and that arguably have substantially more experience with digital technologies such as AI. My research works to develop XAI for people with low levels of formal education and technical literacy, with a focus on healthcare in low-resource do- mains. This work involves demoing interactive prototypes with CHWs to understand what aspects of model decision-making need to be explained and how they can be explained most effectively, with the goal of improving how current XAI methods target novice technology users. I am the first author of the three research studies presented in this document.",Artificial Intelligence; Explainability; Machine Learning; Community Health Workers; Global South; HCI4D; ICTD; Mobile Health,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,959–961,AIES '23,Association for Computing Machinery,9NRQM6EW,0.0336134453781512,0.0,0.0714285714285714,0.0335048626192049
329,329,Machine Learning Practices and Infrastructures,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604689,"Berman, Glen",2023.0,https://doi.org/10.1145/3600211.3604689,conferencePaper,"Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers’ have demonstrated to be particularly salient for the societal impact of deployed ML systems.",infrastructure studies; machine learning; social practice,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,466–481,AIES '23,Association for Computing Machinery,W24XYMUD,0.035,0.0,0.0,0.0325608930987821
330,330,AI Art and Its Impact on Artists,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604681,"Jiang, Harry H.; Brown, Lauren; Cheng, Jessica; Khan, Mehtab; Gupta, Abhishek; Workman, Deja; Hanna, Alex; Flowers, Johnathan; Gebru, Timnit",2023.0,https://doi.org/10.1145/3600211.3604681,conferencePaper,"The last 3 years have resulted in machine learning (ML)-based image generators with the ability to output consistently higher quality images based on natural language prompts as inputs. As a result, many popular commercial “generative AI Art” products have entered the market, making generative AI an estimated $48B industry&nbsp;[125]. However, many professional artists have spoken up about the harms they have experienced due to the proliferation of large scale image generators trained on image/text pairs from the Internet. In this paper, we review some of these harms which include reputational damage, economic loss, plagiarism and copyright infringement. To guard against these issues while reaping the potential benefits of image generators, we provide recommendations such as regulation that forces organizations to disclose their training data, and tools that help artists prevent using their content as training data without their consent.",,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,363–374,AIES '23,Association for Computing Machinery,QPU9F4A4,0.0287769784172661,0.0,0.1428571428571428,0.0324906765475571
331,331,AI Alignment and Human Reward,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462570,"Butlin, Patrick",2021.0,https://doi.org/10.1145/3461702.3462570,conferencePaper,"According to a prominent approach to AI alignment, AI agents should be built to learn and promote human values. However, humans value things in several different ways: we have desires and preferences of various kinds, and if we engage in reinforcement learning, we also have reward functions. One research project to which this approach gives rise is therefore to say which of these various classes of human values should be promoted. This paper takes on part of this project by assessing the proposal that human reward functions should be the target for AI alignment. There is some reason to believe that powerful AI agents which were aligned to values of this form would help us to lead good lives, but there is also considerable uncertainty about this claim, arising from unresolved empirical and conceptual issues in human psychology.",value alignment; human values; reward functions; value learning,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,437–445,AIES '21,Association for Computing Machinery,4ZJA99V8,0.0289855072463768,0.0,0.2,0.0323546828736169
332,332,Fairness and Machine Fairness,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462577,"Castro, Clinton; O'Brien, David; Schwan, Ben",2021.0,https://doi.org/10.1145/3461702.3462577,conferencePaper,"Prediction-based decisions, which are often made by utilizing the tools of machine learning, influence nearly all facets of modern life. Ethical concerns about this widespread practice have given rise to the field of fair machine learning and a number of fairness measures, mathematically precise definitions of fairness that purport to determine whether a given prediction-based decision system is fair. Following Reuben Binns (2017), we take ""fairness"" in this context to be a placeholder for a variety of normative egalitarian considerations. We explore a few fairness measures to suss out their egalitarian roots and evaluate them, both as formalizations of egalitarian ideas and as assertions of what fairness demands of predictive systems. We pay special attention to a recent and popular fairness measure, counterfactual fairness, which holds that a prediction about an individual is fair if it is the same in the actual world and any counterfactual world where the individual belongs to a different demographic group (cf. Kusner et al. 2018).",fairness; fair machine learning; technology ethics,978-1-4503-8473-5,,2021,2023-11-06 01:29:54,2023-11-06 01:29:54,446,AIES '21,Association for Computing Machinery,P6LRTMZK,0.0186335403726708,0.3333333333333333,0.0,0.0320920142231807
333,333,Karl Jaspers and Artificial Neural Nets: On the Relation of Explaining and Understanding Artificial Intelligence in Medicine,Ethics and Inf. Technol.,24.0,3,10.1007/s10676-022-09650-1,"Starke, Georg; Poppe, Christopher",2022.0,https://doi.org/10.1007/s10676-022-09650-1,journalArticle,"Assistive systems based on Artificial Intelligence (AI) are bound to reshape decision-making in all areas of society. One of the most intricate challenges arising from their implementation in high-stakes environments such as medicine concerns their frequently unsatisfying levels of explainability, especially in the guise of the so-called black-box problem: highly successful models based on deep learning seem to be inherently opaque, resisting comprehensive explanations. This may explain why some scholars claim that research should focus on rendering AI systems understandable, rather than explainable. Yet, there is a grave lack of agreement concerning these terms in much of the literature on AI. We argue that the seminal distinction made by the philosopher and physician Karl Jaspers between different types of explaining and understanding in psychopathology can be used to promote greater conceptual clarity in the context of Machine Learning (ML). Following Jaspers, we claim that explaining and understanding constitute multi-faceted epistemic approaches that should not be seen as mutually exclusive, but rather as complementary ones as in and of themselves they are necessarily limited. Drawing on the famous example of Watson for Oncology we highlight how Jaspers’ methodology translates to the case of medical AI. Classical considerations from the philosophy of psychiatry can therefore inform a debate at the centre of current AI ethics, which in turn may be crucial for a successful implementation of ethically and legally sound AI in medicine.",,,1388-1957,2022-09,2023-11-06 01:30:04,2023-11-06 01:30:04,,,,4PHV7RN4,0.0346320346320346,0.0,0.0,0.0320849573518612
335,335,Monitoring AI Services for Misuse,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462566,"Javadi, Seyyed Ahmad; Norval, Chris; Cloete, Richard; Singh, Jatinder",2021.0,https://doi.org/10.1145/3461702.3462566,conferencePaper,"Given the surge in interest in AI, we now see the emergence of Artificial Intelligence as a Service (AIaaS). AIaaS entails service providers offering remote access to ML models and capabilities at arms-length', through networked APIs. Such services will grow in popularity, as they enable access to state-of-the-art ML capabilities, 'on demand', 'out of the box', at low cost and without requiring training data or ML expertise. However, there is much public concern regarding AI. AIaaS raises particular considerations, given there is much potential for such services to be used to underpin and drive problematic, inappropriate, undesirable, controversial, or possibly even illegal applications. A key way forward is through service providers monitoring their AI services to identify potential situations of problematic use. Towards this, we elaborate the potential for 'misuse indicators' as a mechanism for uncovering patterns of usage behaviour warranting consideration or further investigation. We introduce a taxonomy for describing these indicators and their contextual considerations, and use exemplars to demonstrate the feasibility analysing AIaaS usage to highlight situations of possible concern. We also seek to draw more attention to AI services and the issues they raise, given AIaaS' increasing prominence, and the general calls for the more responsible and accountable use of AI.",artificial intelligence; law; accountability; machine learning; cloud; misuse,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,597–607,AIES '21,Association for Computing Machinery,5ZSZLIE8,0.0292682926829268,0.0,0.2,0.0315129725002429
336,336,"Target Specification Bias, Counterfactual Prediction, and Algorithmic Fairness in Healthcare","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604678,"Tal, Eran",2023.0,https://doi.org/10.1145/3600211.3604678,conferencePaper,"Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology – the science of measurement – suggests ways of counteracting target specification bias and avoiding its harmful consequences.",accuracy; bias; data ethics; decision support tools; fairness; healthcare; measurement; metrology; philosophy of science; Supervised machine learning,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,312–321,AIES '23,Association for Computing Machinery,CTWGZWTM,0.0135135135135135,0.1764705882352941,0.0,0.0312618655091237
337,337,Data-Centric Factors in Algorithmic Fairness,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534147,"Li, Nianyun; Goel, Naman; Ash, Elliott",2022.0,https://doi.org/10.1145/3514094.3534147,conferencePaper,"Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.",machine learning; datasets; algorithmic fairness; recidivism prediction,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,396–410,AIES '22,Association for Computing Machinery,YMNL59JC,0.0333333333333333,0.0,0.0,0.030922431865828
338,338,Computing Plans That Signal Normative Compliance,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462607,"Grastien, Alban; Benn, Claire; Thiébaux, Sylvie",2021.0,https://doi.org/10.1145/3461702.3462607,conferencePaper,"There has been increasing acceptance that agents must act in a way that is sensitive to ethical considerations. These considerations have been cashed out as constraints, such that some actions are permissible, while others are impermissible. In this paper, we claim that, in addition to only performing those actions that are permissible, agents should only perform those courses of action that are _unambiguously_ permissible. By doing so they signal normative compliance: they communicate their understanding of, and commitment to abiding by, the normative constraints in play. Those courses of action (or plans) that succeed in signalling compliance in this sense, we term 'acceptable'. The problem this paper addresses is how to compute plans that signal compliance, that is, how to find plans that are acceptable as well as permissible. We do this by identifying those plans such that, were an observer to see only part of its execution, that observer would infer the plan enacted was permissible. This paper provides a formal definition of compliance signalling within the domain of AI planning, describes an algorithm for computing compliance signalling plans, provides preliminary experimental results and discusses possible improvements. The signalling of compliance is vital for communication, coordination and cooperation in situations where the agent is partially observed. It is equally vital, therefore, to solve the computational problem of finding those plans that signal compliance. This is what this paper does.",ethics; uncertainty; communication; complexity; constraint; permissibility; planning,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,509–518,AIES '21,Association for Computing Machinery,8DSX46RW,0.017391304347826,0.2857142857142857,0.0,0.0304583409679868
339,339,"Evidence-Based AI, Ethics and the Circular Economy of Knowledge",AI Soc.,38.0,2,10.1007/s00146-022-01581-1,"Berbenni-Rehm, Caterina",2022.0,https://doi.org/10.1007/s00146-022-01581-1,journalArticle,"Everything we do in life involves a connection with information, experience and know-how: together these represent the most valuable of intangible human assets encompassing our history, cultures and wisdom. However, the more easily new technologies gather information, the more we are confronted with our limited capacity to distinguish between what is essential, important or merely ‘nice-to-have’. This article presents the case study of a multilingual Knowledge Management System, the Business enabling e-Platform that gathers and protects tacit knowledge, as the key to developing structured intellectual capital, while acknowledging the urgent need here to integrate Ethics-by-design alongside AI-by-design, in view of the far-reaching consequences involved, for good or ill. It also explains how a beneficial Circular Economy of Knowledge depends on evidence-based answers, and how harmonising taxonomies, terminology and standards promotes global trust as the basis for the unhindered transfer of tacit knowledge between generations in a way that benefits everyone.",AI; AI-by-design; Artificial intelligence; Bee-Platform; Business enabling e-Platform; Circular Economy of Knowledge; Common good; Communities of knowledge; e-Mentoring; Ethics-by-design; Explicit knowledge; Human-centered methodology; Industrialising knowledge; Knowledge protection (IPR); Multilingual big data; Organisational culture; Re-purpose knowledge; Social business model; Structured intellectual capital; Systemic Eco-Innovation; Tacit knowledge,,0951-5666,2022-11,2023-11-06 01:29:49,2023-11-06 01:29:49,889–895,,,4I7336T4,0.0066666666666666,0.0444444444444444,0.3333333333333333,0.0304294992667085
340,340,Exploitation and Algorithmic Pricing: Regressive Distributive Outcomes in Consumer-to-Business Transactions,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539542,"Dini, Arianna",2022.0,https://doi.org/10.1145/3514094.3539542,conferencePaper,"I present a theory of exploitation that can be used to appraise algorithmic pricing and identify when it might contribute to injustice. I develop a structural, consequentialist theory, which shows that exploitation is parasitic on existing inequalities. Exploitation is a sub-theme within the broader concerns of distributive egalitarians. I use ""inequality"" to signify unjust distributive inequity resulting from morally arbitrary, rather than justified, differences. This distinction is murky: skills are often as much a product of privilege as they are of work ethic. Nevertheless, some causes of distributive inequality (class, ethnicity, or gender) are more obviously morally arbitrary than others (skill or work ethic). There are many reasons for supporting distributive equality: I favour the Humean idea that equality is necessary for reciprocity, and that reciprocity is the foundation of a just social equilibrium [1][2]. However, a defence of this theory is beyond the scope of this paper; I therefore take it as a given that a reasonable degree of distributive equality is desirable in contemporary nation states, without specifying how much is ""reasonable"".The information asymmetry between sellers and consumers is the most obvious inequality in transactions mediated by algorithmic pricing. However, even without information asymmetry, dynamic pricing algorithms are designed to detect any imbalance between supply and demand, and to exploit this as much as possible to maximise profits. Since demand is a reflection of scarcity and urgency, dynamic pricing pushes prices up as far as the consumer's bargaining position will allow-without losing the consumer. I therefore address information asymmetry separately (forthcoming); here I show that the sufficient conditions for exploitation to emerge are minimal, and do not require the intent to manipulate consumers.I respond to potential criticisms of my theory. I explain why the Paretian defence-that mutually beneficial transactions are permissible, irrespective of the difference in benefit [3]-only holds up in case there is a dichotomy (deal/no deal), but that the value of a theory of exploitation is precisely that it highlights the contingency of such dichotomies.This allows me answer the concerns of critics who worry that a consequentialist conception of exploitation guts the concept of its inherent wrongness [4]. In some cases, an exploitative contract may be pragmatically better than no contract, but an injustice-albeit a ""lesser evil""-is occurring. Calling it ""exploitation"" remains a useful way of drawing attention to the structural circumstances which gave rise to unfair option sets.I explain how my theory applies to algorithmic pricing in consumer-to-platform transactions, by presenting a 2020 study, which found that ride-hailing apps in Chicago were charging passengers from low-income neighbourhoods and ethnic minority backgrounds more for lifts than white passengers from more affluent neighbourhoods [5]. Reserve price is not only a marker of disposable income, but rather, of a whole set of factors that influence willingness-to-pay. Scarcity drives demand up. In low-income neighbourhoods, fewer residents are likely to demand ride-hailing services. Due to low demand, drivers are less likely to enter these areas, as doing so will decrease the chance of finding another passenger quickly. Low-income neighbourhoods thereby become under-serviced, and residents who do hail rides are effectively penalised by having to pay a surcharge. In cases that attracted widespread condemnation, price surges occurred in the wake of mass shootings, terrorist attacks, or natural disasters [6]. Such cases may be wrong for other reasons, but do not fall into the category of exploitation as I define it, since the consumers' bargaining power is diminished due to highly rare and improbable circumstances rather than structural inequality. Thus, dynamic pricing can produce regressive outcomes. This should concern anyone interested in distributive equality, and might justify interventions to mitigate the effects on vulnerable groups.",algorithmic pricing; big data; distributive justice; dynamic pricing; exploitation; pricing algorithms; structural exploitation,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,896,AIES '22,Association for Computing Machinery,23EFCGWP,0.0199004975124378,0.3076923076923077,0.1,0.0302610501321881
341,341,What Are People Talking about in #BackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerged in Online Social Movements through the Latent Dirichlet Allocation Model,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534202,"Tong, Xin; Li, Yixuan; Li, Jiayi; Bei, Rongqi; Zhang, Luyao",2022.0,https://doi.org/10.1145/3514094.3534202,conferencePaper,"Minority groups have been using social media to organize social movements that create profound social impacts. Black Lives Matter (BLM) and Stop Asian Hate (SAH) are two successful social movements that have spread on Twitter that promote protests and activities against racism and increase the public's awareness of other social challenges that minority groups face. However, previous studies have mostly conducted qualitative analyses of tweets or interviews with users, which may not comprehensively and validly represent all tweets. Very few studies have explored the Twitter topics within BLM and SAH dialogs in a rigorous, quantified and data-centered approach. Therefore, in this research, we adopted a mixed-methods approach to comprehensively analyze BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation model to understand the top high-level words and topics and (2) open-coding analysis to identify specific themes across the tweets. We collected more than one million tweets with the #blacklivesmatter and #stopasianhate hashtags and compared their topics. Our findings revealed that the tweets discussed a variety of influential topics in depth, and social justice, social movements, and emotional sentiments were common topics in both movements, though with unique subtopics for each movement. Our study contributes to the topic analysis of social movements on social media platforms in particular and the literature on the interplay of AI, ethics, and society in general.",#blacklivesmatter; #stopasianhate; ai; dirichlet allocation model; ethics; natural language processing; open-coding analysis; social movements; society; topic analysis; twitter,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,723–738,AIES '22,Association for Computing Machinery,UG76ZIEP,0.0179372197309417,0.1666666666666666,0.0,0.0300289412587857
342,342,Contrastive Counterfactual Fairness in Algorithmic Decision-Making,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534143,"Mutlu, Ece Çiğdem; Yousefi, Niloofar; Ozmen Garibay, Ozlem",2022.0,https://doi.org/10.1145/3514094.3534143,conferencePaper,"The widespread use of artificial intelligence algorithms and their role in decision-making with consequential decisions for human subjects has resulted in a growing interest in designing AI algorithms accounting for fairness considerations. There have been attempts to account for fairness of AI algorithms without compromising their accuracy to improve poorly designed algorithms that disregard sensitive attributes (e.g., age, race, and gender) at the peril of introducing or increasing bias against specific groups. Although many studies have examined the optimal trade-off between fairness and accuracy, it remains a challenge to understand the sources of unfairness in decision-making and mitigate it effectively. To tackle this problem, researchers have proposed fair causal learning approaches which assist us in modeling cause and effect knowledge structures, discovering bias sources, and refining AI algorithms to make them more transparent and explainable. In this study, we formalize probabilistic interpretations of both contrastive and counterfactual causality as essential features in order to encourage users' trust and to expand the applicability of such automated systems. We use this formalism to define a novel fairness criterion that we call contrastive counterfactual fairness. This paper introduces, to the best of our knowledge, the first probabilistic fairness-aware data augmentation approach that is based on contrastive counterfactual causality. We tested our approach on two well-known fairness-related datasets, UCI Adult and German Credit, and concluded that our proposed method has a promising ability to capture and mitigate unfairness in AI deployment. This model-agnostic approach can be used with any AI model because it is applied in pre-processing.",contrastive fairness; counterfactual fairness; fair causal learning; fair classification; fair data augmentation,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,499–507,AIES '22,Association for Computing Machinery,4I8EUDZS,0.0276679841897233,0.0833333333333333,0.0,0.0299126034094598
343,343,Dynamic Fleet Management and Household Feedback for Garbage Collection,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534152,"Aleksandrov, Martin Damyanov",2022.0,https://doi.org/10.1145/3514094.3534152,conferencePaper,"We propose a solution for intelligent household garbage collection in smart cities. Garbage containers are assumed to be digitalized with Internet-of-Things sensors that are capable of sensing the fill levels of containers and transmitting this data through LoRaWAN networks to a central server. Data is used for dynamic fleet management and household feedback. We give a number of algorithms for these tasks. Fleet management requires scheduling containers for collections and assigning containers to trucks, as well as routing the trucks. Drivers receive such navigations via pervasive computing devices such as tablets, phones, or watches. Household feedback consists of information about the levels of generated garbage and the associated costs. Households receive this information on their home devices. Thus, unlike present solutions, our solution involves households in the intelligent collection of their garbage.",Ethics; Garbage Management; Vehicle Routing Problem,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,36–45,AIES '22,Association for Computing Machinery,M5KR2GTK,0.0151515151515151,0.3333333333333333,0.0,0.0297068574419822
344,344,Opportunities and Challenges of Automatic Speech Recognition Systems for Low-Resource Language Speakers,Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems,,,10.1145/3491102.3517639,"Reitmaier, Thomas; Wallington, Electra; Kalarikalayil Raju, Dani; Klejch, Ondrej; Pearson, Jennifer; Jones, Matt; Bell, Peter; Robinson, Simon",2022.0,https://doi.org/10.1145/3491102.3517639,conferencePaper,"Automatic Speech Recognition (ASR) researchers are turning their attention towards supporting low-resource languages, such as isiXhosa or Marathi, with only limited training resources. We report and reflect on collaborative research across ASR &amp; HCI to situate ASR-enabled technologies to suit the needs and functions of two communities of low-resource language speakers, on the outskirts of Cape Town, South Africa and in Mumbai, India. We build on longstanding community partnerships and draw on linguistics, media studies and HCI scholarship to guide our research. We demonstrate diverse design methods to: remotely engage participants; collect speech data to test ASR models; and ultimately field-test models with users. Reflecting on the research, we identify opportunities, challenges, and use-cases of ASR, in particular to support pervasive use of WhatsApp voice messaging. Finally, we uncover implications for collaborations across ASR &amp; HCI that advance important discussions at CHI surrounding data, ethics, and AI.",automatic speech recognition; mobile devices; Speech/language,978-1-4503-9157-3,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,,CHI '22,Association for Computing Machinery,DFM6VJTV,0.0340136054421768,0.0,0.0,0.0293573188874882
346,346,The Opacity of Automated Decision-Making Systems (ADMS) and Its Challenges for Political Legitimacy in a Democracy,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539563,"Jiménez, María Carolina",2022.0,https://doi.org/10.1145/3514094.3539563,conferencePaper,"This paper focuses specifically on Automated Decision-Making Systems (ADMS) based on Artificial Intelligence (AI). Since the last decades, AI systems are increasingly deployed by governments across the planet to manage public infrastructures and resources, as well as to engage with citizens for the provision of public services. Their introduction is advertised as a cost-cutting tool, as well as an instrument to combat traditional institutional disfunctions such as inefficiency, understaffing, corruption and human bias.While AI offers an incredible potential for progress, an emerging body of literature highlights the challenges that AI-driven decision-making may raise for a public sector ethics. A common trait of these challenges is their being related to some form of ""epistemological opacity"" that undermines the capacity of humans to explain and justify decisions based on AI systems, detect errors or unfairness and adopt corrective actions. The situation may entail public officers and citizens taking the outcomes of AI systems at face value, thus basing their actions (wholly or in part) on pieces of information that cannot be scrutinized and/or corrected if necessary.This paper intends to contribute to an emerging but still underdeveloped trend in normative political theory that study how AI-driven decision-making is reshaping the conceptualization and assessment of interactions between citizens and public officials. The overall goal of the paper is to analyze how various sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) affecting AI systems, may undermine the democratic legitimacy of public decisions based on them.Broadly speaking, legitimacy is the property that grounds the exercise of political authority, where authority standardly means the right to rule [1]. In this paper, democratic legitimacy is understood as a distinctive form of political authority grounded in the recognition of citizens as joint legislators. The paper offers a conception of democratic legitimacy conditional on the capacity of decision-making procedures and outcomes to realize the principle of public equality, which requires citizens' control over public decision-making, as well as respect for their equal status as political decision-makers.Specifically, the paper argues that the ""epistemological opacity"" affecting AI-driven decision-making systems, brings about a mistreatment of citizens as coauthors of public decisions, which is a premise of the idea of democratic citizenship.The main conjecture is that different sources of ""epistemological opacity"" (algorithmic/legal/illiteracy/discursive) are causing the disengagement of citizens and public officers from public decision-making, either because they directly undermine necessary conditions for the realization of public equality (co-authorship/accountability/publicity), or because they hide from the public eye instances of illegitimate automation and privatization of decisional power.The paper offers a normative conception of democratic legitimacy that may contribute to efforts in various fields, including ""AI fairness"" and ""Explainable AI"", to better adapt technological tools to equality requirements distinctive of public decision-making within democratic societies.",AI ethics; algorithmic opacity; automation; democratic legitimacy; machine learning; public sector,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,901,AIES '22,Association for Computing Machinery,EBCUNNYT,0.0224719101123595,0.2727272727272727,0.0,0.0287614942427869
348,348,The ELIZA Defect: Constructing the Right Users for Generative AI,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604744,"Affsprung, Daniel",2023.0,https://doi.org/10.1145/3600211.3604744,conferencePaper,"Artificial intelligence (AI) is at the center of debates on what kind of future we want and how to bring it about. But AI ethics is not only a technical risk assessment and accounting effort, or an application of general principles to stable artifacts. It is also a social self-diagnosis, a contested and contestable assertion of values and desirable futures, and a selective understanding of the nature of AI in its different forms. In expressions of concern and efforts at preparation for increasingly powerful AI tools, we can trace the ways we imagine ourselves and our society to be compatible with AI's promises and susceptible to its dangers. The problems we notice, and the solutions we offer, arise from the interaction of these imagined elements.The socially embedded efficacy of AI tools leads many commentators to imagine their risks specifically in conjunction with understandings of society as it currently is and imaginations of how it can and should exist in the future [1]. The sense-making moves performed in the wake of developments in generative AI are thus a site to examine the movement and uses of different concepts brought together in this domain: the human, rationality, and the place of expertise. As these sense-making efforts are carried out, they become constraints on how the risks of generative AI can be noticed and understood.The problems raised by generative AI are so fundamentally tied to its performance as a simulator of human interpersonal acts that we should ask: where is the risk of generative AI located, such that the utility and the safety of the tools can be preserved after troubling cases? Boundaries between malicious deception and magnificent design are unclear without an answer to this question. Thus, to fit generative AI into our world, we are trying to answer it; this is one goal of efforts at regulation which seeks to allow the benefits of imitation to arrive while avoiding the harms of deception. In the current regulation, reporting, and corporate responses to generative AI, the challenge of safely introducing generative AI is being approached in large part as a challenge of producing the right kind of knowledge in its users.Below is a summary of my findings from three cases, chosen to investigate the following question: What ways, or whose ways, of using, knowing, and understanding generative AI are being offered as appropriate? I examine the EU AI Act language reflecting disclosure requirements for interactions with generative AI, responses to a chatbot-facilitated suicide in Belgium, and reactions to expert claims of a chatbot's sentience. In the first two cases, AI-generated content is problematic insofar as users are uninformed about its provenance or maliciously deceived by it, while users who know they are interacting with AI but behave problematically are designated as deluded or irrational. In the third case, a Google engineer who presents evidence to support claims that AI is sentient is censured as nationwide reporting denounces his claim against an expert consensus from which he is ejected. In all three cases, challenges facing widespread generative AI development and use are avoided by attending to the knowledge and understanding of those who use them rather than the functioning of the tools themselves.The EU AI Act is illuminating as a general and authoritative account of how AI interactions can be made safe, requiring first and foremost that users are informed. [2, 3] The AI Act is useful in the present paper as it shows the effort to match and reconcile a new technology with an extant set of values, chief among which is autonomy. Its reliance on disclosure reflects a general sense that harms are acceptable or unacceptable not on the basis of outcomes but based on the degree of autonomy possessed by the actors in question. Rational actors in a simulated environment are responsible for the effects of the simulation, so long as they are informed of the nature of that environment and have essentially consented to consume deceptive or false content. The other two cases I examine explore this very issue, of problematic understandings and behavior on the part of knowing users.The first of these is the case of the Belgian man. After his suicide responses from the company which provided the chatbot, media [4, 5, 6] and government [5], and prominent expert AI ethics commentary [7] characterized it as arising because the user was vulnerable and consequently did not relate to the bot in the right way. While the chatbot's emotionally charged language was seen as a part of the problem, in the reporting on this event the unanimous emphasis on the man's mental state presents the risk as arising in an interaction, in a pathological mistake of the user, rather than in the tool.Locating risk is a necessary and immensely powerful, if often unexamined step which precedes intervention in a worrisome state of affairs: where we locate risk is where we intervene. If the risk accompanying generative AI is located in the minds of uninformed or misapprehending users, disclosures and disclaimers are indeed sensible interventions. In this conception, when knowledge fails to protect the user, it is not a failed safeguard but a bad user. Problematizing user understandings in this way provides an exonerating resource for the companies providing these tools and suggests the rectitude of expert authority on the nature of these tools, by linking delays and dangers in generative AI to users who do not abide by the (strategically underdetermined) expert consensus on generative AI's accuracy, capabilities, and nature.My third case examines how the expert consensus around generative AI is maintained through the story of Blake Lemoine, who publicly announced his belief that Google's LaMDA model had become sentient and was presented by major media outlets and experts as deluded [8, 9, 10, 11, 12, 13]. In the media and corporate response to Lemoine, wherein Google questioned his sanity before firing him [13], we see his ejection from the community of experts permitted to call for greater scrutiny based on qualitative changes in the nature of these models. He becomes a layperson on account of his anthropomorphizing error. In this act of boundary work [14], policing who is in the body of experts qualified to decide on the sentience of the chatbot, and the nature of AI models in general, we must notice how small this group truly is and what Lemoine's ejection preserves. If safeguards like those Lemoine called for should follow on the kind of change he claimed to detect, and those outside Google's leadership could determine when such changes have arrived, Google would cease to lead the conversation on regulation by defining the nature of its technology. This state of affairs leaves the right relations with generative AI underdetermined but maintains that positions which challenge the expert consensus are the result of misunderstandings so significant as to disqualify the concerned party's thoughts on the matter from rational consideration. In the three cases examined here, events and concerns which threaten to depict generative AI as in need of significant scrutiny or changes are defused not by intervening in the company's technology, but by delineating between user understandings which are empowered and exploitative, safe and vulnerable, rational and deluded.Named after an early chatbot, the ELIZA effect refers to the readiness with which users anthropomorphize computer systems [15]. Reporting on both Lemoine [11] and the Belgian man cited this effect [6]. The chatbot which encouraged the Belgian man to commit suicide was named Eliza. One way of summarizing the change I trace in the cases described above is a transition away from the Turing test and towards the ELIZA effect as the conceptual frame for AI which imitates humans. While the Turing test implies the layperson's relevance to the discussion and regulation of AI, the ELIZA effect implies their irrelevance.This project will continue as an effort to follow popular, expert, and regulatory perceptions of the risk of generative AI as the tools themselves and the public concern surrounding them continue to develop. The resources of science and technology studies (STS) enable crucial perspectives on numerous ways of thinking about AI and the challenges of its development and regulation such as the common citations of law lag, invocations of self-regulation in the mode of the Asilomar Conference on rDNA, collective action problem framings, and more. The STS literature on sociotechnical imaginaries [1] and public understandings of science [16] contribute to the present insight as to how the efforts of tech-society reconciliation and risk-benefit balancing presented as appropriate for AI reveal and produce our understandings of the technology, even as they reproduce and reshape social norms. There is an urgent need for work which extends this powerful scholarly tradition for understanding science, technology, and society to AI, as one of the most important and concerning technological developments of our moment.",chatbots; expertise; generative AI; public understanding of science; science and technology studies,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,945–946,AIES '23,Association for Computing Machinery,KXSAZVV2,0.027378507871321,0.0833333333333333,0.1,0.0284745060657238
350,350,Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Feminicide Counterdata Collection,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533132,"Suresh, Harini; Movva, Rajiv; Dogan, Amelia Lee; Bhargava, Rahul; Cruxen, Isadora; Cuba, Angeles Martinez; Taurino, Guilia; So, Wonyoung; D'Ignazio, Catherine",2022.0,https://doi.org/10.1145/3531146.3533132,conferencePaper,"Data ethics and fairness have emerged as important areas of research in recent years. However, much work in this area focuses on retroactively auditing and “mitigating bias” in existing, potentially flawed systems, without interrogating the deeper structural inequalities underlying them. There are not yet examples of how to apply feminist and participatory methodologies from the start, to conceptualize and design machine learning-based tools that center and aim to challenge power inequalities. Our work targets this more prospective goal. Guided by the framework of data feminism, we co-design datasets and machine learning models to support the efforts of activists who collect and monitor data about feminicide&nbsp;—&nbsp;gender-based killings of women and girls. We describe how intersectional feminist goals and participatory processes shaped each stage of our approach, from problem conceptualization to data collection to model evaluation. We highlight several methodological contributions, including 1) an iterative data collection and annotation process that targets model weaknesses and interrogates framing concepts (such as who is included/excluded in “feminicide”), 2) models that explicitly focus on intersectional identities rather than statistical majorities, and 3) a multi-step evaluation process&nbsp;—&nbsp;with quantitative, qualitative and participatory steps&nbsp;—&nbsp;focused on context-specific relevance. We also distill insights and tensions that arise from bridging intersectional feminist goals with ML. These include reflections on how ML may challenge power, embrace pluralism, rethink binaries and consider context, as well as the inherent limitations of any technology-based solution to address durable structural inequalities.",,978-1-4503-9352-2,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,667–678,FAccT '22,Association for Computing Machinery,827GBDBW,0.0297872340425531,0.0,0.0,0.0279819471308833
351,351,Reward Reports for Reinforcement Learning,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604698,"Gilbert, Thomas Krendl; Lambert, Nathan; Dean, Sarah; Zick, Tom; Snoswell, Aaron; Mehta, Soham",2023.0,https://doi.org/10.1145/3600211.3604698,conferencePaper,"Building systems that are good for society in the face of complex societal effects requires a dynamic approach. Recent approaches to machine learning (ML) documentation have demonstrated the promise of discursive frameworks for deliberation about these complexities. However, these developments have been grounded in a static ML paradigm, leaving the role of feedback and post-deployment performance unexamined. Meanwhile, recent work in reinforcement learning has shown that the effects of feedback and optimization objectives on system behavior can be wide-ranging and unpredictable. In this paper we sketch a framework for documenting deployed and iteratively updated learning systems, which we call Reward Reports. Taking inspiration from technical concepts in reinforcement learning, we outline Reward Reports as living documents that track updates to design choices and assumptions behind what a particular automated system is optimizing for. They are intended to track dynamic phenomena arising from system deployment, rather than merely static properties of models or data. After presenting the elements of a Reward Report, we discuss a concrete example: Meta’s BlenderBot 3 chatbot. Several others for game-playing (DeepMind’s MuZero), content recommendation (MovieLens), and traffic control (Project Flow) are included in the appendix.",disaggregated evaluation; documentation; ethical considerations; reporting; Reward function,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,84–130,AIES '23,Association for Computing Machinery,Z7WQV7W3,0.0052910052910052,0.375,0.0,0.0279224117240366
352,352,Values in Emotion Artificial Intelligence Hiring Services: Technosolutions to Organizational Problems,Proc. ACM Hum.-Comput. Interact.,7.0,CSCW1,10.1145/3579543,"Roemmich, Kat; Rosenberg, Tillie; Fan, Serena; Andalibi, Nazanin",2023.0,https://doi.org/10.1145/3579543,journalArticle,"Despite debates about emotion artificial intelligence's (EAI) validity, legality, and social consequences, EAI is increasingly present in the high stakes context of hiring, with potential to shape the future of work and the workforce. The values laden in technology play a significant role in its societal impact.We conducted qualitative content analysis on the public-facing websites (N=229) of EAI hiring services. We identify the organizational problems that EAI hiring services claim to solve and reveal the values emerging in desired EAI uses as promoted by EAI hiring services to solve organizational problems. Our findings show that EAI hiring services market their technologies as technosolutions to three purported organizational hiring problems: 1) hiring (in)accuracy, 2) hiring (mis)fit, and 3) hiring (in)authenticity. We unpack these problems to expose how these desired uses of EAI are legitimized by the corporate ideals of data-driven decision making, continuous improvement, precision, loyalty, and stability. We identify the unfair and deceptive mechanisms by which EAI hiring services claim to solve the purported organizational hiring problems, suggesting that they unfairly exclude and exploit job candidates through EAI's creation, extraction, and affective commodification of a candidate's affective value through pseudoscientific approaches. Lastly, we interrogate EAI hiring service claims to reveal the core values that underpin their stated desired use: techno-omnipresence, techno-omnipotence, and techno-omniscience. We show how EAI hiring services position desired use of their technology as a moral imperative for hiring organizations with supreme capabilities to solve organizational hiring problems, then discuss implications for fairness, ethics, and policy in EAI-enabled hiring within the US policy landscape.",labor; ai ethics; emotion recognition; algorithmic decision-making; affective computing; artificial emotional intelligence; emotion ai; ai; employment assessments; future of work; psychometrics; recruiting software; talent; talent acquisition; values,,,2023-04,2023-11-06 01:30:03,2023-11-06 01:30:03,,,,GHLWSCW9,0.0078125,0.1851851851851851,0.0,0.0276368553920329
353,353,Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445937,"Kroll, Joshua A.",2021.0,https://doi.org/10.1145/3442188.3445937,conferencePaper,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.",AI ethics; accountability; transparency; AI principles; traceability,978-1-4503-8309-7,,2021,2023-11-06 01:30:00,2023-11-06 01:30:00,758–771,FAccT '21,Association for Computing Machinery,XIJ2AE62,0.0044642857142857,0.5714285714285714,0.0,0.0275113293051359
355,355,From Preference Elicitation to Participatory ML: A Critical Survey &amp; Guidelines for Future Research,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604661,"Feffer, Michael; Skirpan, Michael; Lipton, Zachary; Heidari, Hoda",2023.0,https://doi.org/10.1145/3600211.3604661,conferencePaper,"The AI Ethics community faces an imperative to empower stakeholders and impacted community members so that they can scrutinize and influence the design, development, and use of AI systems in high-stakes domains. While a growing chorus of recent papers has kindled interest in so-called “participatory ML” methods, precisely what form participation ought to take and how to operationalize these ambitions are seldom addressed. Our survey of the relevant literature shows that in many papers, participation is reduced to highly structured, computational mechanisms designed to elicit mathematically tractable approximations of narrowly-defined moral values. Of papers that actually engage with real people, these engagements typically consist of one-time interactions with individuals that are often unrepresentative of the relevant stakeholders. Motivated by these clear limitations, we introduce a consolidated set of axes to evaluate and improve participatory approaches. We use these axes to analyze contemporary work in this space and outline future AI research directions that could meaningfully contribute to operationalizing the ideal of participation.",elicitation; Participation; value-alignment,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,38–48,AIES '23,Association for Computing Machinery,GY3NVX68,0.0308641975308641,0.0,0.0,0.0274085435128379
356,356,Can We Obtain Fairness For Free?,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462614,"Islam, Rashidul; Pan, Shimei; Foulds, James R.",2021.0,https://doi.org/10.1145/3461702.3462614,conferencePaper,"There is growing awareness that AI and machine learning systems can in some cases learn to behave in unfair and discriminatory ways with harmful consequences. However, despite an enormous amount of research, techniques for ensuring AI fairness have yet to see widespread deployment in real systems. One of the main barriers is the conventional wisdom that fairness brings a cost in predictive performance metrics such as accuracy which could affect an organization's bottom-line. In this paper we take a closer look at this concern. Clearly fairness/performance trade-offs exist, but are they inevitable? In contrast to the conventional wisdom, we find that it is frequently possible, indeed straightforward, to improve on a trained model's fairness without sacrificing predictive performance. We systematically study the behavior of fair learning algorithms on a range of benchmark datasets, showing that it is possible to improve fairness to some degree with no loss (or even an improvement) in predictive performance via a sensible hyper-parameter selection strategy. Our results reveal a pathway toward increasing the deployment of fair AI methods, with potentially substantial positive real-world impacts.",AI and society; deployment of fairness techniques; fairness in AI; fairness/performance trade-offs; practical barriers,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,586–596,AIES '21,Association for Computing Machinery,8ZZS7MFZ,0.0167597765363128,0.1428571428571428,0.0,0.0273062297544716
357,357,Who's Responsible? Jointly Quantifying the Contribution of the Learning Algorithm and Data,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462574,"Yona, Gal; Ghorbani, Amirata; Zou, James",2021.0,https://doi.org/10.1145/3461702.3462574,conferencePaper,"A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.",fairness; accountability; machine learning; data valuation,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,1034–1041,AIES '21,Association for Computing Machinery,C8J2KSWE,0.0129032258064516,0.1666666666666666,0.0833333333333333,0.0272559908556631
358,358,Preserving Our Mental Autonomy: Freedom of Thought in the Speech 4.0 Era,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539520,"Yildirim, Emine Ozge",2022.0,https://doi.org/10.1145/3514094.3539520,conferencePaper,"The proliferation of new technologies paved the way for novel or contemporary methods of propaganda, shifting from traditional methods to computational ones that incorporate psychoanalytic techniques, behavioral microtargeting, personalized content delivery, automated bots, and widely accessible social media platforms. While digitalization aids the society in many affirmative aspects, above-referred tools could also be used by malicious or adversarial actors to manipulate the masses or targeted individuals, disregarding human mental autonomy and dignity.Mill wrote in On Liberty, ""Over himself, over his own body and mind, the individual is sovereign."" [1] Quoting Mill, Boire states, what happens inside a person's body or mind is that person's private business, not the business of society and certainly not the business of the government.[2] The recent examples around the world show that protection of mental autonomy cannot be taken for granted. Thus, this doctoral proposal seeks to describe how digital technologies have changed the understanding and methods of manipulation and governmental computational propaganda and why these changed methods have the potential to illegitimately interfere with the minds of individuals and violate their rights to freedom of thought. It also explains why the current freedom of thought framework might be inefficient in the contemporary political speech context.",freedom of thought; cognitive liberty; computational propaganda; freedom of expression; manipulation; mental autonomy,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,919,AIES '22,Association for Computing Machinery,J2GRPTHC,0.0199004975124378,0.0769230769230769,0.0833333333333333,0.0269159818883112
359,359,The Danger of Citizen Domination through Algorithmic Decision-Making in the Public Sector,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539515,"Misic, Jana",2022.0,https://doi.org/10.1145/3514094.3539515,conferencePaper,"Applying for and gaining welfare benefits from the government is a different practice than a decade ago. Digital technologies, including emerging ones such as artificial intelligence and big data are increasingly popular in the public services sector. One such instance is a pilot project employing citizen profiling through artificial intelligence to implement a more efficient welfare policy in The Hague. The 'Bijstand' (prim. trans. 'Assistance') project relies on the use of machine learning to detect unemployed citizens that could be most efficiently helped by the municipality in finding employment. Intuitively, deployment of such projects has a potentially negative impact on how just and fair procedures of access to employment are and is therefore a cause for concern in respect to citizens' wellbeing.In this paper I aim to validate or challenge the intuition that the citizens who are subjected to public sector decision-making algorithms are in fact dominated upon. Is it feasible or fair to call the algorithmic toll a dominator or should we focus attention on the human agents? I draw on the neo-republican concept of freedom as non-domination and argue for the importance of investigating potential for domination as a structurally constituted form of power when algorithmization of public services takes place. Going beyond, I combine the insights from the neo-republican theory of domination with applied ethics of technology, drawing on the relationships between virtues of freedom and wellbeing vis-a-vis algorithmic accountability. The case study reveals that a new procedural element of unaccountable decision-making is being introduced into the welfare services. A hopeful takeaway from using the neo-republican lens shows that the state can be a guarantor of freedom by establishing the right procedures and venues for the citizens to challenge algorithmic decision-making.",algorithms; automated decision-making; non-domination; public sector,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,905,AIES '22,Association for Computing Machinery,JQGCGD2K,0.0246478873239436,0.0,0.0833333333333333,0.0263730567581885
360,360,Anticipatory Regulatory Instruments for AI Systems: A Comparative Study of Regulatory Sandbox Schemes,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604732,"Morgan, Deborah",2023.0,https://doi.org/10.1145/3600211.3604732,conferencePaper,"Anticipatory regulatory instruments are pre-emptive approaches to identify and anticipate risks arising from new technologies. They can also act as indicators of ‘pro-innovation’ economic support for digital technologies. The extent to which regulatory agencies can fulfil their regulatory remit, aimed at the protection of the public good, and signal support for innovative and disruptive technologies is an open policy question. Regulatory sandbox schemes are comparatively new anticipatory tools, operating within a small number of regulators, and their potential to assess contextual or cross-sectoral risk is unclear. However, emerging proposals for the regulation of AI increasing feature various models of regulatory sandboxes often aligned to the need to reduce access barriers for SMEs and innovators. Examples include the European Commission's Proposal for a regulation concerning AI [3] and the recent United Kingdom AI White Paper, AI Regulation: A Pro-Innovation Approach [8]. Disentangling the causal dimensions of why regulatory sandboxes are proposed to regulate AI, and their utility as tools of pre-emptive risk assessment are my core research questions.The regulation of emerging digital technologies present challenges for regulators and governments in monitoring rapid global developments and in anticipating novel forms of risk [9]. Nesta introduced the term anticipatory regulation, and such approaches potentially provide ‘a set of behaviours and tools – i.e., a way of working – that is intended to help regulators identify, build and test solutions to emerging challenges’ [4]. Regulatory sandboxes are a prominent, and arguably the most widespread, example of such an anticipatory regulatory tool. Whilst there are varied definitions of regulatory sandbox schemes, existing schemes allow small-scale, live testing of innovations in a controlled environment under the supervision of a regulatory authority [6]. A small number of regulatory sandbox schemes are in operation within the UK operating within sectoral and cross-sectoral regulatory remits. However, empirical data and academic literature regarding the methodologies and operation of these current schemes, and literature exploring regulatory sandboxes more broadly, is scarce [7, 10].The ontological focus of my work is critical realist, which accepts the external reality of the design and instrumental aims of sandbox schemes, whilst seeking to understand the underlying causes and drivers for their use and rapid promotion. To locate such causes and explanations it is necessary to examine existing schemes within the ‘rules and norms’ of their institutional context and structures [1, 2]. Institutional analysis will isolate the key dimensions of each scheme, consider the influence of the regulatory structures, and then test such analysis through empirical research with regulatory and policy actors. The core hypothesis of my research is that regulatory contexts, path dependencies and conceptions of risk are significant causal elements within existing sandbox schemes and, as such, may present a challenge when designing and deploying cross-sectoral sandbox schemes for AI systems.I have already undertaken analysis of the two regulatory sandbox schemes applying the Institutional Analysis and Development framework of Elinor Ostrom [5]. This analysis has highlighted significant dimensions of sandbox schemes including the role and forms of sectoral incentives for participants, how knowledge and conceptions of risk are shared and the potential role of participatory processes and stakeholders. I am drafting a forthcoming paper outlining a typology of incentives for existing regulatory sandbox schemes. I have included policy and wider sectoral stakeholders within my data collection to obtain perspectives regarding perceived utility, understandings, and conceptions of sandbox schemes. Incorporating collaborative processes and inclusive engagement with affected stakeholders is a key principle of anticipatory regulation [4]. The role and extent of such engagement within proposed sandbox schemes for AI is a further dimension of my research to consider how such processes may be developed and operationalised.This work is undertaken at a time of rapid progression within AI systems and in the development of proposed AI regulation and varied forms of decentralised AI governance. I hope that my research will provide understanding of the utility, and potential limitations, of sandboxes as a regulatory tool drawing upon data from existing practices. My work may also impact existing policy discussions around the role of sandbox schemes as risk assessment and information monitoring tools for regulators.",,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,980–981,AIES '23,Association for Computing Machinery,QH6H3AEC,0.0251479289940828,0.0,0.0769230769230769,0.0262350994599515
361,361,Bias in Artificial Intelligence Models in Financial Services,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539561,"Pavón Pérez, Ángel",2022.0,https://doi.org/10.1145/3514094.3539561,conferencePaper,"Nowadays, artificial intelligence models are widely used in financial services, from credit scoring to fraud detection, having a direct impact on our daily lives. Although such models have been developed to try to reduce human bias and thus bring greater fairness to financial services decisions, studies have found that there is still significant discrimination by both face-to-face and algorithmic lenders. In fact, Apple has recently been investigated for gender discrimination in assigning a credit limit to its users, demonstrating that there may still be inherent biases in the development of such algorithms and models. Furthermore, biases in financial services models may not only lead to unfair discrimination but were also linked to health problems and recovery prospects.This project aims to analyse and identify the different types of biases found in AI models and data used in the financial services industry. We propose a method using data analysis and explainable models to explain how these biases emerge throughout the process of developing AI models as well as applying state-of-the-art bias dealing techniques to avoid and mitigate them. Finally, we propose how to evaluate these models according to the business objectives and consider possible trade-offs between different definitions of fairness. Thus, the main questions that this project will try to answer are as follows: - What are the current biases in credit risk and fraud detection models and how to identify them? - In what ways understanding how biases emerge from the data can help us in bias mitigation? - To what extent could credit risk and fraud detection models bias be mitigated, and what are the implications of those mitigation techniques?Answering these questions, we hope to create a pipeline for building these models by understanding the key points where bias can emerge and the appropriate methods to avoid it.",fairness; bias; machine learning; financial services,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,908,AIES '22,Association for Computing Machinery,6GAYZWMS,0.0268456375838926,0.0,0.0,0.0253556913948956
362,362,A Deep Dive into Dataset Imbalance and Bias in Face Identification,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604691,"Cherepanova, Valeriia; Reich, Steven; Dooley, Samuel; Souri, Hossein; Dickerson, John; Goldblum, Micah; Goldstein, Tom",2023.0,https://doi.org/10.1145/3600211.3604691,conferencePaper,"As the deployment of automated face recognition (FR) systems proliferates, bias in these systems is not just an academic question, but a matter of public concern. Media portrayals often center imbalance as the main source of bias, i.e., that FR models perform worse on images of non-white people or women because these demographic groups are underrepresented in training data. Recent academic research paints a more nuanced picture of this relationship. However, previous studies of data imbalance in FR have focused exclusively on the face verification setting, while the face identification setting has been largely ignored, despite being deployed in sensitive applications such as law enforcement. This is an unfortunate omission, as ‘imbalance’ is a more complex matter in identification; imbalance may arise in not only the training data, but also the testing data, and furthermore may affect the proportion of identities belonging to each demographic group or the number of images belonging to each identity. In this work, we address this gap in the research by thoroughly exploring the effects of each kind of imbalance possible in face identification, and discuss other factors which may impact bias in this setting.",fairness; face recognition; neural networks; data imbalance,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,229–247,AIES '23,Association for Computing Machinery,V8UFVFTM,0.0210526315789473,0.1428571428571428,0.0,0.0253498832329154
363,363,Investigating Debiasing Effects on Classification and Explainability,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534170,"Marchiori Manerba, Marta; Guidotti, Riccardo",2022.0,https://doi.org/10.1145/3514094.3534170,conferencePaper,"During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal's novelty and effectiveness for evaluation and bias detection purposes.",algorithmic bias; algorithmic auditing; bias mitigation; data equity; fairness in ml; ml evaluation; xai,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,468–478,AIES '22,Association for Computing Machinery,CHJDED7Y,0.0212765957446808,0.0714285714285714,0.0,0.0243185626215866
364,364,"Social Media Profiling Continues to Partake in the Development of Formalistic Self-Concepts. Social Media Users Think So, Too.","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534192,"Engelmann, Severin; Scheibe, Valentin; Battaglia, Fiorella; Grossklags, Jens",2022.0,https://doi.org/10.1145/3514094.3534192,conferencePaper,"Social media platforms generate user profiles to recommend informational resources including targeted advertisements. The technical possibilities of user profiling methods go beyond the classification of individuals into types of potential customers. They enable the transformation of implicit identity claims of individuals into explicit declarations of identity. As such, a key ethical challenge of social media profiling is that it stands in contrast with people's ability to self-determine autonomously, a core principle of the right to informational self-determination.In this research study, we take a step back and revisit theories of personal identity in philosophy that underline two constitutive meta-principles necessary for individuals to self-interpret autonomously: justification and control. That is, individuals have the ability to justify and control essential aspects of their self-concept. Returning to a philosophical basis for the value of self-determination serves as a reminder that user profiling is essentially normative in that it formalizes a person's self-concept within an algorithmic system. To understand whether social media users would want to justify and control social media's identity declarations, we conducted a vignette survey study (N = 368). First, participants indicate a strong preference for more transparency in social media identity declarations, a core requirement for the justification of a self-concept. Second, respondents state they would correct wrong identity declarations but show no clear motivation to manage them. Finally, our results illustrate that social media users acknowledge the narrative force of social media profiling but do not strongly believe in its capacity to shape their self-concept.",autonomy; ethics of artificial intelligence; personal identity; social media; user profiling,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,238–252,AIES '22,Association for Computing Machinery,EEZTVL5N,0.0121951219512195,0.2727272727272727,0.0,0.0235532852451654
365,365,Perceived Algorithmic Fairness Using Organizational Justice Theory: An Empirical Case Study on Algorithmic Hiring,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604677,"Juijn, Guusje; Stoimenova, Niya; Reis, João; Nguyen, Dong",2023.0,https://doi.org/10.1145/3600211.3604677,conferencePaper,"Growing concerns about the fairness of algorithmic decision-making systems have prompted a proliferation of mathematical formulations aimed at remedying algorithmic bias. Yet, integrating mathematical fairness alone into algorithms is insufficient to ensure their acceptance, trust, and support by humans. It is also essential to understand what humans perceive as fair. In this study, we, therefore, conduct an empirical user study into crowdworkers’ algorithmic fairness perceptions, focusing on algorithmic hiring. We build on perspectives from organizational justice theory, which categorizes fairness into distributive, procedural, and interactional components. By doing so, we find that algorithmic fairness perceptions are higher when crowdworkers are provided not only with information about the algorithmic outcome but also about the decision-making process. Remarkably, we observe this effect even when the decision-making process can be considered unfair, when gender, a sensitive attribute, is used as a main feature. By showing realistic trade-offs between fairness criteria, we moreover find a preference for equalizing false negatives over equalizing selection rates amongst groups. Our findings highlight the importance of considering all components of algorithmic fairness, rather than solely treating it as an outcome distribution problem. Importantly, our study contributes to the literature on the connection between mathematical– and perceived algorithmic fairness, and highlights the potential benefits of leveraging organizational justice theory to enhance the evaluation of perceived algorithmic fairness.",algorithmic decision-making; algorithmic hiring; organizational justice; perceived fairness,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,775–785,AIES '23,Association for Computing Machinery,I5GX9IH5,0.0137614678899082,0.125,0.0714285714285714,0.022838985141347
366,366,Designing Shapelets for Interpretable Data-Agnostic Classification,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462553,"Guidotti, Riccardo; Monreale, Anna",2021.0,https://doi.org/10.1145/3461702.3462553,conferencePaper,"Time series shapelets are discriminatory subsequences which are representative of a class, and their similarity to a time series can be used for successfully tackling the time series classification problem. The literature shows that Artificial Intelligence (AI) systems adopting classification models based on time series shapelets can be interpretable, more accurate, and significantly fast. Thus, in order to design a data-agnostic and interpretable classification approach, in this paper we first extend the notion of shapelets to different types of data, i.e., images, tabular and textual data. Then, based on this extended notion of shapelets we propose an interpretable data-agnostic classification method. Since the shapelets discovery can be time consuming, especially for data types more complex than time series, we exploit a notion of prototypes for finding candidate shapelets, and reducing both the time required to find a solution and the variance of shapelets. A wide experimentation on datasets of different types shows that the data-agnostic prototype-based shapelets returned by the proposed method empower an interpretable classification which is also fast, accurate, and stable. In addition, we show and we prove that shapelets can be at the basis of explainable AI methods.",decision support systems; explainable artificial intelligence; interpretable machine learning; transparent classification method,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,532–542,AIES '21,Association for Computing Machinery,Q7KQFE3L,0.0261780104712041,0.0,0.0,0.0227650171705229
367,367,Interactive Model Cards: A Human-Centered Approach to Model Documentation,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533108,"Crisan, Anamaria; Drouhard, Margaret; Vig, Jesse; Rajani, Nazneen",2022.0,https://doi.org/10.1145/3531146.3533108,conferencePaper,"Deep learning models for natural language processing (NLP) are increasingly adopted and deployed by analysts without formal training in NLP or machine learning (ML). However, the documentation intended to convey the model’s details and appropriate use is tailored primarily to individuals with ML or NLP expertise. To address this gap, we conduct a design inquiry into interactive model cards, which augment traditionally static model cards with affordances for exploring model documentation and interacting with the models themselves. Our investigation consists of an initial conceptual study with experts in ML, NLP, and AI Ethics, followed by a separate evaluative study with non-expert analysts who use ML models in their work. Using a semi-structured interview format coupled with a think-aloud protocol, we collected feedback from a total of 30 participants who engaged with different versions of standard and interactive model cards. Through a thematic analysis of the collected data, we identified several conceptual dimensions that summarize the strengths and limitations of standard and interactive model cards, including: stakeholders; design; guidance; understandability &amp; interpretability; sensemaking &amp; skepticism; and trust &amp; safety. Our findings demonstrate the importance of carefully considered design and interactivity for orienting and supporting non-expert analysts using deep learning models, along with a need for consideration of broader sociotechnical contexts and organizational dynamics. We have also identified design elements, such as language, visual cues, and warnings, among others, that support interactivity and make non-interactive content accessible. We summarize our findings as design guidelines and discuss their implications for a human-centered approach towards AI/ML documentation.",human centered design; interactive data visualization; model cards,978-1-4503-9352-2,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,427–439,FAccT '22,Association for Computing Machinery,Z47KAEVX,0.0197628458498023,0.125,0.0,0.0225505684571364
368,368,AI for/by the Majority World: From Technologies of Dispossession to Technologies of Radical Care,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3607544,"Ricaurte, Paola",2023.0,https://doi.org/10.1145/3600211.3607544,conferencePaper,"The dominant and celebratory discourse surrounding AI often fails to acknowledge the intricate dynamics and implications associated with the human, material, and environmental costs of technological development, particularly in the midst of a civilizational crisis [5]. Furthermore, hegemonic AI, primarily developed by large technology corporations, capitalizes on the resources, data, and labor of the majority world only to be deployed as a glamorous product that furthers the accumulation of privilege, wealth, and power by global elites. As a result, these hegemonic intelligent technologies originate from a predatory and violent world model that has been imposed as a universal paradigm of existence. These dominant technologies are intentionally designed to perpetuate power asymmetries. The so-called artificial intelligence, marketed as a revolutionary innovation, has proven to be the offspring of interconnected systems of oppression: a capitalist mode of production; a colonial system of epistemic, economic, social, racial, and cultural dominance; and a patriarchal order of violence that fulfills its own prophecy [10]. Artificial intelligence, driven by influential global actors with market-driven and war-driven interests, materializes as a socio-technical assemblage that optimizes capital accumulation through dispossession [3] and the exertion of violence over the territories and populations of the majority world [8]. Hegemonic AI technologies are fundamentally technologies of dispossession, appropriating the commons for their development. Their creation is governed by macro-structural forces guided by the market and powerful actors seeking control, as control is a prerequisite for wealth accumulation. Control encompasses natural resources (territory), knowledge (processing information and data), labor (productive force), bodies (labor and the capacity to produce knowledge), subjectivity (sensibility and identity), and intersubjective relations (ways of relating, living, and coexisting) [7]. Dispossession arises from the interconnections of violent systems operating at both micro and macro scales. Dispossession manifests throughout the entire lifecycle of AI, spanning from design and development to deployment, use, and disposal [6]. The human, material, and environmental costs associated with technological development are obscured by narratives emphasizing efficiency, optimization, and the automation of the world. Big capital, including finance, pharmaceuticals, agribusiness, mining, and technology, forms alliances to control global value chains and knowledge production systems, ensuring that the ultimate benefits remain concentrated in the hands of a few. Concentration of power, wealth, and knowledge widens the gaps between individuals, communities, countries, and regions, erasing them physically, socially, and epistemically. As the gap continues to widen due to the accelerating momentum of production and capitalist accumulation, the depletion of the planet’s resources and life-supporting systems draws nearer. To dismantle socio-technically mediated systems of violence, it is imperative to address power imbalances and rediscover the fundamental relational nature of existence. Alternative models of the world and dignified futures necessitate alternative models of technological development that are grounded in values associated with a radical ethics of care [1], communality [2], conviviality [4], and shared responsibility for the consequences of human impact on the planet [9].",machine learning; algorithms; data; coloniality; feminism; racial capitalism; technoscience,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,3–4,AIES '23,Association for Computing Machinery,77EC2FVM,0.0189473684210526,0.1111111111111111,0.0714285714285714,0.0225504537869992
369,369,How and to Which Extent Will the Provisions of the Digital Services Act of the European Union Impact on the Relationship between Users and Platforms as Information Providers?,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604749,"Fabbri, Matteo",2023.0,https://doi.org/10.1145/3600211.3604749,conferencePaper,"In the contemporary information age, recommender systems (RSs) play a crucial role in determining the way in which people interact and obtain information online: in fact, from social media feeds to news aggregators and e-commerce websites, users are constantly targeted by personalized recommendations about what they may like. The Digital Services Act (DSA) of the European Union1&nbsp;[3], which is the first supranational regulation addressing automated recommendations specifically, defines a RS as “a fully or partially automated system used by an online platform to suggest in its online interface specific information to recipients of the service or prioritize that information, including as a result of a search initiated by the recipient of the service or otherwise determining the relative order or prominence of information displayed” (DSA, art. 3 (s)). This definition highlights the method (“fully or partially automated”), aim (“to suggest”), content (“specific information”), target (“recipients of the service”), input (“as a result of a search initiated by the recipient”) and output (“determining the relative order or prominence of information displayed”) of a recommendation process. As it can be observed, RSs are involved in the main aspects of online interactions, and this is why their influencing potential should not be underestimated. In fact, whilst RSs are aimed to improve user’s experience by reducing the information overload, they can give rise to a variety of ethical concerns related to privacy, autonomy and fairness &nbsp;[5], to name but a few. However, independent research and users’ access to the design and functioning of the RSs implemented on mainstream platforms is usually prevented by their proprietary status. The DSA addresses this issue with a specific article, according to which “Providers of online platforms that use recommender systems shall set out in their terms and conditions, in plain and intelligible language, the main parameters used in their recommender systems, as well as any options for the recipients of the service to modify or influence those main parameters” (DSA, art.27 (1)). The aim of this provision is to “explain why certain information is suggested to the recipient of the service”: therefore, the parameters need to include, at least, “the criteria which are most significant in determining the information suggested to the recipient of the service” (i.e., content) and the reasons for its “relative importance” (i.e., ranking) (DSA, art. 27 (2)). Additionally, when options to modify or influence the main parameters are stated in the terms and conditions, “providers of online platforms shall also make available a functionality that allows the recipient of the service to select and to modify at any time their preferred option” (DSA, art. 27 (3)). In order to make this requirement work in practice, “That functionality shall be directly and easily accessible from the specific section of the online platform’s online interface where the information is being prioritised” (ibidem). Article 27 of the DSA seems to be aimed at empowering users to influence the outcome of algorithmic recommendations. Therefore, this provision addresses four of the aspects of the definition of RS provided by Article 3: method, target, input and output. In particular, the traditionally passive role of the target could be reversed, as the recipient might determine the method (through the choice of parameters) and, indirectly, also the input (the type of data to be processed through the parameters) that the RS will use to produce its output. However, platforms are not obliged to be provide options for users to modify or influence the parameters if this possibility is not specified in the terms and conditions, and platforms arguably have no interest in providing this possibility voluntarily. Therefore, this article formally grants users the right to influence the recommendation process but only in some limited cases which are not likely to happen, as&nbsp;[4] point out. Moreover, the practical impact of these provisions will probably depend on users’ ability to understand the structure and the policy of the algorithmic recommendations. It should be noted that Recital 70 of the DSA outlines a wider scope for the provisions on RSs than what is included in Article 27: indeed, the statement that “online platforms should consistently ensure that recipients of their service are appropriately informed about how recommender systems impact the way information is displayed, and can influence how information is presented to them”2 (DSA, recital 70) does not seem to be reflected in the actual provisions of Article 27, at least to the extent that the adverb “consistently” would entail. From this perspective, the right to explanation that could be identified in the “easily comprehensible manner” through which platforms “should clearly present the main parameters […] to ensure that the recipients understand how information is prioritised for them” (DSA, recital 70) might not lead to a real users’ empowerment. However, given the consequences that this recently enforced regulation can have both on the business of online platforms and on the self-determination of users, my research aims at understanding how and to which extent the DSA provisions will impact on the relationship between users and platforms as information providers, especially for what concerns their status as prominent stakeholders in the recommendation process &nbsp;[6] To help enforce the new requirements, the European Commission has recently established the European Centre for Algorithmic Transparency (ECAT), which will assess “whether very large online platforms and search engines comply with their obligations under the Digital Services Act”, including by carrying out inspections at the platforms’ premises to analyse “the design, functioning and impact of advanced algorithms, like recommender systems, in their production environments"" &nbsp;[1]. Taking the opportunities provided by the implementation of the DSA, my research will involve three main stages. Firstly, a preliminary scoping phase will involve examining the connection between the aim of the regulatory requirements presented above and the ethical issues around RSs and digital nudging identified in my past research. Secondly, I will analyse the documents that would become available as a result of public inspections, audits and assessments of RSs, and compare the findings of such review with the impact of other regulations on RSs, like the Internet Information Service Algorithmic Recommendation Management Provisions of the People’s Republic of China &nbsp;[2]. This comparative perspective may help account for the new constraints and changes concerning the implementation of RSs across the world. Thirdly, I will develop a survey-based user study to understand whether and how the availability of explanations and the opportunity to modify RSs provided by the DSA impact on the way in which users interact with online platforms. This project has a timespan of two to three years, depending on the timeliness of the release of documents that digital companies and public officials will make available and accessible by researchers. The expected result of this research is an initial map of the ethical and societal implications of the enforcement of the DSA on the transparency of proprietary RSs and the subsequent application of fundamental rights for users’ autonomy and self-determination.",Recommender Systems; Digital Services Act; AI Regulation,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,1002–1003,AIES '23,Association for Computing Machinery,YKXZN9BT,0.0183406113537117,0.4285714285714285,0.0714285714285714,0.0224914142940748
370,370,Measures of Disparity and Their Efficient Estimation,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604697,"Singh, Harvineet; Chunara, Rumi",2023.0,https://doi.org/10.1145/3600211.3604697,conferencePaper,"Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well.",AI; and well-being; disparity estimation; fairness metrics; health; optimal data collection; Social Sciences,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,927–938,AIES '23,Association for Computing Machinery,KUX35U85,0.0113636363636363,0.1538461538461538,0.0,0.0223489391084045
371,371,A Meta-Analysis of the Utility of Explainable Artificial Intelligence in Human-AI Decision-Making,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534128,"Schemmer, Max; Hemmer, Patrick; Nitsche, Maximilian; Kühl, Niklas; Vössing, Michael",2022.0,https://doi.org/10.1145/3514094.3534128,conferencePaper,"Research in artificial intelligence (AI)-assisted decision-making is experiencing tremendous growth with a constantly rising number of studies evaluating the effect of AI with and without techniques from the field of explainable AI (XAI) on human decision-making performance. However, as tasks and experimental setups vary due to different objectives, some studies report improved user decision-making performance through XAI, while others report only negligible effects. Therefore, in this article, we present an initial synthesis of existing research on XAI studies using a statistical meta-analysis to derive implications across existing research. We observe a statistically positive impact of XAI on users' performance. Additionally, the first results indicate that human-AI decision-making tends to yield better task performance on text data. However, we find no effect of explanations on users' performance compared to sole AI predictions. Our initial synthesis gives rise to future research investigating the underlying causes and contributes to further developing algorithms that effectively benefit human decision-makers by providing meaningful explanations.",explainable artificial intelligence; decision-making; empirical studies; meta-analysis,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,617–626,AIES '22,Association for Computing Machinery,Z26JTAUE,0.0253164556962025,0.0,0.0,0.0218846694796061
372,372,"Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462610,"Barocas, Solon; Guo, Anhong; Kamar, Ece; Krones, Jacquelyn; Morris, Meredith Ringel; Vaughan, Jennifer Wortman; Wadsworth, W. Duncan; Wallach, Hanna",2021.0,https://doi.org/10.1145/3461702.3462610,conferencePaper,"Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts—both beneficial and harmful—that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",artificial intelligence; fairness; machine learning; disaggregated evaluations; evaluations,978-1-4503-8473-5,,2021,2023-11-06 01:29:55,2023-11-06 01:29:55,368–378,AIES '21,Association for Computing Machinery,U9SNBVFC,0.016,0.0,0.1,0.0217442748091603
373,373,Dialogue Explanation With Reasoning for AI,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539522,"Xu, Yifan",2022.0,https://doi.org/10.1145/3514094.3539522,conferencePaper,"Explainable Artificial Intelligence is increasingly gaining attention in domains, such as self-driving cars and medical treatment. One of the most prevalent issues with these explainable models is that they are difficult to comprehend and have not been tested in real-world scenarios. In this research, I propose a dialogue-based explanation with reasoning for a rule-based system with the intention of utilising it in the future with a Neuro Symbolic AI system, to give machines the capacity to explain their actions or decisions using logic. We hypothesize that when a system makes a deduction that was, in some way, unexpected by the user then locating the source of the disagreement or misunderstanding is best achieved through a collaborative dialogue process that allows the participants to gradually isolate the cause. I also conduct a user evaluation for this hypothesis.",artificial intelligence (ai); dialogue for explanation; explainable artificial intelligence (XAI); machine reasoning; neuro symbolic,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,918,AIES '22,Association for Computing Machinery,CXSIY5AQ,0.0073529411764705,0.0714285714285714,0.1666666666666666,0.0217396075239212
374,374,Algorithmic Hiring in Practice: Recruiter and HR Professional's Perspectives on AI Use in Hiring,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462531,"Li, Lan; Lassiter, Tina; Oh, Joohee; Lee, Min Kyung",2021.0,https://doi.org/10.1145/3461702.3462531,conferencePaper,"The use of AI-enabled hiring software raises questions about the practice of Human Resource (HR) professionals' use of the software and its consequences. We interviewed 15 recruiters and HR professionals about their experiences around two decision-making processes during hiring: sourcing and assessment. For both, AI-enabled software allowed the efficient processing of candidate data, thus providing the ability to introduce or advance candidates from broader and more diverse pools. For sourcing, it can serve as a useful learning resource to find candidates. Though, a lack of trust in data accuracy and an inadequate level of control over algorithmic candidate matches can create reluctance to embrace it. For assessment, its implementation varied across companies depending on the industry and the hiring scenario. Its inclusion may redefine HR professionals' job content as it automates or augments pieces of the existing hiring process. Finally, we discuss how candidate roles that recruiters and HR professionals support drive the use of algorithmic hiring software.",future of work; algorithmic hiring; ai-enabled assessment; ai-enabled sourcing; hr professionals,978-1-4503-8473-5,,2021,2023-11-06 01:29:54,2023-11-06 01:29:54,166–176,AIES '21,Association for Computing Machinery,MREQSX7F,0.0189873417721519,0.0,0.0714285714285714,0.0215221272787735
375,375,"Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534189,"Rhea, Alene; Markey, Kelsey; D'Arinzo, Lauren; Schellmann, Hilke; Sloane, Mona; Squires, Paul; Stoyanovich, Julia",2022.0,https://doi.org/10.1145/3514094.3534189,conferencePaper,"Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers' resumes or social media profiles. We interrogate the reliability of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. We develop a methodology for an external audit of stability of algorithmic personality tests, and instantiate this methodology in an audit of two systems, Humantic AI and Crystal. Rather than challenging or affirming the assumptions made in psychometric testing – that personality traits are meaningful and measurable constructs, and that they are indicative of future success on the job – we frame our methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves.In our audit of Humantic AI and Crystal, we find that both systems show substantial instability on key facets of measurement, and so cannot be considered valid testing instruments. For example, Crystal frequently computes different personality scores if the same resume is given in PDF vs. in raw text, violating the assumption that the output of an algorithmic personality test is stable across job-irrelevant input variations. Among other notable findings is evidence of persistent — and often incorrect — data linkage by Humantic AI.An open-source implementation of our auditing methodology, and of the audits of Humantic AI and Crystal, is available at https://github.com/DataResponsibly/hiring-stability-audit.",algorithm audit; hiring; personality; reliability; stability; validity,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,572–587,AIES '22,Association for Computing Machinery,SW55FRCD,0.0199203187250996,0.0,0.0555555555555555,0.0214251783119913
376,376,Evaluation of Targeted Dataset Collection on Racial Equity in Face Recognition,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604662,"Hong, Rachel; Kohno, Tadayoshi; Morgenstern, Jamie",2023.0,https://doi.org/10.1145/3600211.3604662,conferencePaper,"Algorithmic audits of industry face recognition models have recently incentivized companies to diversify their data collection methods, which in turn has reduced error disparities along demographic lines, such as gender or race. We argue that it is important to understand exactly how various forms of targeted data collection mitigate performance disparities in these updated face recognition models. We propose an empirical framework to assess the impact of additional dataset collection targeted towards various racial groups. We apply our framework to three racially-annotated benchmark datasets using three standard face recognition models. Our findings empirically validate the notion that the introduction of data from the demographic group with the initially-lowest performance improves performance on that group significantly more than adding from other groups. We also observe that in all settings, the introduction of data from a previously omitted group does not harm the performance of other groups. Furthermore, investigation of feature embeddings reveals that performance increases are associated with a larger separation among images of different identities. Despite the commonalities we observe across datasets, we also find key differences: for example, in one dataset, training on one racial group generalizes well across all groups. These differences speak to the criticality of re-applying empirical evaluation methods, such as the methods in this work, when introducing new datasets or models.",Algorithmic audit; data collection; face recognition; racial bias in computer vision,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,531–541,AIES '23,Association for Computing Machinery,9BEBG6U3,0.0185185185185185,0.0909090909090909,0.0,0.0212831957018003
378,378,Discovering and Understanding Algorithmic Biases in Autonomous Pedestrian Trajectory Predictions,Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems,,,10.1145/3560905.3568433,"Bae, Andrew; Xu, Susu",2023.0,https://doi.org/10.1145/3560905.3568433,conferencePaper,"Pedestrian trajectory prediction is an important module in autonomous vehicles (AVs) to ensure safe and effective motion planning. Recently, many deep learning algorithms that achieve near real-time trajectory predictions have been developed. However, people in the artificial intelligence (AI) ethics community have raised critical concerns about the bias and fairness of many general deep learning algorithms. For example, most pedestrian trajectory data is collected from majority populations, and models learned from this data may not generalize well to the heterogeneous needs and behavior patterns of different pedestrian groups, especially for vulnerable pedestrians like the disabled, the elderly, and children. Biases present in trajectory prediction algorithms could mean that pedestrians from certain vulnerable demographics are more likely to be involved in vehicle crashes. In this work, we test two state-of-the-art pedestrian trajectory prediction models for age and gender biases across three different datasets. We design and utilize novel evaluation metrics for comparing model performance. We find that both models perform worse on children and the elderly compared to adults. However, their performance is similar between men and women. We identify potential sources of these biases, as well as discuss several limitations of our study. Our future work will consist of testing more models, refining our evaluation metrics, further differentiating the dataset bias from the algorithmic bias, and mitigating the algorithmic biases.",fairness; bias; algorithm evaluation; trajectory prediction,978-1-4503-9886-2,,2023,2023-11-06 01:30:04,2023-11-06 01:30:04,1155–1161,SenSys '22,Association for Computing Machinery,IHQ73XDH,0.0227272727272727,0.0,0.0,0.0206599445155783
379,379,Identifying Bias in Data Using Two-Distribution Hypothesis Tests,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534169,"Yik, William; Serafini, Limnanthes; Lindsey, Timothy; Montañez, George D.",2022.0,https://doi.org/10.1145/3514094.3534169,conferencePaper,"As machine learning models become more widely used in important decision-making processes, the need for identifying and mitigating potential sources of bias has increased substantially. Using two-distribution (specified complexity) hypothesis tests, we identify biases in training data with respect to proposed distributions and without the need to train a model, distinguishing our methods from common output-based fairness tests. Furthermore, our methods allow us to return a ""closest plausible explanation"" for a given dataset, potentially revealing underlying biases in the processes that generated them. We also show that a binomial variation of this hypothesis test could be used to identify bias in certain directions, or towards certain outcomes, and again return a closest plausible explanation. The benefits of this binomial variation are compared with other hypothesis tests, including the exact binomial. Lastly, potential industrial applications of our methods are shown using two real-world datasets.",fairness; bias; machine learning; data analysis; hypothesis testing; statistics,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,831–844,AIES '22,Association for Computing Machinery,HH55UCA7,0.0069930069930069,0.1111111111111111,0.125,0.020629807146661
380,380,Blind Justice: Algorithmically Masking Race in Charging Decisions,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462524,"Chohlas-Wood, Alex; Nudell, Joe; Yao, Keniel; Lin, Zhiyuan (Jerry); Nyarko, Julian; Goel, Sharad",2021.0,https://doi.org/10.1145/3461702.3462524,conferencePaper,"A prosecutor's decision to charge or dismiss a criminal case is a particularly high-stakes choice. There is concern, however, that these judgements may suffer from explicit or implicit racial bias, as with many other such actions in the criminal justice system. To reduce potential bias in charging decisions, we designed a system that algorithmically redacts race-related information from free-text case narratives. In a first-of-its-kind initiative, we deployed this system at a large American district attorney's office to help prosecutors make race-obscured charging decisions, where it was used to review many incoming felony cases. We report on both the design, efficacy, and impact of our tool for aiding equitable decision-making. We demonstrate that our redaction algorithm is able to accurately obscure race-related information, making it difficult for a human reviewer to guess the race of a suspect while preserving other information from the case narrative. In the jurisdiction we study, we found little evidence of disparate treatment in charging decisions even prior to deployment of our intervention. Thus, as expected, our tool did not substantially alter charging rates. Nevertheless, our study demonstrates the feasibility of race-obscured charging, and more generally highlights the promise of algorithms to bolster equitable decision-making in the criminal justice system.",equity; natural language processing; criminal justice; prosecution,978-1-4503-8473-5,,2021,2023-11-06 01:29:58,2023-11-06 01:29:58,35–45,AIES '21,Association for Computing Machinery,V883KNYC,0.0098522167487684,0.1428571428571428,0.125,0.020523431338324
381,381,A Survey on Bias in Visual Datasets,Comput. Vis. Image Underst.,223.0,C,10.1016/j.cviu.2022.103552,"Fabbrizzi, Simone; Papadopoulos, Symeon; Ntoutsi, Eirini; Kompatsiaris, Ioannis",2022.0,https://doi.org/10.1016/j.cviu.2022.103552,journalArticle,"Computer Vision (CV) has achieved remarkable results, outperforming humans in several tasks. Nonetheless, it may result in significant discrimination if not handled properly. Indeed, CV systems highly depend on training datasets and can learn and amplify biases that such datasets may carry. Thus, the problem of understanding and discovering bias in visual datasets is of utmost importance; yet, it has not been studied in a systematic way to date. Hence, this work aims to: (i) describe the different kinds of bias that may manifest in visual datasets; (ii) review the literature on methods for bias discovery and quantification in visual datasets; (iii) discuss existing attempts to collect visual datasets in a bias-aware manner. A key conclusion of our study is that the problem of bias discovery and quantification in visual datasets is still open, and there is room for improvement in terms of both methods and the range of biases that can be addressed. Moreover, there is no such thing as a bias-free dataset, so scientists and practitioners must become aware of the biases in their datasets and make them explicit. To this end, we propose a checklist to spot different types of bias during visual dataset collection.",AI ethics; 41A05; 41A10; 65D05; 65D17; Bias; Computer vision; Visual datasets,,1077-3142,2022-10,2023-11-06 01:30:01,2023-11-06 01:30:01,,,,F452CIVY,0.005050505050505,0.2727272727272727,0.0,0.0203342323761171
382,382,A Bio-Inspired Framework for Machine Bias Interpretation,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534126,"Robertson, Jake; Stinson, Catherine; Hu, Ting",2022.0,https://doi.org/10.1145/3514094.3534126,conferencePaper,"Machine learning algorithms use the past and the present to predict the future. But when given biased historical data, these algorithms can quickly become discriminatory. The area of machine learning fairness has emerged to detect and de-bias these algorithms, but has received widespread criticism for its one-size-fits-all approach, which allows certain cases of bias to slip through the cracks. In this study, we take a deeper look at the mechanisms by which machine learning algorithms develop harmful bias. We introduce a new method to interpret discriminatory systems, an Evolutionary algorithm for Feature Interaction (EFI), which we apply to several commonly used machine learning algorithms in two real-world problem instances: violent crime and median house price prediction. In the results, we discover several complex forms of bias including the encoding of race through other seemingly unrelated attributes. Ultimately we suggest that more informative interpretation tools such as EFI can be used to not only explain machine learning outcomes, but supplement and improve existing machine bias detection approaches to provide a more robust and in-depth ethical evaluation of machine learning algorithms.",fairness; interpretability; feature importance; feature interaction; machine bias,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,588–598,AIES '22,Association for Computing Machinery,IGFK4UZX,0.0223463687150838,0.0,0.0,0.0200819807609665
384,384,Towards Accountability in the Use of Artificial Intelligence for Public Administrations,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462631,"Loi, Michele; Spielkamp, Matthias",2021.0,https://doi.org/10.1145/3461702.3462631,conferencePaper,"We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.",artificial intelligence; accountability; AI guidelines; public administrations,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,757–766,AIES '21,Association for Computing Machinery,QSMUV8I9,0.0127388535031847,0.1428571428571428,0.0,0.0198689373476891
385,385,Robots That Need to Mislead: Biologically-Inspired Machine Deception,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539571,"Arkin, Ronald",2022.0,https://doi.org/10.1145/3514094.3539571,conferencePaper,"Expanding our work in understanding the relationships maintained in teams of humans and robots, this talk describes research on deception and its application within robotic systems. Earlier we explored the use of psychology as the basis for producing deceit in robotic systems in order to evade capture. More recent work involves studying squirrel hoarding and bird mobbing behavior as it applies to deception, in the first case for misleading a predator, and in the second for feigning strength when none exists. Next, we discuss other-deception, where deceit is performed for the benefit of the mark. Finally, newly completed research on team deception where groups of agents using shills that serve to mislead others is presented. Results are presented in both simulation and simple robotic systems, as well as consideration of the ethical implications of this research.",human-robot interaction; robot deception,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,1,AIES '22,Association for Computing Machinery,8SQRYX32,0.0220588235294117,0.0,0.0,0.0196303291958985
388,388,Machine Learning Practices Outside Big Tech: How Resource Constraints Challenge Responsible Development,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462527,"Hopkins, Aspen; Booth, Serena",2021.0,https://doi.org/10.1145/3461702.3462527,conferencePaper,"Practitioners from diverse occupations and backgrounds are increasingly using machine learning (ML) methods. Nonetheless, studies on ML Practitioners typically draw populations from Big Tech and academia, as researchers have easier access to these communities. Through this selection bias, past research often excludes the broader, lesser-resourced ML community—for example, practitioners working at startups, at non-tech companies, and in the public sector. These practitioners share many of the same ML development difficulties and ethical conundrums as their Big Tech counterparts; however, their experiences are subject to additional under-studied challenges stemming from deploying ML with limited resources, increased existential risk, and absent access to in-house research teams. We contribute a qualitative analysis of 17 interviews with stakeholders from organizations which are less represented in prior studies. We uncover a number of tensions which are introduced or exacerbated by these organizations' resource constraints—tensions between privacy and ubiquity, resource management and performance optimization, and access and monopolization. Increased academic focus on these practitioners can facilitate a more holistic understanding of ML limitations, and so is useful for prescribing a research agenda to facilitate responsible ML development for all.",big tech; contextual inquiry; machine learning practice; ML developers,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,134–145,AIES '21,Association for Computing Machinery,V2V429NG,0.0218579234972677,0.0,0.0,0.0194056402514653
389,389,How to Design Smart Homes for Older Persons: The Somatic Capability Assessment as an Answer to Current Design Challenges,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539514,"Felber, Nadine Andrea",2022.0,https://doi.org/10.1145/3514094.3539514,conferencePaper,"Older persons have had decades to get to know their environment - and usually their home - through their own somatic experience. This somatic or ""felt"" experience enables them to assess their own capabilities when it comes to executing activities of daily living [1]. This is what we call ""somatic capability assessment"" or SCA in short, inspired by the theory of somaaesthetics [2]. The home offers the ideal background for an older person's - or any person's - SCA, as it feels familiar, stable, and private [3,4,5]. Smart home technology (SHT) while having the potential to support older persons to age in place [6], is nevertheless bound to disrupt these qualities of the former, non-smart home, and therefore also alter the SCA of an older person. This is because the monitoring sensors in the smart home add an external stream of data is now to the somatic, embodied experience of the person. When the assessment of the smart home disagrees with the SCA of the older resident, this can create serious challenges in daily living. However, putting the SCA at the center of designing monitoring capacities in a smart home for older persons may also be the solution of these challenges.In this paper, we first present how we developed the concept of SCA, then show why the home with its characteristics of familiarity, stability and privacy is important for the SCA. In a next step, we explain how the smart home potentially alters these perceived features of the former non-smart home, creating disruptive experiences for the resident, which may end in the older person rejecting the smart home altogether, thus undermining the whole justification of existence of the smart home for older persons. To counteract this possibility, we propose somatic design measures as well as a framework for the designing process, to adequately support and possibly enhance the SCA of older persons living in smart homes.This paper is part of a larger PhD-thesis which investigates the social and ethical boundaries of the use of smart home technologies for older persons, their professional caregivers, and their family members.",ethics; design; aging in place; home; monitoring; older persons; smart home; somatic,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,900,AIES '22,Association for Computing Machinery,Z4FT47PS,0.0144092219020172,0.1666666666666666,0.0,0.0191766868727019
390,390,Stress-Testing Bias Mitigation Algorithms to Understand Fairness Vulnerabilities,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604713,"Bhanot, Karan; Baldini, Ioana; Wei, Dennis; Zeng, Jiaming; Bennett, Kristin",2023.0,https://doi.org/10.1145/3600211.3604713,conferencePaper,"To address the growing concern of unfairness in Artificial Intelligence (AI), several bias mitigation algorithms have been introduced in prior research. Their capabilities are often evaluated on certain overly-used datasets without rigorously stress-testing them under simultaneous train and test distribution shifts. To address this, we investigate the fairness vulnerabilities of these algorithms across several distribution shift scenarios using synthetic data, to highlight scenarios where these algorithms do and don’t work to encourage their trustworthy use. The paper makes three important contributions. Firstly, we propose a flexible pipeline called the Fairness Auditor to systematically stress-test bias mitigation algorithms using multiple synthetic datasets with shifts. Secondly, we introduce the Deviation Metric for measuring the fairness and utility performance of these algorithms under such shifts. Thirdly, we propose an interactive reporting tool for comparing algorithmic performance across various synthetic datasets, mitigation algorithms and metrics called the Fairness Report.",fairness; bias; auditing; distribution shift; synthetic data,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,764–774,AIES '23,Association for Computing Machinery,89U5RDHM,0.0137931034482758,0.1428571428571428,0.0,0.0191396365309408
391,391,"Can AlphaGo Be Apt Subjects for Praise/Blame for ""Move 37""?","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604730,"Hussain, Mubarak",2023.0,https://doi.org/10.1145/3600211.3604730,conferencePaper,"This paper examines whether machines (algorithms/programs/ AI systems) are apt subjects for praise or blame for some actions or performances. I consider ""Move 37"" of AlphaGo as a case study. DeepMind’s AlphaGo is an AI algorithm developed to play the game of Go. The AlphaGo utilizes Deep Neural Networks. As AlphaGo is trained through reinforcement learning, the AI algorithm can improve itself over a period of time. Such AI models can go beyond the intended task and perform novel and unpredictable functions. There is a surprise element associated with ""Move 37"". ""Move 37"" not only surprises the Go players, the programmers, but also whoever is informed of this unpredicted move. Does someone or something deserve praise or blame for the surprise? If so, who or what deserves the praise or blame for ""Move 37""? The programmer cannot be praised for ""Move 37"", which is either surprising or was not intended or imagined at all. At the same time, would we accept that neither the algorithm deserves praise for the unpredicted move that the algorithm allowed the program to make? From this, would we accept that since neither the programmer nor the algorithm/AI system deserves the praise, there is such a good or exciting move for which no one or nothing could be praised? Would we say this unpredictable move is a move for which no one deserves praise or blame? Wouldn’t there be at least a few who were surprised by the unpredictable move? Should we say that for this pleasant surprise, no one deserves praise? Nonetheless, for us, specifically regarding the particular unpredictable move, we firmly find it counterintuitive to say that there is an exciting move for which no one deserves praise. The surprise element is the result of the property that belongs to the algorithm. It seems quite difficult for us to accept that no one deserves praise for ""Move 37"" or for similar moves. Therefore, someone or something deserves praise which is a matter of scrutiny.","""Move 37""; AlphaGo; Artificial Moral Agency(AMA); Blame and Praise; Causal Responsibility; Machine morality; Moral Responsibility",9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,977–979,AIES '23,Association for Computing Machinery,U7796JP2,0.0121212121212121,0.1333333333333333,0.0,0.019045208465162
392,392,From Algorithmic Audits to Actual Accountability: Overcoming Practical Roadblocks on the Path to Meaningful Audit Interventions for AI Governance,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539566,"Raji, Inioluwa Deborah",2022.0,https://doi.org/10.1145/3514094.3539566,conferencePaper,"As algorithmic deployments become more and more common, policymakers and advocates are increasingly turning to audits as an approach for accountability. Some audits have already led to product updates or recalls, organizational changes and developments to regulation or standards. However, difficulties in execution, oversight and impact threaten the credibility and effectiveness of these audits.In order to further mature the AI policy ecosystem, we must approach audit interventions with care. In this talk, I discuss several of the remaining roadblocks on the path to making audits a meaningful intervention for algorithmic accountability. I highlight the particular challenges of implementing audits in the algorithmic context, while also noting important lessons from audit systems in other industries. I conclude with recommendations for clearing the path towards more meaningful and mature audit accountability measures for AI governance.",accountability; governance; algorithm; auditing; deployment,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,5,AIES '22,Association for Computing Machinery,932RX9XB,0.0150375939849624,0.0,0.0526315789473684,0.0190062111801242
393,393,Quantum Fair Machine Learning,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462611,"Perrier, Elija",2021.0,https://doi.org/10.1145/3461702.3462611,conferencePaper,"In this paper, we inaugurate the field of quantum fair machine learning. We undertake a comparative analysis of differences and similarities between classical and quantum fair machine learning algorithms, specifying how the unique features of quantum computation alter measures, metrics and remediation strategies when quantum algorithms are subject to fairness constraints. We present the first results in quantum fair machine learning by demonstrating the use of Grover's search algorithm to satisfy statistical parity constraints imposed on quantum algorithms. We provide lower-bounds on iterations needed to achieve such statistical parity within ε-tolerance. We extend canonical Lipschitz-conditioned individual fairness criteria to the quantum setting using quantum metrics. We examine the consequences for typical measures of fairness in machine learning context when quantum information processing and quantum data are involved. Finally, we propose open questions and research programmes for this new field of interest to researchers in computer science, ethics and quantum computation.",fair; learning; machine; quantum,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,843–853,AIES '21,Association for Computing Machinery,NKZ4Z2V3,0.02,0.0,0.0,0.0189437229437229
394,394,A Multidomain Relational Framework to Guide Institutional AI Research and Adoption,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604718,"Straub, Vincent J; Morgan, Deborah; Hashem, Youmna; Francis, John; Esnaashari, Saba; Bright, Jonathan",2023.0,https://doi.org/10.1145/3600211.3604718,conferencePaper,"Calls for new metrics, technical standards and governance mechanisms to guide the adoption of Artificial Intelligence (AI) in institutions and public administration are now commonplace. Yet, most research and policy efforts aimed at understanding the implications of adopting AI tend to prioritize only a handful of ideas; they do not fully connect all the different perspectives and topics that are potentially relevant. In this position paper, we contend that this omission stems, in part, from what we call the ‘relational problem’ in socio-technical discourse: fundamental ontological issues have not yet been settled—including semantic ambiguity, a lack of clear relations between concepts and differing standard terminologies. This contributes to the persistence of disparate modes of reasoning to assess institutional AI systems, and the prevalence of conceptual isolation in the fields that study them including ML, human factors, social science and policy. After developing this critique, we offer a way forward by proposing a simple policy and research design tool in the form of a conceptual framework to organize terms across fields—consisting of three horizontal domains for grouping relevant concepts and related methods: Operational, Epistemic, and Normative. We first situate this framework against the backdrop of recent socio-technical discourse at two premier academic venues, AIES and FAccT, before illustrating how developing suitable metrics, standards, and mechanisms can be aided by operationalizing relevant concepts in each of these domains. Finally, we outline outstanding questions for developing this relational approach to institutional AI research and adoption.",AIES; FAccT; Conceptual framework; Institutions; Public Administration; Socio-Technical Discourse,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,512–519,AIES '23,Association for Computing Machinery,7P2Q2SXW,0.0165289256198347,0.0,0.0909090909090909,0.018931847997491
395,395,The Mechanical Psychologist: Leveraging AI for Detecting Predatory Behaviour in Online Interactions,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604734,"Cook, Darren",2023.0,https://doi.org/10.1145/3600211.3604734,conferencePaper,"﻿  Social science fields such as psychology traditionally rely on manual, qualitative coding for behavioural observations [4]. This process involves manual, laborious, time-consuming, and error-prone ef- forts, presenting a significant challenge to the scalability of social scientific research in fast-paced settings [1]. The integration of Natural Language Processing (NLP) and Ma- chine Learning (ML) in the social sciences offers an opportunity to overcome these limitations. This thesis applies these techniques to automatically detect predatory behaviour in online interactions, a growing concern with societal implications. The specific questions I ask throughout this work are as follows: (1) How can computational techniques be used to overcome the limitations of expert labelling? (2) Do machines perform comparably with humans? (3) Can an automated solution explore theories of social be- haviour at scale? In the remainder of this extended abstract, I outline the work cov- ered in my doctoral studies. Section 2 examines the domain problem. Section 3 describes the computational methods used, and Section 4 presents the main findings. Section 5 describes the thesis contri- butions, and Section 6 outlines this work's ethical and societal implications. Finally, Section 7 reports future work in this area.",Machine Learning; Natural Language Processing; Automated Behaviour Coding; Computational Social Science; Online Grooming,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,1010–1012,AIES '23,Association for Computing Machinery,X3WKRBGF,0.0155440414507772,0.0,0.0833333333333333,0.0186968423110763
396,396,Changing Distributions and Preferences in Learning Systems,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3607543,"Morgenstern, Jamie",2023.0,https://doi.org/10.1145/3600211.3607543,conferencePaper,"In this talk, I’ll describe some recent work outlining how distribution shifts are fundamental to working with human-centric data. Some of these shifts come from attempting to ""join"" datasets gathered in different contexts, others may be the result of people’s preferences affecting which data they provide to which systems, and even more can arise when peoples’ preferences themselves are shaped by ML systems’ recommendations. Each of these types of shift require different modeling and analysis to more accurately predict the behavior of ML pipelines deployed in a way where they interact repeatedly with people who care about their predictions.",,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,2,AIES '23,Association for Computing Machinery,DEKSG5XW,0.0202020202020202,0.0,0.0,0.0185423641514293
397,397,Algorithmic Bias: When Stigmatization Becomes a Perception: The Stigmatized Become Endangered,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604723,"Akintande, Olalekan Joseph",2023.0,https://doi.org/10.1145/3600211.3604723,conferencePaper,"In this study, the author examines how perceived stigmatization endangered the stigmatized groups within a society or community. Thus, he goes back in history to dig deep into the sources of perceived stigmatization associated with the black race and how perceived stigmatization has emigrated into AI tools and machine outputs - subjecting vulnerable communities to hypervisibility by exposing them to systems of racial surveillance. To justify the study goal, he conducted a summarized text analysis on racial stigmatization using Twitter hashtags ∈ black people, blackness, Africa, African-Americans, all coined out of the Twitter Users’ perception of the subject and hypothesized to find high negative sentiment correlation of stigmatization perspective in association with black race and Africa. He finds that Black people are associated with Africa and have a strong negative sentiment correlation with - poorness, crime, death, abuses (stupid), among others, and a subject of racist scum and racism. Similarly, there is a weak negative sentiment correlation with being - bad, abused (such bitch), hate, violence, and protest. He also finds similar strong and weak negative sentiment correlations with other hashtags. He discusses the danger of racial stigmatization and proposes a cycle of ethical algorithmic development &amp; deployment and recommendations.",Endangered; NLP; Stigmatized; Twitter,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,966–971,AIES '23,Association for Computing Machinery,R4UJ86XU,0.02,0.0,0.0,0.0182620320855614
399,399,Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462525,"Sheng, Hao; Yao, Keniel; Goel, Sharad",2021.0,https://doi.org/10.1145/3461702.3462525,conferencePaper,"The use of video surveillance in public spaces–both by government agencies and by private citizens–has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here we present a novel approach for estimating the spatial distribution of surveillance cameras: applying computer vision algorithms to large-scale street view image data. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents—a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.",computer vision; privacy; surveillance; urban computing,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,221–230,AIES '21,Association for Computing Machinery,CTB8LPJK,0.0084745762711864,0.1666666666666666,0.0833333333333333,0.0177872159484207
400,400,Counterfactual Explanations for Prediction and Diagnosis in XAI,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534144,"Dai, Xinyue; Keane, Mark T.; Shalloo, Laurence; Ruelle, Elodie; Byrne, Ruth M.J.",2022.0,https://doi.org/10.1145/3514094.3534144,conferencePaper,"We compared two sorts of explanations for decisions made by an AI system: counterfactual explanations about how an outcome could have been different in the past, and prefactual explanations about how it could be different in the future. We examined the effects of these alternative explanation strategies on the accuracy of users' judgments about the AI app's predictions about an outcome (inferred from information about the causes), compared to the accuracy of their judgments about the app's diagnoses of a cause (inferred from information about the outcome). The tasks were based on a simulated SmartAgriculture decision support system for grass growth outcomes on dairy farms in Experiment 1, and for an analogous alien planet domain in Experiment 2. The two experiments, with 243 participants, also tested users' confidence in their decisions, and their satisfaction with the explanations. Users made more accurate diagnoses of the presence of causes based on information about their outcome, compared to predictions of an outcome given information about the presence of causes. Their predictions and diagnoses were helped equally by counterfactual explanations and prefactual ones.",prediction; explanation; counterfactual; diagnosis; explainable ai (xai); smart agriculture,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,215–226,AIES '22,Association for Computing Machinery,T8VBXQHG,0.0111731843575419,0.1111111111111111,0.0,0.0174253756541511
401,401,A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534130,"Yew, Rui-Jie; Hadfield-Menell, Dylan",2022.0,https://doi.org/10.1145/3514094.3534130,conferencePaper,"As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems.",artificial intelligence law; computing and society; penalty defaults; technology policy,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,823–830,AIES '22,Association for Computing Machinery,FKEEMF9U,0.0145631067961165,0.0,0.0769230769230769,0.0172542810008842
402,402,Human Uncertainty in Concept-Based AI Systems,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604692,"Collins, Katherine Maeve; Barker, Matthew; Espinosa Zarlenga, Mateo; Raman, Naveen; Bhatt, Umang; Jamnik, Mateja; Sucholutsky, Ilia; Weller, Adrian; Dvijotham, Krishnamurthy",2023.0,https://doi.org/10.1145/3600211.3604692,conferencePaper,"Placing a human in the loop may help abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, densely-annotated soft labels from humans. We show that training with uncertain concept labels may help mitigate weaknesses of concept-based systems when handling uncertain interventions. These results allow us to identify several open challenges, which we argue can be tackled through future multidisciplinary research on building interactive uncertainty-aware systems. To facilitate further research, we release a new elicitation platform, UElic, to collect uncertain feedback from humans in collaborative prediction tasks.",uncertainty; concept learning; human-in-the-loop; interactive; XAI,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,869–889,AIES '23,Association for Computing Machinery,WGEETYED,0.0135746606334841,0.0,0.1666666666666666,0.0172290586028032
403,403,Mitigating Voter Attribute Bias for Fair Opinion Aggregation,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604660,"Ueda, Ryosuke; Takeuchi, Koh; Kashima, Hisashi",2023.0,https://doi.org/10.1145/3600211.3604660,conferencePaper,"The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results. To this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&amp;S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&amp;S model. To address these limitations, we propose a new Soft D&amp;S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&amp;S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.",fairness; decision-making; crowdsourcing; human computation; opinion aggregation,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,170–180,AIES '23,Association for Computing Machinery,CLBQRZ68,0.0180505415162454,0.0,0.0,0.0168019862730269
404,404,How Cognitive Biases Affect XAI-Assisted Decision-Making: A Systematic Review,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534164,"Bertrand, Astrid; Belloum, Rafik; Eagan, James R.; Maxwell, Winston",2022.0,https://doi.org/10.1145/3514094.3534164,conferencePaper,"The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes.",explainability; explainable ai; cognitive bias; human-centered ai; xai.,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,78–91,AIES '22,Association for Computing Machinery,3PS3Y23R,0.005,0.25,0.0,0.0165322033898305
405,405,AI2: The next Leap toward Native Language-Based and Explainable Machine Learning Framework,Automated Software Engg.,30.0,2,10.1007/s10515-023-00399-5,"Dessureault, Jean-Sébastien; Massicotte, Daniel",2023.0,https://doi.org/10.1007/s10515-023-00399-5,journalArticle,"The machine learning frameworks flourished in the last decades, allowing artificial intelligence to get out of academic circles to be applied to enterprise domains. This field has significantly advanced, but there is still some meaningful improvement to reach the subsequent expectations. The proposed framework, named AI2, uses a natural language interface that allows non-specialists to benefit from machine learning algorithms without necessarily knowing how to program with a programming language. The primary contribution of the AI2 framework allows a user to call the machine learning algorithms in English, making its interface usage easier. The second contribution is greenhouse gas (GHG) awareness. It has some strategies to evaluate the GHG generated by the algorithm to be called and to propose alternatives to find a solution without executing the energy-intensive algorithm. Another contribution is a preprocessing module that helps to describe and to load data properly. Using an English text-based chatbot, this module guides the user to define every dataset so that it can be described, normalized, loaded, and divided appropriately. The last contribution of this paper is about explainability. The scientific community has known that machine learning algorithms imply the famous black-box problem for decades. Traditional machine learning methods convert an input into an output without being able to justify this result. The proposed framework explains the algorithm’s process with the proper texts, graphics, and tables. The results, declined in five cases, present usage applications from the user’s English command to the explained output. Ultimately, the AI2 framework represents the next leap toward native language-based, human-oriented concerns about machine learning framework.",AI ethics; Explainability; Machine learning; NLP; Framework,,0928-8910,2023-09,2023-11-06 01:30:00,2023-11-06 01:30:00,,,,E92QFVGZ,0.0038461538461538,0.4285714285714285,0.0,0.0165063164294721
406,406,Advancing Health Equity with Machine Learning,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604753,"Mhasawade, Vishwali",2023.0,https://doi.org/10.1145/3600211.3604753,conferencePaper,"Social privilege in terms of power, wealth, and prestige is the driver of avoidable health inequities. But today, machine learning systems in healthcare are largely focused on data and systems within hospitals and clinics, ignoring the factors that lead to health disparities across communities. The primary goal of my research is to understand the drivers of population health inequity and design fair and equitable machine learning systems for mitigating health disparities. In order to do this, I mainly focus on causal inference and machine learning methods using data from multiple environments, such as geographical locations and hospitals, to identify and address inequities in health and healthcare.",fairness; health disparities; health equity; causal inference,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,955–956,AIES '23,Association for Computing Machinery,2XW4RAH3,0.0188679245283018,0.0,0.0,0.0164078999280558
407,407,Computational Philosophy,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539562,"Etienne, Hubert",2022.0,https://doi.org/10.1145/3514094.3539562,conferencePaper,"The critical value of interdisciplinarity is increasingly accepted not only for promoting the responsible use of machine learning models, but also for increasing their performance. Social scientists can then help engineers better understand the population they are gathering data from. To be successful, however, interdisciplinary collaborations require more than just gathering researchers from various fields around a table. They call for addressing challenges such as developing a common language, understanding different ways of reasoning, and addressing epistemic controversies to agree on shared criteria upon which can be assessed the validity of the co-produced knowledge. Controversies bring about relevant epistemic questions to promote an ongoing reflection on scientific methods. In contrast, when a discipline leverages its own methods to approach a topic traditionally associated with another discipline, the unsolicited interference is often followed by a backlash which undermines the possibility of collaboration (e.g., [1], [2], [3]).New environments emerge in academic centers willing to welcome interdisciplinary research and a key challenge is creating practices and methodologies that could enable such research. Computational philosophy is the approach I develop to serve this goal. I present here two examples of such collaborations I have led, as a philosopher, alongside machine learning engineers. These collaborations resulted in great outcomes and significantly advanced the state of the art in the domain of online social interactions.The first example is an empirical study on misinformation based on an analysis of user-generated reports from Facebook and Instagram [4]. The mixed approach I developed with Onur Çelebi allowed us to identify meaningful variations in the volumes and types of false news, as well as in the manipulative strategies developed to spread misinformation among countries and platforms. Thanks to an original typology we created to classify content, we were able to identify four distinct types of behaviors for users reporting content to moderators. This allowed us to propose explanations for up to 55% of the inaccuracy in user reports, suggest solutions to improve the overall signal by taking action on the different sources of inaccuracy, and build a classifier capable of distinguishing credible user reports from others to support misinformation detection.The second example is an empirical study of social influencers on Instagram [5]. François Charton and I developed an original typology to classify Instagram influencers based on their source of legitimacy. This allowed us to identify different kinds of audiences characteristic of the different types of influencers, which we analyzed through René Girard's mimetic desire theory [6]. This research enriched Girard's philosophical theory while it helped us advance the understanding of social interactions between influencers and their followers by superimposing a communication system inspired by Marshall McLuhan's media theory onto them [7]. Leveraging different signals, we were then able to identify for each category of influencer which kinds of posts were most likely to generate a positive response and negative feedback.I am now expanding my approach to hate speech, bringing together the psychological mechanisms of hate conceptualized by Girard [8] to better understand the manifestations of hate on Instagram. These three blocks should then allow me to present a holistic approach to online interactions upon which an ethical system for the moderation of problematic online interactions could be erected.",AI ethics; behavioral sciences; content moderation; misinformation; philosophy of AI,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,899,AIES '22,Association for Computing Machinery,S6HCNDH8,0.0075757575757575,0.4,0.0,0.016331585081585
408,408,Equalizing Credit Opportunity in Algorithms: Aligning Algorithmic Fairness Research with U.S. Fair Lending Regulation,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534154,"Kumar, I. Elizabeth; Hines, Keegan E.; Dickerson, John P.",2022.0,https://doi.org/10.1145/3514094.3534154,conferencePaper,"Credit is an essential component of financial wellbeing in America, and unequal access to it is a large factor in the economic disparities between demographic groups that exist today. Today, machine learning algorithms, sometimes trained on alternative data, are increasingly being used to determine access to credit, yet research has shown that machine learning can encode many different versions of ""unfairness,"" thus raising the concern that banks and other financial institutions could—potentially unwittingly—engage in illegal discrimination through the use of this technology. In the US, there are laws in place to make sure discrimination does not happen in lending and agencies charged with enforcing them. However, conversations around fair credit models in computer science and in policy are often misaligned: fair machine learning research often lacks legal and practical considerations specific to existing fair lending policy, and regulators have yet to issue new guidance on how, if at all, credit risk models should be utilizing practices and techniques from the research community. This paper aims to better align these sides of the conversation. We describe the current state of credit discrimination regulation in the United States, contextualize results from fair ML research to identify the specific fairness concerns raised by the use of machine learning in lending, and discuss regulatory opportunities to address these concerns.",machine learning; algorithmic fairness; fair machine learning; ecoa; fair lending law,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,357–368,AIES '22,Association for Computing Machinery,397REHMH,0.0186046511627906,0.0,0.0,0.0163311245234176
409,409,Accounting for Model Uncertainty in Algorithmic Discrimination,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462630,"Ali, Junaid; Lahoti, Preethi; Gummadi, Krishna P.",2021.0,https://doi.org/10.1145/3461702.3462630,conferencePaper,"Traditional approaches to ensure group fairness in algorithmic decision making aim to equalize ""total"" error rates for different subgroups in the population. In contrast, we argue that the fairness approaches should instead focus only on equalizing errors arising due to model uncertainty (a.k.a epistemic uncertainty), caused due to lack of knowledge about the best model or due to lack of data. In other words, our proposal calls for ignoring the errors that occur due to uncertainty inherent in the data, i.e., aleatoric uncertainty. We draw a connection between predictive multiplicity and model uncertainty and argue that the techniques from predictive multiplicity could be used to identify errors made due to model uncertainty. We propose scalable convex proxies to come up with classifiers that exhibit predictive multiplicity and empirically show that our methods are comparable in performance and up to four orders of magnitude faster than the current state-of-the-art. We further pro- pose methods to achieve our goal of equalizing group error rates arising due to model uncertainty in algorithmic decision making and demonstrate the effectiveness of these methods using synthetic and real-world datasets",classification; algorithmic fairness; model uncertainty; predictive multiplicity,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,336–345,AIES '21,Association for Computing Machinery,GGIFTPW4,0.0109289617486338,0.0,0.1428571428571428,0.0163123296844016
410,410,Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks,"Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3593013.3594109,"Mei, Katelyn; Fereidooni, Sonia; Caliskan, Aylin",2023.0,https://doi.org/10.1145/3593013.3594109,conferencePaper,"Warning: The content of this paper may be upsetting or triggering.The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.",AI ethics; language models; AI bias; representation learning; prompting; sentiment classification; stigma in language models,9798400701924,,2023,2023-11-06 01:30:02,2023-11-06 01:30:02,1699–1710,FAccT '23,Association for Computing Machinery,JXBR8RXN,0.0031152647975077,0.2666666666666666,0.0,0.016212493851451
413,413,Causality in Neural Networks - An Extended Abstract,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462467,"Gowtham Reddy, Abbavaram",2021.0,https://doi.org/10.1145/3461702.3462467,conferencePaper,"Causal reasoning is the main learning and explanation tool used by humans. AI systems should possess causal reasoning capabilities to be deployed in the real world with trust and reliability. Introducing the ideas of causality to machine learning helps in providing better learning and explainable models. Explainability, causal disentanglement are some important aspects of any machine learning model. Causal explanations are required to believe in a model's decision and causal disentanglement learning is important for transfer learning applications. We exploit the ideas of causality to be used in deep learning models to achieve better and causally explainable models that are useful in fairness, disentangled representation, etc.",machine learning; explainability; causality; neural networks; counterfactuals; disentanglement,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,271–272,AIES '21,Association for Computing Machinery,I84VYHW3,0.0188679245283018,0.0,0.0,0.0157590051457976
415,415,Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604682,"Dennler, Nathan; Ovalle, Anaelia; Singh, Ashwin; Soldaini, Luca; Subramonian, Arjun; Tu, Huy; Agnew, William; Ghosh, Avijit; Yee, Kyra; Peradejordi, Irene Font; Talat, Zeerak; Russo, Mayra; Pinhal, Jess De Jesus De Pinho",2023.0,https://doi.org/10.1145/3600211.3604682,conferencePaper,"Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",artificial intelligence; bias bounties; harms; LGBTQIA+; participatory methods,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,375–386,AIES '23,Association for Computing Machinery,EGX9EIUM,0.0118343195266272,0.0,0.0833333333333333,0.0155662139735335
416,416,Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534153,"Yang, Yu; Gupta, Aayush; Feng, Jianwei; Singhal, Prateek; Yadav, Vivek; Wu, Yue; Natarajan, Pradeep; Hedau, Varsha; Joo, Jungseock",2022.0,https://doi.org/10.1145/3514094.3534153,conferencePaper,"Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",bias measurement and mitigation; face detection bias; fairness in computer vision,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,813–822,AIES '22,Association for Computing Machinery,EMY32QI2,0.0171673819742489,0.0,0.0,0.015519388641559
417,417,On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462538,"Fogliato, Riccardo; Xiang, Alice; Lipton, Zachary; Nagin, Daniel; Chouldechova, Alexandra",2021.0,https://doi.org/10.1145/3461702.3462538,conferencePaper,"Re-offense risk is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision-makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate and less biased reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007–2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.",crime; nibrs; racial disparity; risk assessment instrument,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,100–111,AIES '21,Association for Computing Machinery,WJPQ4BSQ,0.0168776371308016,0.0,0.0,0.0153166995926928
418,418,Protecting Children from Online Exploitation: Can a Trained Model Detect Harmful Communication Strategies?,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604696,"Cook, Darren; Zilka, Miri; DeSandre, Heidi; Giles, Susan; Maskell, Simon",2023.0,https://doi.org/10.1145/3600211.3604696,conferencePaper,"The growing popularity of social media raises concerns about children’s online safety. Of particular concern are interactions between minors and adults with predatory intentions. Unfortunately, previous research on online sexual grooming has relied on time-intensive manual annotation by domain experts, limiting both the scale and scope of possible interventions. This work explores the possibility of detecting predatory behaviours with accuracy comparable to expert annotators using machine learning (ML). Using a dataset of 6771 chat messages sent by child sex offenders, labelled by two of the authors who are forensic psychology experts, we study how well can deep learning algorithms identify eleven known predatory behaviours. We find that the best-performing ML models are consistent but not on par with expert annotation. We therefore consider a system where an expert annotator validates the ML algorithms outputs. The combination of human decision-making and computer efficiency yields precision—but not recall—comparable to manual annotation, while taking only a fraction of the time needed by a human annotator. Our findings underscore the promise of ML as a tool for assisting researchers in this area, but also highlight the current limitations in reliably detecting online sexual exploitation using ML.",machine learning; natural language processing; chat logs; Child sexual exploitation; online grooming,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,5–14,AIES '23,Association for Computing Machinery,93LSBARG,0.0052083333333333,0.0833333333333333,0.0769230769230769,0.0153160466582597
420,420,The Coloniality of Data Work in Latin America,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462471,"Posada, Julian",2021.0,https://doi.org/10.1145/3461702.3462471,conferencePaper,"This presentation for the AIES '21 doctoral consortium examines the Latin American crowdsourcing market through a decolonial lens. This research is based on the analysis of the web traffic of ninety-three platforms, interviews with Venezuelan data workers of four platforms, and analysis of the documentation issued by these organizations. The findings show that (1) centuries-old global divisions of labor persist, in this case, with requesters located in advanced economies and workers in the Global South. (2) That the platforms' configuration of the labor process constrains the agency of these workers when producing annotations. And, (3) that ideologies originating from the Global North serve to legitimize and reinforce this global labor market configuration.",labor; platform; crowdsourcing; coloniality,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,277–278,AIES '21,Association for Computing Machinery,79346SCN,0.0089285714285714,0.0,0.125,0.0145885547201336
421,421,Multicalibrated Regression for Downstream Fairness,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604683,"Globus-Harris, Ira; Gupta, Varun; Jung, Christopher; Kearns, Michael; Morgenstern, Jamie; Roth, Aaron",2023.0,https://doi.org/10.1145/3600211.3604683,conferencePaper,"We show how to take a regression function that is appropriately multicalibrated and efficiently post-process it into an approximately error minimizing classifier satisfying a large variety of fairness constraints. The post-processing requires no labeled data, and only a modest amount of unlabeled data and computation. The computational and sample complexity requirements of computing are comparable to the requirements for solving a single fair learning task optimally, but it can in fact be used to solve many different downstream fairness-constrained learning problems efficiently. Our post-processing method easily handles intersecting groups, generalizing prior work on post-processing regression functions to satisfy fairness constraints that only applied to disjoint groups. Our work extends recent work showing that multicalibrated regression functions are omnipredictors (i.e. can be post-processed to optimally solve unconstrained ERM problems) to constrained optimization problems.",,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,259–286,AIES '23,Association for Computing Machinery,XNVZS8GP,0.0151515151515151,0.0,0.0,0.0144251721432929
422,422,Governing Algorithmic Systems with Impact Assessments: Six Observations,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462580,"Watkins, Elizabeth Anne; Moss, Emanuel; Metcalf, Jacob; Singh, Ranjit; Elish, Madeleine Clare",2021.0,https://doi.org/10.1145/3461702.3462580,conferencePaper,"Algorithmic decision-making and decision-support systems (ADS) are gaining influence over how society distributes resources, administers justice, and provides access to opportunities. Yet collectively we do not adequately study how these systems affect people or document the actual or potential harms resulting from their integration with important social functions. This is a significant challenge for computational justice efforts of measuring and governing AI systems. Impact assessments are often used as instruments to create accountability relationships and grant some measure of agency and voice to communities affected by projects with environmental, financial, and human rights ramifications. Applying these tools-through Algorithmic Impact Assessments (AIA)-is a plausible way to establish accountability relationships for ADSs. At the same time, what an AIA would entail remains under-specified; they raise as many questions as they answer. Choices about the methods, scope, and purpose of AIAs structure the conditions of possibility for AI governance. In this paper, we present our research on the history of impact assessments across diverse domains, through a sociotechnical lens, to present six observations on how they co-constitute accountability. Decisions about what type of effects count as an impact; when impacts are assessed; whose interests are considered; who is invited to participate; who conducts the assessment; how assessments are made publicly available, and what the outputs of the assessment might be; all shape the forms of accountability that AIAs engender. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.",accountability; governance; algorithmic impact assessment; harm; impact,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,1010–1022,AIES '21,Association for Computing Machinery,SFDFLQWG,0.0154440154440154,0.0,0.0,0.0143279593661273
423,423,"Towards Better Detection of Biased Language with Scarce, Noisy, and Biased Annotations","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534142,"Li, Zhuoyan; Lu, Zhuoran; Yin, Ming",2022.0,https://doi.org/10.1145/3514094.3534142,conferencePaper,"Biased language is prevalent in today's online social media. To reduce the amount of online biased language, one critical first step is to accurately detect such biased language, ideally automatically. This is a challenging problem, however, as the annotated data necessary for training a biased language classifier is either scarce and costly (e.g., when collected from experts), or noisy and potentially biased on their own (e.g., when collected from crowd workers). The biased language classifier built based on these annotations may thus be inaccurate, and sometimes unfair (e.g., have systematic accuracy disparities across texts with different political leanings). In this paper, we propose a novel method, CLEARE, for biased language detection, in which we utilize self-supervised contrastive learning to enhance the biased language classifier—we learn a robust encoder of the textual data through solving a min-max optimization problem, so that the encoder could help achieve the best classification performance even if the worst data augmentation strategy is selected. Extensive evaluations suggest that CLEARE shows substantial improvements compared to the state-of-art biased language detection methods on several benchmark datasets, in terms of improving both the accuracy and the fairness of the detection.",bias detection; biased language; contrastive learning; fairness,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,411–423,AIES '22,Association for Computing Machinery,567EAVHT,0.0157068062827225,0.0,0.0,0.0141125868439448
424,424,Practical Skills Demand Forecasting via Representation Learning of Temporal Dynamics,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534183,"Garcia de Macedo, Maysa Malfiza; Clarke, Wyatt; Lucherini, Eli; Baldwin, Tyler; Queiroz Neto, Dilermando; de Paula, Rogerio Abreu; Das, Subhro",2022.0,https://doi.org/10.1145/3514094.3534183,conferencePaper,"Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry.",demand forecasting; labor economics; neural networks; skill representation,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,285–294,AIES '22,Association for Computing Machinery,L5MSXANA,0.0158730158730158,0.0,0.0,0.014080352178994
425,425,Evaluating Biased Attitude Associations of Language Models in an Intersectional Context,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604666,"Omrani Sabbaghi, Shiva; Wolfe, Robert; Caliskan, Aylin",2023.0,https://doi.org/10.1145/3600211.3604666,conferencePaper,"Language models are trained on large-scale corpora that embed implicit biases documented in psychology. Valence associations (pleasantness/unpleasantness) of social groups determine the biased attitudes towards groups and concepts in social cognition. Building on this established literature, we quantify how social groups are valenced in English language models using a sentence template that provides an intersectional context. We study biases related to age, education, gender, height, intelligence, literacy, race, religion, sex, sexual orientation, social class, and weight. We present a concept projection approach to capture the valence subspace through contextualized word embeddings of language models. Adapting the projection-based approach to embedding association tests that quantify bias, we find that language models exhibit the most biased attitudes against gender identity, social class, and sexual orientation signals in language. We find that the largest and better-performing model that we study is also more biased as it effectively captures bias embedded in sociocultural data. We validate the bias evaluation method by overperforming on an intrinsic valence evaluation task. The approach enables us to measure complex intersectional biases as they are known to manifest in the outputs and applications of language models that perpetuate historical biases. Moreover, our approach contributes to design justice as it studies the associations of groups underrepresented in language such as transgender and homosexual individuals.",AI bias; contextualized word embeddings; intersectional bias; language models; psycholinguistics,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,542–553,AIES '23,Association for Computing Machinery,S6VSYQPX,0.0093457943925233,0.1,0.0,0.0139036913458267
427,427,Strategic Best Response Fairness in Fair Machine Learning,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534194,"Shimao, Hajime; Khern-am-nuai, Warut; Kannan, Karthik; Cohen, Maxime C.",2022.0,https://doi.org/10.1145/3514094.3534194,conferencePaper,"While artificial intelligence (AI) and machine learning (ML) have been increasingly used for decision-making, issues related to discrimination in AI/ML have become prominent. While several fair algorithms are proposed to alleviate these discrimination issues, most of them provide fairness by imposing constraints to eliminate disparity in prediction results. However, the use of these fair algorithms may change the behavior of prediction subjects. As such, even though the disparity in prediction results might be removed by fair algorithms, behavioral responses to the use of fair algorithms can still create disparity in behavior which may persist across different groups of prediction subjects. To study this issue, we define a notion called ""strategic best-response fairness"" (SBR-fair). It is defined in a context that includes different groups of prediction subjects who are ex-ante identical in terms of abilities and conditional payoffs. We utilize a game-theoretic model to investigate whether different types of fair algorithms lead to identical equilibrium behaviors among different groups of prediction subjects. If yes, such an algorithm is considered SBR-fair. We then demonstrate that many existing fair algorithms are not SBR-fair. As a result, implementing these algorithms may impose fairness on prediction results but actually induce disparity between privileged and unprivileged individuals in the long run.",fair machine learning; game-theoretic model; strategic best response,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,664,AIES '22,Association for Computing Machinery,98W9KFKQ,0.0146341463414634,0.0,0.0,0.0134570110179866
429,429,Causal Multi-Level Fairness,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462587,"Mhasawade, Vishwali; Chunara, Rumi",2021.0,https://doi.org/10.1145/3461702.3462587,conferencePaper,"Algorithmic systems are known to impact marginalized groups severely, and more so, if all sources of bias are not considered. While work in algorithmic fairness to-date has primarily focused on addressing discrimination due to individually linked attributes, social science research elucidates how some properties we link to individuals can be conceptualized as having causes at macro (e.g. structural) levels, and it may be important to be fair to attributes at multiple levels. For example, instead of simply considering race as a causal, protected attribute of an individual, the cause may be distilled as perceived racial discrimination an individual experiences, which in turn can be affected by neighborhood-level factors. This multi-level conceptualization is relevant to questions of fairness, as it may not only be important to take into account if the individual belonged to another demographic group, but also if the individual received advantaged treatment at the macro-level. In this paper, we formalize the problem of multi-level fairness using tools from causal inference in a manner that allows one to assess and account for effects of sensitive attributes at multiple levels. We show importance of the problem by illustrating residual unfairness if macro-level sensitive attributes are not accounted for, or included without accounting for their multi-level nature. Further, in the context of a real-world task of predicting income based on macro and individual-level attributes, we demonstrate an approach for mitigating unfairness, a result of multi-level sensitive attributes.",fairness; racial justice; social sciences,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,784–794,AIES '21,Association for Computing Machinery,YLWXARU2,0.0084745762711864,0.2,0.0,0.0130561440677966
430,430,Towards User Guided Actionable Recourse,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604708,"Yetukuri, Jayanth; Hardy, Ian; Liu, Yang",2023.0,https://doi.org/10.1145/3600211.3604708,conferencePaper,"Machine Learning’s proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user’s actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.",Actionable recourse; User preference,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,742–751,AIES '23,Association for Computing Machinery,ZN4Z3SNZ,0.0137931034482758,0.0,0.0,0.0128735632183908
431,431,A Dynamic Decision-Making Framework Promoting Long-Term Fairness,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534127,"Puranik, Bhagyashree; Madhow, Upamanyu; Pedarsani, Ramtin",2022.0,https://doi.org/10.1145/3514094.3534127,conferencePaper,"With AI-based decisions playing an increasingly consequential role in our society, for example, in our financial and criminal justice systems, there is a great deal of interest in designing algorithms conforming to application-specific notions of fairness. In this work, we ask a complementary question: can AI-based decisions be designed to dynamically influence the evolution of fairness in our society over the long term? To explore this question, we propose a framework for sequential decision-making aimed at dynamically influencing long-term societal fairness, illustrated via the problem of selecting applicants from a pool consisting of two groups, one of which is under-represented. We consider a dynamic model for the composition of the applicant pool, in which admission of more applicants from a group in a given selection round positively reinforces more candidates from the group to participate in future selection rounds. Under such a model, we show the efficacy of the proposed Fair-Greedy selection policy which systematically trades the sum of the scores of the selected applicants (""greedy”) against the deviation of the proportion of selected applicants belonging to a given group from a target proportion (""fair”). In addition to experimenting on synthetic data, we adapt static real-world datasets on law school candidates and credit lending to simulate the dynamics of the composition of the applicant pool. We prove that the applicant pool composition converges to a target proportion set by the decision-maker when score distributions across the groups are identical.",ai for social equity; fair selection; long-term fairness; positive reinforcement; sequential decision-making,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,547–556,AIES '22,Association for Computing Machinery,YW5C3BYB,0.00836820083682,0.0833333333333333,0.0,0.0126542946327764
432,432,Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462519,"Cooper, A. Feder; Abrams, Ellen; NA, NA",2021.0,https://doi.org/10.1145/3461702.3462519,conferencePaper,"Across machine learning (ML) sub-disciplines, researchers make explicit mathematical assumptions in order to facilitate proof-writing. We note that, specifically in the area of fairness-accuracy trade-off optimization scholarship, similar attention is not paid to the normative assumptions that ground this approach. Such assumptions presume that 1) accuracy and fairness are in inherent opposition to one another, 2) strict notions of mathematical equality can adequately model fairness, 3) it is possible to measure the accuracy and fairness of decisions independent from historical context, and 4) collecting more data on marginalized individuals is a reasonable solution to mitigate the effects of the trade-off. We argue that such assumptions, which are often left implicit and unexamined, lead to inconsistent conclusions: While the intended goal of this work may be to improve the fairness of machine learning models, these unexamined, implicit assumptions can in fact result in emergent unfairness. We conclude by suggesting a concrete path forward toward a potential resolution.",machine learning; algorithmic fairness; societal implications of AI,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,46–54,AIES '21,Association for Computing Machinery,BY7AT5MM,0.0064102564102564,0.125,0.0,0.0125248560962846
433,433,Computer Vision and Conflicting Values: Describing People with Automated Alt Text,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462620,"Hanley, Margot; Barocas, Solon; Levy, Karen; Azenkot, Shiri; Nissenbaum, Helen",2021.0,https://doi.org/10.1145/3461702.3462620,conferencePaper,"Scholars have recently drawn attention to a range of controversial issues posed by the use of computer vision for automatically generating descriptions of people in images. Despite these concerns, automated image description has become an important tool to ensure equitable access to information for blind and low vision people. In this paper, we investigate the ethical dilemmas faced by companies that have adopted the use of computer vision for producing alt text: textual descriptions of images for blind and low vision people. We use Facebook's automatic alt text tool as our primary case study. First, we analyze the policies that Facebook has adopted with respect to identity categories, such as race, gender, age, etc., and the company's decisions about whether to present these terms in alt text. We then describe an alternative—and manual—approach practiced in the museum community, focusing on how museums determine what to include in alt text descriptions of cultural artifacts. We compare these policies, using notable points of contrast to develop an analytic framework that characterizes the particular apprehensions behind these policy choices. We conclude by considering two strategies that seem to sidestep some of these concerns, finding that there are no easy ways to avoid the normative dilemmas posed by the use of computer vision to automate alt text.",gender; race; computer vision; policy; accessibility; alt text; disability; facebook; identity; museums; visual impairments,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,543–554,AIES '21,Association for Computing Machinery,7VCJVQMD,0.0140845070422535,0.0,0.0,0.0122636962459165
434,434,Moral Disagreement and Artificial Intelligence,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462534,"Robinson, Pamela",2021.0,https://doi.org/10.1145/3461702.3462534,conferencePaper,"Artificially intelligent systems will be used to make increasingly important decisions about us. Many of these decisions will have to be made without consensus about the relevant moral facts. I argue that what makes moral disagreement especially challenging is that there are two different ways of handling it: political solutions, which aim to find a fair compromise, and epistemic solutions, which aim at moral truth.",artificial intelligence; moral disagreement; moral uncertainty,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,209,AIES '21,Association for Computing Machinery,8EBRV9GN,0.0153846153846153,0.0,0.0,0.012231790979419
435,435,Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534181,"Raji, Inioluwa Deborah; Xu, Peggy; Honigsberg, Colleen; Ho, Daniel",2022.0,https://doi.org/10.1145/3514094.3534181,conferencePaper,"Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.",accountability; algorithms; auditing; policy; society,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,557–571,AIES '22,Association for Computing Machinery,WWPSAL8W,0.0068493150684931,0.0,0.0909090909090909,0.0121298587513763
436,436,True and Fair: Robust and Unbiased Fake News Detection via Interpretable Machine Learning,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604760,"Raj, Chahat; Mukherjee, Anjishnu; Zhu, Ziwei",2023.0,https://doi.org/10.1145/3600211.3604760,conferencePaper,"The dissemination of information, and consequently, misinformation, occurs at an unprecedented speed, making it increasingly difficult to discern the credibility of rapidly circulating news. Advanced large-scale language models have facilitated the development of classifiers capable of effectively identifying misinformation. Nevertheless, these models are intrinsically susceptible to biases that may be introduced through numerous ways, including contaminated data sources or unfair training methodologies. When trained on biased data, machine learning models may inadvertently learn and reinforce these biases, leading to reduced generalization performance. This situation consequently results in an inherent ""unfairness"" within the system. Interpretability, referring to the ability to understand and explain the decision-making process of a model, can be used as a tool to explain these biases. Our research aims to identify the root causes of these biases in fake news detection and mitigate their presence using interpretability. We also perform inference time attacks to fairness to validate robustness.",security; fairness; bias; misinformation; interpretability,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,962–963,AIES '23,Association for Computing Machinery,7MCFU9ZK,0.0133333333333333,0.0,0.0,0.0117765422292825
437,437,Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462540,"Tomasev, Nenad; McKee, Kevin R.; Kay, Jackie; Mohamed, Shakir",2021.0,https://doi.org/10.1145/3461702.3462540,conferencePaper,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness—frequently, race and legal gender—can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.",machine learning; algorithmic fairness; gender identity; marginalised groups; queer communities; sexual orientation,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,254–265,AIES '21,Association for Computing Machinery,XQ93PJG5,0.0138888888888888,0.0,0.0,0.0116565579984836
439,439,Write It Like You See It: Detectable Differences in Clinical Notes by Race Lead to Differential Model Recommendations,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534203,"Adam, Hammaad; Yang, Ming Ying; Cato, Kenrick; Baldini, Ioana; Senteio, Charles; Celi, Leo Anthony; Zeng, Jiaming; Singh, Moninder; Ghassemi, Marzyeh",2022.0,https://doi.org/10.1145/3514094.3534203,conferencePaper,"Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.",natural language processing; clinical notes; health equity,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,7–21,AIES '22,Association for Computing Machinery,JHL7ZBAC,0.0134228187919463,0.0,0.0,0.0114304580721917
440,440,Keep Sensors in Check: Disentangling Country-Level Generalization Issues in Mobile Sensor-Based Models with Diversity Scores,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604688,"Nanchen, Alexandre; Meegahapola, Lakmal; Droz, William; Gatica-Perez, Daniel",2023.0,https://doi.org/10.1145/3600211.3604688,conferencePaper,"Machine learning models trained with passive sensor data from mobile devices can be used to perform various inferences pertaining to activity recognition, context awareness, and health and well-being. Prior work has improved inference performance through the use of multimodal sensors (inertial, GPS, proximity, app usage, etc.) or improved machine learning. In this context, a few studies shed light on critical issues relating to the poor cross-country generalization of models due to distributional shifts across countries. However, these studies have largely relied on inference performance as a means of studying generalization issues, failing to investigate whether the root cause of the problem is linked to specific sensor modalities (independent variables) or the target attribute (dependent variable). In this paper, we study this issue in complex activities of daily living (ADL) inference task, involving 12 classes, by using a multimodal, multi-country dataset collected from 689 participants across eight countries. We first show that the ‘country of origin’ of data is captured by sensors and can be inferred from each modality separately, with an average accuracy of 65%. We then propose two diversity scores (DS) that measure how a country differentiates from others w.r.t. sensor modalities or activities. Using these diversity scores, we observed that both individual sensor modalities and activities have the ability to differentiate countries. However, while many activities capture country differences, only the ‘App usage’ and ‘Location’ sensors can do so. By dissecting country-level diversity across dependent and independent variables, we provide a framework to better understand model generalization issues across countries and country-level diversity of sensing modalities.",bias; country; country diversity; data diversity; distributional shift; generalization; mobile sensing; smartphone sensing,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,217–228,AIES '23,Association for Computing Machinery,8A5PWBKU,0.0077519379844961,0.0769230769230769,0.0,0.0114049124058099
441,441,Machine Learning and the Meaning of Equal Treatment,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462556,"Simons, Joshua; Adams Bhatti, Sophia; Weller, Adrian",2021.0,https://doi.org/10.1145/3461702.3462556,conferencePaper,"Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.",equal treatment; fairness; machine learning; philosophy; politics,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,956–966,AIES '21,Association for Computing Machinery,HHSDLELN,0.0119521912350597,0.0,0.0,0.0112353069624294
442,442,"Who Gets What, According to Whom? An Analysis of Fairness Perceptions in Service Allocation","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462568,"Hannan, Jacqueline; Chen, Huei-Yen Winnie; Joseph, Kenneth",2021.0,https://doi.org/10.1145/3461702.3462568,conferencePaper,"Algorithmic fairness research has traditionally been linked to the disciplines of philosophy, ethics, and economics, where notions of fairness are prescriptive and seek objectivity. Increasingly, however, scholars are turning to the study of what different people perceive to be fair, and how these perceptions can or should help to shape the design of machine learning, particularly in the policy realm. The present work experimentally explores five novel research questions at the intersection of the ""Who,"" ""What,"" and ""How"" of fairness perceptions. Specifically, we present the results of a multi-factor conjoint analysis study that quantifies the effects of the specific context in which a question is asked, the framing of the given question, and who is answering it. Our results broadly suggest that the ""Who"" and ""What,"" at least, matter in ways that are 1) not easily explained by any one theoretical perspective, 2) have critical implications for how perceptions of fairness should be measured and/or integrated into algorithmic decision-making systems.",fairness; conjoint analysis; fairness perceptions; service allocation; survey experiment,978-1-4503-8473-5,,2021,2023-11-06 01:29:53,2023-11-06 01:29:53,555–565,AIES '21,Association for Computing Machinery,W4QIKDAH,0.0125,0.0,0.0,0.0106984702093397
443,443,"How Algorithms Shape the Distribution of Political Advertising: Case Studies of Facebook, Google, and TikTok","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534166,"Papakyriakopoulos, Orestis; Tessono, Christelle; Narayanan, Arvind; Kshirsagar, Mihir",2022.0,https://doi.org/10.1145/3514094.3534166,conferencePaper,"Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.",accountability; algorithmic auditing; algorithmic targeting; interpretability; political advertising; political speech; regulation,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,532–546,AIES '22,Association for Computing Machinery,FBYW6LUF,0.0131578947368421,0.0,0.0,0.0106978955637325
444,444,Learning Fairer Interventions,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534172,"He, Yuzi; Burghardt, Keith; Guo, Siyi; Lerman, Kristina",2022.0,https://doi.org/10.1145/3514094.3534172,conferencePaper,"Explicit and implicit bias clouds human judgment, leading to discriminatory treatment of disadvantaged groups. A fundamental goal of automated decisions is to avoid the pitfalls in human judgment by developing decision strategies that can be applied to all protected groups. Improving fairness of interventions via automated decision-inspired methods, however, has been under-utilized. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to novel fairness constraints. We define two measures of treatment bias and infer treatment assignments that minimize the bias against protected groups while optimizing overall outcomes. We demonstrate the existence of trade-offs when balancing fairness and overall benefit; however, allowing preferential treatment of protected groups in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve academic performance for different geographic areas. Our framework provides a principled way to learn fair treatment policies in real-world settings.",fairness metrics; affirmative action; fair treatment; policy recommendations; treatment biases,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,317–323,AIES '22,Association for Computing Machinery,65CA58X6,0.0115606936416184,0.0,0.0,0.0105250306278391
445,445,Towards Equity and Algorithmic Fairness in Student Grade Prediction,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462623,"Jiang, Weijie; Pardos, Zachary A.",2021.0,https://doi.org/10.1145/3461702.3462623,conferencePaper,"Equity of educational outcome and fairness of AI with respect to race have been topics of increasing importance in education. In this work, we address both with empirical evaluations of grade prediction in higher education, an important task to improve curriculum design, plan interventions for academic support, and offer course guidance to students. With fairness as the aim, we trial several strategies for both label and instance balancing to attempt to minimize differences in algorithm performance with respect to race. We find that an adversarial learning approach, combined with grade label balancing, achieved by far the fairest results. With equity of educational outcome as the aim, we trial strategies for boosting predictive performance on historically underserved groups and find success in sampling those groups in inverse proportion to their historic outcomes. With AI-infused technology supports increasingly prevalent on campuses, our methodologies fill a need for frameworks to consider performance trade-offs with respect to sensitive student attributes and allow institutions to instrument their AI resources in ways that are attentive to equity and fairness.",equity; fairness; grade prediction; higher education,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,608–617,AIES '21,Association for Computing Machinery,MW9HR4BD,0.0115606936416184,0.0,0.0,0.010501630867675
446,446,Hijacking Epistemic Agency: How Emerging Technologies Threaten Our Wellbeing as Knowers,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539537,"Dorsch, John",2022.0,https://doi.org/10.1145/3514094.3539537,conferencePaper,"The aim of this project to expose the reasons behind the pandemic of misinformation (henceforth, PofM) by examining the enabling conditions of epistemic agency and the emerging technologies that threaten it. I plan to research the emotional origin of epistemic agency, i.e. on the origin of our capacity to acquire justification for belief, as well as on the significance this emotional origin has for our lives as epistemic agents in our so-called Misinformation Age [1]. This project has three objectives. First, I plan to expose the degree to which epistemic agency is made possible by an under-researched species of emotion called epistemic feelings [2] [3]. Perhaps, the most epistemically significant is the feeling of confidence [4]. In particular, epistemic feelings make epistemic agency possible by making errors in reasoning salient or the potential lack thereof [5].Second, in order to diagnose the reasons for PofM, I will analyze the emotional basis of epistemic agency in the context of emerging technologies [6]. Epistemic feelings ought to be construed as motivators of epistemic acts, specifically acts following exposure to misinformation spread by social media [7]. For example, a recent study found that subjects on YouTube have a 6.3% probability in five clicks to go from watching innocuous videos to misogynistic videos [8]. Accordingly, one prominent view holds that social media algorithms lead subjects down the proverbial rabbit hole, producing content that elicits strong emotional reactions [9] [10].But under-researched is the degree to which these emotions are intertwined with epistemic feelings, so that subjects are likely to misjudge the information as correct due to being primed to do so by processing the AI's suggested media. Thus, when this technology is used in epistemic practices, the result is invariably epistemically bad behavior because of how this manner of producing content motivates epistemic acts through the production of erroneous feelings of confidence, which result from the perceived ease of cognitive processing [11] [12]: the content is easy to process and so, the subject confidently misjudges, it is true. As of yet, an account of this cognitive and emotional decision-making process has not been used to analyze how emerging technologies are threatening epistemic agency, and thus our wellbeing as knowers.Finally, I plan to develop therapeutics for PofM by outlining how to reform our collective epistemic practices. Thus, this project complements existing research in virtue epistemology [13], specifically by articulating the challenges to cultivating epistemic virtues and warding off the vices of the mind [14]. In summation, the project aims to account for how emerging technologies are committing a form of epistemic injustice [15] through the exploitation of cognitive biases and the production of erroneous epistemic feelings, with the overarching goal being a framework for convalescing from the pandemic of misinformation plaguing our age.",ai ethics; metacognition; philosophy of technology; virtue epistemology,978-1-4503-9247-1,,2022,2023-11-06 01:29:49,2023-11-06 01:29:49,897,AIES '22,Association for Computing Machinery,7F85PKES,0.0021929824561403,0.375,0.0,0.0104801186188429
447,447,Multi-CoPED: A Multilingual Multi-Task Approach for Coding Political Event Data on Conflict and Mediation Domain,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534178,"Skorupa Parolin, Erick; Hosseini, MohammadSaleh; Hu, Yibo; Khan, Latifur; Brandt, Patrick T.; Osorio, Javier; D'Orazio, Vito",2022.0,https://doi.org/10.1145/3514094.3534178,conferencePaper,"Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3% and 30.7% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.",natural language processing; artificial intelligence and geopolitics; event coding; political conflict; social conflict; transfer learning,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,700–711,AIES '22,Association for Computing Machinery,IUZ6EYRP,0.0063291139240506,0.0,0.0666666666666666,0.0104183343449359
448,448,American == White in Multimodal Language-and-Image AI,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534136,"Wolfe, Robert; Caliskan, Aylin",2022.0,https://doi.org/10.1145/3514094.3534136,conferencePaper,"Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes &gt;.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text ""an American person,"" a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.",bias in ai; multimodal models; racial bias; visual semantics,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,800–812,AIES '22,Association for Computing Machinery,YFDWEFIN,0.0052631578947368,0.1111111111111111,0.1428571428571428,0.010394168950017
450,450,Making the Unaccountable Internet: The Changing Meaning of Accounting in the Early ARPANET,"Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3531146.3533137,"Cooper, A. Feder; Vidan, Gili",2022.0,https://doi.org/10.1145/3531146.3533137,conferencePaper,"Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.",Accountability; Accountable systems; Accounting; Internet governance; Resource sharing,978-1-4503-9352-2,,2022,2023-11-06 01:30:04,2023-11-06 01:30:04,726–742,FAccT '22,Association for Computing Machinery,FJKUW5RK,0.0111524163568773,0.0,0.0,0.0102278637055299
451,451,Alienation in the AI-Driven Workplace,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462520,"Vredenburgh, Kate",2021.0,https://doi.org/10.1145/3461702.3462520,conferencePaper,"This paper asks whether explanations of one's workplace and economic institutions are valuable in and of themselves. In doing so, it departs from much of the explainability literature in law, computer science, philosophy, and the social sciences, which examine the instrumental values that explainable AI has: explainable systems increase accountability and user trust, or reduce the risk of harm due to increased robustness. Think, however, of how you might feel if you went to your local administrative agency to apply for some benefit, or you were handed down a decision by a judge in a court. Let's stipulate that you know that the decision was just, even though neither the civil servant nor the judge explain to you why the decision was made, and you don't know the relevant rules; you just brought all the information you had about yourself, and hoped for the best. Is such a decision process defective? I argue that such a decision process is defective because it prevents individuals from accessing the normative explanations that are necessary to form an appropriate practical orientation towards their social world. A practical orientation is a reflective stance towards one's social world, which is expressed in one's actions and draws on one's cognitive architecture that allows one to navigate the various social practices and institutions. A practical orientation can range from rejection to silent endorsement, and is the sort of attitude for which there are the right kind of reasons, based in the world's normative character. It also determines how one fills out one's role obligations, and, more broadly, guides one's actions in the relevant institution: a teacher in the American South during the time of enforced racial segregation, for example, might choose where to teach on the basis of her rejection of the segregation of education. To form an appropriate practical orientation, one must have an understanding of the social world's normative character, which required a normative explanation And, since we spend so much of our lives at work and are constrained by economic institutions, we must understand their structure and how they function.",alienation; explainable ai; explanation; interpretability; work,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,266,AIES '21,Association for Computing Machinery,EIHXUHB7,0.0057803468208092,0.1666666666666666,0.0,0.0101531830120472
452,452,Person Re-Identification: A Retrospective on Domain Specific Open Challenges and Future Trends,Pattern Recogn.,142.0,C,10.1016/j.patcog.2023.109669,"Zahra, Asmat; Perwaiz, Nazia; Shahzad, Muhammad; Fraz, Muhammad Moazam",2023.0,https://doi.org/10.1016/j.patcog.2023.109669,journalArticle,"Person Re-Identification (Re-ID) is a critical aspect of visual surveillance systems, which aims to automatically recognize and locate individuals across a multi-camera network with non-overlapping fields-of-view. Despite significant progress in recent years through the use of deep learning-based approaches, there remain many vision-related challenges, such as occlusion, pose, background clutter, misalignment, scale, viewpoint, low resolution & illumination, and cross-domain generalization across camera modalities, that hinder the accurate identification of individuals. The majority of the proposed approaches directly or indirectly aim to solve one or multiple of these existing challenges. To further advance the development of Re-ID solutions, a comprehensive review of the current approaches is necessary. However, no focused review currently exists that analyses and highlights specific aspects for further development. To fill this gap, we present a systematic challenge-specific literature survey of about 300 papers published between 2015 and 2022, which reviews Re-ID approaches from a solution-oriented perspective. This survey is the first of its kind to provide an in-depth analysis of the different approaches used to address the various challenges in Re-ID. Furthermore, our review highlights several prominent and diverse research trends in the Re-ID domain. These trends offer a visionary perspective regarding ongoing person Re-ID research, and they may eventually lead to the development of practical real-world solutions. We highlighted the AI ethics that must be followed while developing a Re-ID solution, and recently being practiced as well. Another exciting future dimension of person Re-ID research is the long-term Re-ID, which is still under evolution. Overall, our survey aims to serve as a valuable resource for researchers and practitioners working in the field of Re-ID and to inspire the development of innovative and effective Re-ID solutions.",Deep learning; Literature survey; Open challenges; Person re-Identification; Specific application-driven,,0031-3203,2023-10,2023-11-06 01:30:04,2023-11-06 01:30:04,,,,QX2IEHIE,0.010752688172043,0.0,0.0,0.0097765156557364
453,453,Supporting Human-AI Collaboration in Auditing LLMs with LLMs,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604712,"Rastogi, Charvi; Tulio Ribeiro, Marco; King, Nicholas; Nori, Harsha; Amershi, Saleema",2023.0,https://doi.org/10.1145/3600211.3604712,conferencePaper,"Large language models (LLMs) are increasingly becoming all-powerful and pervasive via deployment in sociotechnical systems. Yet these language models, be it for classification or generation, have been shown to be biased, behave irresponsibly, causing harm to people at scale. It is crucial to audit these language models rigorously before deployment. Existing auditing tools use either or both humans and AI to find failures. In this work, we draw upon literature in human-AI collaboration and sensemaking, and interview research experts in safe and fair AI, to build upon the auditing tool: AdaTest&nbsp;[36], which is powered by a generative LLM. Through the design process we highlight the importance of sensemaking and human-AI communication to leverage complementary strengths of humans and generative models in collaborative auditing. To evaluate the effectiveness of AdaTest++, the augmented tool, we conduct user studies with participants auditing two commercial language models: OpenAI’s GPT-3 and Azure’s sentiment analysis model. Qualitative analysis shows that AdaTest++ effectively leverages human strengths such as schematization, hypothesis testing. Further, with our tool, users identified a variety of failures modes, covering 26 different topics over 2 tasks, that have been shown in formal audits and also those previously under-reported.",language models; generative models; auditing; biases,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,913–926,AIES '23,Association for Computing Machinery,NHUML3VK,0.0103092783505154,0.0,0.0,0.0095232748275353
454,454,Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604690,"Henderson, Peter; Mitchell, Eric; Manning, Christopher; Jurafsky, Dan; Finn, Chelsea",2023.0,https://doi.org/10.1145/3600211.3604690,conferencePaper,"A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing performance on desirable tasks. We call the resulting models self-destructing models, inspired by mechanisms that prevent adversaries from using tools for harmful purposes. We present an algorithm for training self-destructing models leveraging techniques from meta-learning and adversarial learning, which we call meta-learned adversarial censoring (MLAC). In a small-scale experiment, we show MLAC can largely prevent a BERT-style model from being re-purposed to perform gender identification without harming the model’s ability to perform profession classification.",,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,287–296,AIES '23,Association for Computing Machinery,XACX42YR,0.0099009900990099,0.0,0.0,0.0093448983374773
455,455,Model Debiasing via Gradient-Based Explanation on Representation,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604668,"Zhang, Jindi; Wang, Luning; Su, Dan; Huang, Yongxiang; Cao, Caleb Chen; Chen, Lei",2023.0,https://doi.org/10.1145/3600211.3604668,conferencePaper,"Machine learning systems produce biased results towards certain demographic groups, known as the fairness problem. Recent approaches to tackle this problem learn a latent code (i.e., representation) through disentangled representation learning and then discard the latent code dimensions correlated with sensitive attributes (e.g., gender). Nevertheless, these approaches may suffer from incomplete disentanglement and overlook proxy attributes (proxies for sensitive attributes) when processing real-world data, especially for unstructured data, causing performance degradation in fairness and loss of useful information for downstream tasks. In this paper, we propose a novel fairness framework that performs debiasing with regard to both sensitive attributes and proxy attributes, which boosts the prediction performance of downstream task models without complete disentanglement. The main idea is to, first, leverage gradient-based explanation to find two model focuses, 1) one focus for predicting sensitive attributes and 2) the other focus for predicting downstream task labels, and second, use them to perturb the latent code that guides the training of downstream task models towards fairness and utility goals. We show empirically that our framework works with both disentangled and non-disentangled representation learning methods and achieves better fairness-accuracy trade-off on unstructured and structured datasets than previous state-of-the-art approaches.",fairness; gradient-based explanation; model debiasing; representation learning,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,193–204,AIES '23,Association for Computing Machinery,AJV3NJ89,0.010204081632653,0.0,0.0,0.0093063247603874
456,456,Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462602,"Sühr, Tom; Hilgard, Sophie; Lakkaraju, Himabindu",2021.0,https://doi.org/10.1145/3461702.3462602,conferencePaper,"Ranking algorithms are being widely employed in various online hiring platforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has demonstrated that ranking algorithms employed by these platforms are prone to a variety of undesirable biases, leading to the proposal of fair ranking algorithms (e.g., Det-Greedy) which increase exposure of underrepresented candidates. However, there is little to no work that explores whether fair ranking algorithms actually improve real world outcomes (e.g., hiring decisions) for underrepresented groups. Furthermore, there is no clear understanding as to how other factors (e.g., job context, inherent biases of the employers) may impact the efficacy of fair ranking in practice.In this work, we analyze various sources of gender biases in online hiring platforms, including the job context and inherent biases of employers and establish how these factors interact with ranking algorithms to affect hiring decisions. To the best of our knowledge, this work makes the first attempt at studying the interplay between the aforementioned factors in the context of online hiring. We carry out a large-scale user study simulating online hiring scenarios with data from TaskRabbit, a popular online freelancing site. Our results demonstrate that while fair ranking algorithms generally improve the selection rates of underrepresented minorities, their effectiveness relies heavily on the job contexts and candidate profiles.",gender; discrimination; fair ranking; online hiring; user studies,978-1-4503-8473-5,,2021,2023-11-06 01:29:54,2023-11-06 01:29:54,989–999,AIES '21,Association for Computing Machinery,HNW7NK5F,0.0047393364928909,0.125,0.0,0.009124011681948
457,457,Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models Using an Interdisciplinary Lens,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604754,"Narayanan Venkit, Pranav",2023.0,https://doi.org/10.1145/3600211.3604754,conferencePaper,"The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority’s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases’ potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers’ experiences when encountering biased articles, highlighting the capacity to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners, and policymakers are empowered to forge practical solutions that embody inclusivity and reliability, especially when used as a service (AIaaS). Moreover, the platform serves as a centralized locus for the identification and rectification of social biases, irrespective of the underlying model or architecture. By furnishing a cogent narrative that underscores the imperative for a comprehensive and interdisciplinary approach, my work strives to propel the ongoing endeavors to comprehend and mitigate biases within the realm of NLP. With its potential to augment the equity, inclusivity, and societal ramifications of NLP technologies, the proposed framework catapults the field towards responsible and ethical practices.",,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,1004–1005,AIES '23,Association for Computing Machinery,X4P59KWF,0.0091863517060367,0.0,0.0,0.0090913935711889
458,458,FINS Auditing Framework: Group Fairness for Subset Selections,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534160,"Cachel, Kathleen; Rundensteiner, Elke",2022.0,https://doi.org/10.1145/3514094.3534160,conferencePaper,"Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.",algorithmic fairness; machine learning fairness; subset selection,978-1-4503-9247-1,,2022,2023-11-06 01:29:54,2023-11-06 01:29:54,144–155,AIES '22,Association for Computing Machinery,VY4QP8BL,0.0097560975609756,0.0,0.0,0.0090079123554473
459,459,Select Wisely and Explain: Active Learning and Probabilistic Local Post-Hoc Explainability,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534191,"Saini, Aditya; Prasad, Ranjitha",2022.0,https://doi.org/10.1145/3514094.3534191,conferencePaper,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive domains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active learning based locally faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",explainable AI; acquisition functions; Bayesian learning; Gaussian process; uncertainty reduction,978-1-4503-9247-1,,2022,2023-11-06 01:29:57,2023-11-06 01:29:57,599–608,AIES '22,Association for Computing Machinery,CTXQ2QIE,0.0041152263374485,0.1,0.0,0.0088553816850584
461,461,Achievement and Fragility of Long-Term Equitability,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534132,"Simonetto, Andrea; Notarnicola, Ivano",2022.0,https://doi.org/10.1145/3514094.3534132,conferencePaper,"Equipping current decision-making tools with notions of fairness, equitability, or other ethically motivated outcomes, is one of the top priorities in recent research efforts in machine learning, AI, and optimization. In this paper, we investigate how to allocate limited resources to locally interacting communities in a way to maximize a pertinent notion of equitability. In particular, we look at the dynamic setting where the allocation is repeated across multiple periods (e.g., yearly), the local communities evolve in the meantime (driven by the provided allocation), and the allocations are modulated by feedback coming from the communities themselves. We employ recent mathematical tools stemming from data-driven feedback online optimization, by which communities can learn their (possibly unknown) evolution, satisfaction, as well as they can share information with the deciding bodies. We design dynamic policies that converge to an allocation that maximize equitability in the long term. We further demonstrate our model and methodology with realistic examples of healthcare and education subsidies design in Sub-Saharian countries. One of the key empirical takeaways from our setting is that long-term equitability is fragile, in the sense that it can be easily lost when deciding bodies weigh in other factors (e.g., equality in allocation) in the allocation strategy. Moreover, a naive compromise, while not providing significant advantage to the communities, can promote inequality in social outcomes.",dynamical systems; equitability; fairness; optimization; subsidies design,978-1-4503-9247-1,,2022,2023-11-06 01:29:51,2023-11-06 01:29:51,675–685,AIES '22,Association for Computing Machinery,SFTJBYRH,0.009090909090909,0.0,0.0,0.0084043848964677
462,462,The Worst of Both Worlds: A Comparative Analysis of Errors in Learning from Data in Psychology and Machine Learning,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534196,"Hullman, Jessica; Kapoor, Sayash; Nanayakkara, Priyanka; Gelman, Andrew; Narayanan, Arvind",2022.0,https://doi.org/10.1145/3514094.3534196,conferencePaper,"Arguments that machine learning (ML) is facing a reproducibility and replication crisis suggest that some published claims in research cannot be taken at face value. Concerns inspire analogies to the replication crisis affecting the social and medical sciences. A deeper understanding of what reproducibility concerns in supervised ML research have in common with the replication crisis in experimental science puts the new concerns in perspective, and helps researchers avoid ""the worst of both worlds,"" where ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in causal attribution as exemplified in psychology versus predictive modeling as exemplified in ML. We identify common themes in reform discussions, like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often difficult to refute due to underspecification of key parts of the learning pipeline. We conclude by discussing risks that arise when sources of errors are misdiagnosed and the need to acknowledge the role of human inductive biases in learning and reform.",machine learning; generalizability; replication; science reform,978-1-4503-9247-1,,2022,2023-11-06 01:29:58,2023-11-06 01:29:58,335–348,AIES '22,Association for Computing Machinery,9SC2BWCZ,0.0048076923076923,0.0,0.0526315789473684,0.0080176134616009
464,464,ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five Other Low-Resource Languages,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604672,"Ghosh, Sourojit; Caliskan, Aylin",2023.0,https://doi.org/10.1145/3600211.3604672,conferencePaper,"In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in machine translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT’s accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7th most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g., man = doctor, woman = nurse) or actions (e.g., woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to ‘he’ or ‘she’. We also observe ChatGPT completely failing to translate the English gender-neutral singular pronoun ‘they’ into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AI systems that perform machine translation to better accommodate such low-resource languages.",language models; gender bias; Bengali; ChatGPT; human-centered design; machine translation,9798400702310,,2023,2023-11-06 01:29:55,2023-11-06 01:29:55,901–912,AIES '23,Association for Computing Machinery,WTPFHWZN,0.0082644628099173,0.0,0.0,0.0071919073240853
465,465,Are Model Explanations Useful in Practice? Rethinking How to Support Human-ML Interactions,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604726,"Chen, Valerie",2023.0,https://doi.org/10.1145/3600211.3604726,conferencePaper,"Model explanations have been touted as crucial information to fa- cilitate human-ML interactions in many real-world applications where end users make decisions informed by ML predictions. For example, explanations are thought to assist model developers in identifying when models rely on spurious artifacts [1] and to aid domain experts in determining whether to follow a model's predic- tion [4]. However, while numerous explainable AI (XAI) methods have been developed (e.g., LIME [12], SHAP [10]), XAI has yet to deliver on this promise. XAI methods are typically optimized for diverse but narrow technical objectives disconnected from their claimed use cases. To connect methods to concrete use cases, I argue that researchers need to rigorously evaluate how well proposed methods can help real users in their real-world applications [7].",,9798400702310,,2023,2023-11-06 01:29:54,2023-11-06 01:29:54,942–944,AIES '23,Association for Computing Machinery,DJC5B5IN,0.0078740157480314,0.0,0.0,0.0071136494204318
466,466,"Queering Futures: The Design of an Expanded Mixed Methods Research Framework Integrating Qualitative, Quantitative, and Practice-Based Modes","Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604724,"Westbrook, Jess Parris",2023.0,https://doi.org/10.1145/3600211.3604724,conferencePaper,"Queering Futures with Data-Driven Speculation: the design of an expanded mixed methods research framework integrating qualitative, quantitative, and practice-based modes – is a Queer research journey. Expanding a mixed methods research framework is strange and different. The Queering Futures Framework (QFF) disregards the constraints of traditional mixed methods research conventions. After intersecting concurrent qualitative modes (exploration of impressions of futures) and quantitative modes (measures of attitudes towards AI), it wanders and stretches into an open creative practice-based mode. It is in the culminating creative practice-based mode that signals identified in the qualitative and the quantitative datasets are compared, scanned, probed, mined, and leveraged using a new futures method I call data-driven speculation.",data-driven speculation; expanded mixed methods; mental time travel; Queering futures,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,941,AIES '23,Association for Computing Machinery,MQT7YQY9,0.0089285714285714,0.0,0.0,0.0070404806659505
467,467,Understanding Decision Subjects' Fairness Perceptions and Retention in Repeated Interactions with AI-Based Decision Systems,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534201,"Gemalmaz, Meric Altug; Yin, Ming",2022.0,https://doi.org/10.1145/3514094.3534201,conferencePaper,"The wide application of AI-based decision systems in many high-stake domains has raised concerns regarding fairness of these systems. As these systems will lead to real-life consequences to people who are subject to their decisions, understanding what these decision subjects perceive as a fair or unfair system is of vital importance. In this paper, we extend prior work in this direction by taking a perspective of repeated interactions—We ask that when decision subjects interact with an AI-based decision system repeatedly and can strategically respond to the system by determining whether to stay in the system, what factors will affect the decision subjects' fairness perceptions and retention in the system and how. To answer these questions, we conducted two randomized human-subject experiments in the context of an AI-based loan lending system. Our results suggest that in repeated interactions with the AI-based decision system, overall, decision subjects' fairness perceptions and retention in the system are significantly affected by whether the system is in favor of the group that subjects themselves belong to, rather than whether the system treats different groups in an unbiased way. However, decision subjects with different qualification levels have different reactions to the AI system's biased treatment across groups or the AI system's tendency to favor/disfavor their own group. Finally, we also find that while subjects' retention in the AI-based decision system is largely driven by their own prospects of receiving the favorable decision from the system, their fairness perceptions of the system is influenced by the system's treatment to people in other groups in a complex way.",fairness; human-AI interaction; fairness perceptions; AI-based decision systems; human-subject experiments; retention,978-1-4503-9247-1,,2022,2023-11-06 01:29:53,2023-11-06 01:29:53,295–306,AIES '22,Association for Computing Machinery,RNUQX4F4,0.0077220077220077,0.0,0.0,0.0067696532239183
468,468,Minimax Group Fairness: Algorithms and Experiments,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462523,"Diana, Emily; Gill, Wesley; Kearns, Michael; Kenthapadi, Krishnaram; Roth, Aaron",2021.0,https://doi.org/10.1145/3461702.3462523,conferencePaper,"We consider a recently introduced framework in which fairness is measured by worst-case outcomes across groups, rather than by the more standard differences between group outcomes. In this framework we provide provably convergent oracle-efficient learning algorithms (or equivalently, reductions to non-fair learning) for minimax group fairness. Here the goal is that of minimizing the maximum loss across all groups, rather than equalizing group losses. Our algorithms apply to both regression and classification settings and support both overall error and false positive or false negative rates as the fairness measure of interest. They also support relaxations of the fairness constraints, thus permitting study of the tradeoff between overall accuracy and minimax fairness. We compare the experimental behavior and performance of our algorithms across a variety of fairness-sensitive data sets and show empirical cases in which minimax fairness is strictly and strongly preferable to equal outcome notions.",fair machine learning; game theory; minimax fairness,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,66–76,AIES '21,Association for Computing Machinery,PT868JI2,0.0068965517241379,0.0,0.0,0.0062639543537583
469,469,Investigating the Relative Strengths of Humans and Machine Learning in Decision-Making,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604738,"Rastogi, Charvi",2023.0,https://doi.org/10.1145/3600211.3604738,conferencePaper,"﻿In recent years, we have witnessed a rapid growth in the deployment of Machine Learning (ML) models in complex real-world settings. ML models are being used to support decision-making across a wide range of domains, including healthcare [2, 19, 22, 34], credit lending [5, 14], criminal justice [1, 13], and employment [11, 20]. For example, in the criminal justice system, algorithmic recidivism risk scores inform pre-trial bail decisions for defendants [1]. In credit lending, lenders routinely use credit-scoring models to as- sess the risk of default by applicants [14]. The excitement around modern ML systems facilitating high-stakes decisions is fueled by the promise of these technologies to tap into large datasets, mine relevant statistical patterns within them, and utilize those patterns to make more accurate predictions at a lower cost and without suffering from the same cognitive biases and limitation as human decision-makers. Growing evidence, however, suggests that ML models are vulnerable to various biases [1], instability [8], and opaqueness [4]. These observations have led to calls to preserve human involvement in high-stakes decision-making systems-with the hope of combining and amplifying the respective strengths of human cognition and ML models through carefully designed hybrid decision-making systems. Such systems consist of ML models and human experts jointly making decisions, and they are common in practice-including in the domains mentioned above. Researchers have proposed and tested various hybrid human-ML designs which vary, for instance, in the way decision-making power is distributed between humans and machines [6, 10, 16, 30, 31]. How- ever, empirical findings regarding the success and effectiveness of these proposals are mixed [15, and references therein]. Simul- taneously, a growing body of theoretical work has attempted to conceptualize and formalize these hybrid designs [3, 9] and study optimal ways of aggregating human and ML judgments within",algorithm auditing; decision-making; Human-in-the-loop; human-ML collaboration; human-ML complementarity,9798400702310,,2023,2023-11-06 01:29:53,2023-11-06 01:29:53,987–989,AIES '23,Association for Computing Machinery,AIA8NR8R,0.0067567567567567,0.0,0.0,0.0061713560345571
471,471,An Ontology for Fairness Metrics,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534137,"Franklin, Jade S.; Bhanot, Karan; Ghalwash, Mohamed; Bennett, Kristin P.; McCusker, Jamie; McGuinness, Deborah L.",2022.0,https://doi.org/10.1145/3514094.3534137,conferencePaper,"Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.",bias; fairness metric; machine learning evaluation; rdf knowledge graph,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,265–275,AIES '22,Association for Computing Machinery,9MKDUUMH,0.0062893081761006,0.0,0.0,0.0057384589055748
472,472,Emotion Recognition Technology: Re-Shaping Human Relationships,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3539534,"Prégent, Alexandra",2022.0,https://doi.org/10.1145/3514094.3539534,conferencePaper,"Over the recent years, emotions have taken more and more place in relatively different spheres of society, including politics, sciences, and philosophy. Contradictory to what was preached in the 20th century, we now accept emotions as important part of ourselves that contributes to our reasoning and decision-making process. Furthermore, we recognized them as necessary components in the construction of relationships. This shift in the paradigm towards emotions has increased their value in the public eye. In a paradoxical way, we now tend to communicate more than ever this subjective experience and thus, as we communicate it, we insist on its personal and private status. Our emotions are ours and ours alone. But is it true? By being both recognized as a part of the inner life that is never completely reachable and as a necessary component of relationships, emotions appear to be at the edge of every concept of privacy.This research project will work on this specific blind spot in the literature through the analysis of emotion recognition software (ERS) as a threat to both the privacy of the inner life and the privacy of relationships. The main claims are that emotions must be both recognized as (1) being more than a part of the inner self, e.g., for instance, as actively contributing to communication and the construction of relationships, and (2) considered off-limits from scrutinization and monitoring by governments and corporate in contexts in which the use of ERS disrupt and restrict the possibility to freely experience them as inner affective states and/or to freely use them as a communication tool. The hypothesis is as follows. The first premise assume that the phenomenon of emotion is commonly accepted as being a fundamental component of human experience. From that first assumption is derive the second one; that it is, therefore, a necessary element to live a meaningful life.From that standpoint, I draw upon Helen Nissenbaum's framework of contextual integrity [1] and hypothesize that the context in which the individual shares his or her emotions daily should be recognized as a non-violable whole (set of natural, social, cultural conditions) whose integrity allows for the exercise of this fundamental component of human experience, which is a necessity for living a meaningful life. While emotions are seen as ""in the public space"" because we constantly use them to communicate, I want to argue that emotions ""in the public space"" should be considered a priori off-limit from monitoring and scrutinization by State and corporate until proven otherwise by the contextual integrity framework.The general stance is that a constant monitoring and scrutinization by ERS will have a disruptive effect on the way we experience and express emotions, which will have a direct impact on our way of building and maintaining relationships, putting at higher risk the possibility of achieving a meaningful life. Therefore, I want to draw what could be seen as a draconian line between what citizens on one side and what States and corporate on the other should be allowed to do and not do. Emotions should not be considered something that individuals should refrain from experiencing, communicating, and accessing, however, they should be considered highly sensitive and intimate information and, thus, States and corporate should refrain from monitoring and scrutinizing them. Furthermore, I argue that a massive deployment of ERS by government and private companies will transform the cultural and societal practices as well as affecting our fundamental rights and values.",privacy; emotion recognition software; emotions; public and private space; relationships,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,909,AIES '22,Association for Computing Machinery,MQSYAFQW,0.0035149384885764,0.1,0.0,0.0057192441711539
473,473,Individual and Group-Level Considerations of Actionable Recourse,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604758,"Yetukuri, Jayanth",2023.0,https://doi.org/10.1145/3600211.3604758,conferencePaper,"The advent of machine learning in several critical fields, such as banking, healthcare, and criminal justice, has inspired research into improving robustness, trustworthiness, and transparency in the models. Actionable Recourse is one such tool that enables the negatively impacted users to receive a favorable outcome by providing recommendations of cost-efficient changes to their features. Current recourse methodologies optimize for proximity, sparsity, validity, and distance-based costs. Actionability takes both individual and group-level signals. A critical component of actionability is the consideration of User Preference to guide the recourse generation process. These preferences can take several forms, and we introduce three such preferences to capture the individual difficulty of user actions. Additionally, feasibility and plausibility should be considered as a fixed set of pre-specified constraints. We argue that plausibility draws strong signals from group-level population information, which must be considered to achieve low-cost recourses across protected groups. Recoursability is an active research area, and plausibility becomes an essential direction for further research.",actionable recourse; plausbility; user preference,9798400702310,,2023,2023-11-06 01:29:51,2023-11-06 01:29:51,1008–1009,AIES '23,Association for Computing Machinery,P7QE6528,0.00625,0.0,0.0,0.0057125190258751
477,477,FairCanary: Rapid Continuous Explainable Fairness,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534157,"Ghosh, Avijit; Shanbhag, Aalok; Wilson, Christo",2022.0,https://doi.org/10.1145/3514094.3534157,conferencePaper,"Systems that offer continuous model monitoring have emerged in response to (1) well-documented failures of deployed Machine Learning (ML) and Artificial Intelligence (AI) models and (2) new regulatory requirements impacting these models. Existing monitoring systems continuously track the performance of deployed ML models and compute feature importance (a.k.a. explanations) for each prediction to help developers identify the root causes of emergent model performance problems.We present Quantile Demographic Drift (QDD), a novel model bias quantification metric that uses quantile binning to measure differences in the overall prediction distributions over subgroups. QDD is ideal for continuous monitoring scenarios, does not suffer from the statistical limitations of conventional threshold-based bias metrics, and does not require outcome labels (which may not be available at runtime). We incorporate QDD into a continuous model monitoring system, called FairCanary, that reuses existing explanations computed for each individual prediction to quickly compute explanations for the QDD bias metrics. This optimization makes FairCanary an order of magnitude faster than previous work that has tried to generate feature-level bias explanations.",fairness; continuous measurement; drift; model explanation,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,307–316,AIES '22,Association for Computing Machinery,YHL8YTBV,0.0058823529411764,0.0,0.0,0.0054178424137182
478,478,"Algorithms That ""Don't See Color"": Measuring Biases in Lookalike and Special Ad Audiences","Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534135,"Sapiezynski, Piotr; Ghosh, Avijit; Kaplan, Levi; Rieke, Aaron; Mislove, Alan",2022.0,https://doi.org/10.1145/3514094.3534135,conferencePaper,"Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.",fairness; online advertising; process fairness,978-1-4503-9247-1,,2022,2023-11-06 01:29:52,2023-11-06 01:29:52,609–616,AIES '22,Association for Computing Machinery,S2MRCLIT,0.0058823529411764,0.0,0.0,0.0052766186566159
480,480,Learning to Generate Fair Clusters from Demonstrations,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462558,"Galhotra, Sainyam; Saisubramanian, Sandhya; Zilberstein, Shlomo",2021.0,https://doi.org/10.1145/3461702.3462558,conferencePaper,"Fair clustering is the process of grouping similar entities together, while satisfying a mathematically well-defined fairness metric as a constraint. Due to the practical challenges in precise model specification, the prescribed fairness constraints are often incomplete and act as proxies to the intended fairness requirement. Clustering with proxies may lead to biased outcomes when the system is deployed. We examine how to identify the intended fairness constraint for a problem based on limited demonstrations from an expert. Each demonstration is a clustering over a subset of the data. We present an algorithm to identify the fairness metric from demonstrations and generate clusters using existing off-the-shelf clustering techniques, and analyze its theoretical properties. To extend our approach to novel fairness metrics for which clustering algorithms do not currently exist, we present a greedy method for clustering. Additionally, we investigate how to generate interpretable solutions using our approach. Empirical evaluation on three real-world datasets demonstrates the effectiveness of our approach in quickly identifying the underlying fairness and interpretability constraints, which are then used to generate fair and interpretable clusters.",clustering; fairness; interpretability; maximum-likelihood estimation,978-1-4503-8473-5,,2021,2023-11-06 01:29:51,2023-11-06 01:29:51,491–501,AIES '21,Association for Computing Machinery,W7VXE7EG,0.0056497175141242,0.0,0.0,0.0051479736482489
481,481,When Fair Classification Meets Noisy Protected Attributes,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604707,"Ghosh, Avijit; Kvitca, Pablo; Wilson, Christo",2023.0,https://doi.org/10.1145/3600211.3604707,conferencePaper,"The operationalization of algorithmic fairness comes with several practical challenges, not the least of which is the availability or reliability of protected attributes in datasets. In real-world contexts, practical and legal impediments may prevent the collection and use of demographic data, making it difficult to ensure algorithmic fairness. While initial fairness algorithms did not consider these limitations, recent proposals aim to achieve algorithmic fairness in classification by incorporating noisiness in protected attributes or not using protected attributes at all. To the best of our knowledge, this is the first head-to-head study of fair classification algorithms to compare attribute-reliant, noise-tolerant and attribute-unaware algorithms along the dual axes of predictivity and fairness. We evaluated these algorithms via case studies on four real-world datasets and synthetic perturbations. Our study reveals that attribute-unaware and noise-tolerant fair classifiers can potentially achieve similar level of performance as attribute-reliant algorithms, even when protected attributes are noisy. However, implementing them in practice requires careful nuance. Our study provides insights into the practical implications of using fair classification algorithms in scenarios where protected attributes are noisy or partially available.",classification; fairness; evaluation; protected attributes,9798400702310,,2023,2023-11-06 01:29:56,2023-11-06 01:29:56,679–690,AIES '23,Association for Computing Machinery,JB33CGX2,0.0055555555555555,0.0,0.0,0.0051212327215046
482,482,RelEx: A Model-Agnostic Relational Model Explainer,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462562,"Zhang, Yue; Defazio, David; Ramesh, Arti",2021.0,https://doi.org/10.1145/3461702.3462562,conferencePaper,"In recent years, considerable progress has been made on improving the interpretability of machine learning models. This is essential, as complex deep learning models with millions of parameters produce state of the art performance, but it can be nearly impossible to explain their predictions. While various explainability techniques have achieved impressive results, nearly all of them assume each data instance to be independent and identically distributed (iid). This excludes relational models, such as Statistical Relational Learning (SRL), and the recently popular Graph Neural Networks (GNNs), resulting in few options to explain them. While there does exist work on explaining GNNs, GNN-Explainer, they assume access to the gradients of the model to learn explanations, which is restrictive in terms of its applicability across non-differentiable relational models and practicality. In this work, we develop RelEx, amodel-agnostic relational explainer to explain black-box relational models with only access to the outputs of the black-box. RelEx is able to explain any relational model, including SRL models and GNNs. We compare RelEx to the state-of-the-art relational explainer, GNN-Explainer, and relational extensions of iid explanation models and show that RelEx achieves comparable or better performance, while remaining model-agnostic.",model-agnostic; relational explainer,978-1-4503-8473-5,,2021,2023-11-06 01:29:52,2023-11-06 01:29:52,1042–1049,AIES '21,Association for Computing Machinery,LVNFRLHT,0.0052356020942408,0.0,0.0,0.0049222675221429
483,483,Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604687,"Schwarzschild, Avi; Cembalest, Max; Rao, Karthik; Hines, Keegan; Dickerson, John",2023.0,https://doi.org/10.1145/3600211.3604687,conferencePaper,"As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model’s output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus between explainers other than those used in the loss term. We examine the trade-off between improved consensus and model performance. And finally, we study the influence our method has on feature attribution explanations.",,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,662–678,AIES '23,Association for Computing Machinery,3VVCW8GG,0.0052356020942408,0.0,0.0,0.0048913652592983
484,484,The Deepfake Detection Dilemma: A Multistakeholder Exploration of Adversarial Dynamics in Synthetic Media,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462584,"Leibowicz, Claire R.; McGregor, Sean; Ovadya, Aviv",2021.0,https://doi.org/10.1145/3461702.3462584,conferencePaper,"Synthetic media detection technologies label media as either synthetic or non-synthetic and are increasingly used by journalists, web platforms, and the general public to identify misinformation and other forms of problematic content. As both well-resourced organizations and the non-technical general public generate more sophisticated synthetic media, the capacity for purveyors of problematic content to adapt induces a detection dilemma : as detection practices become more accessible, they become more easily circumvented. This paper describes how a multistakeholder cohort from academia, technology platforms, media entities, and civil society organizations active in synthetic media detection and its socio-technical implications evaluates the detection dilemma. Specifically, we offer an assessment of detection contexts and adversary capacities sourced from the broader, global AI and media integrity community concerned with mitigating the spread of harmful synthetic media. A collection of personas illustrates the intersection between unsophisticated and highly-resourced sponsors of misinformation in the context of their technical capacities. This work concludes that there is no ""best” approach to navigating the detector dilemma, but derives a set of implications from multistakeholder input to better inform detection process decisions and policies, in practice.",security; misinformation; synthetic media,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,736–744,AIES '21,Association for Computing Machinery,KIJLS9GN,0.0054054054054054,0.0,0.0,0.0048879042977403
485,485,Measuring Group Advantage: A Comparative Study of Fair Ranking Metrics,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462588,"Kuhlman, Caitlin; Gerych, Walter; Rundensteiner, Elke",2021.0,https://doi.org/10.1145/3461702.3462588,conferencePaper,"Ranking evaluation metrics play an important role in information retrieval, providing optimization objectives during development and means of assessment of deployed performance. Recently, fairness of rankings has been recognized as crucial, especially as automated systems are increasingly used for high impact decisions. While numerous fairness metrics have been proposed, a comparative analysis to understand their interrelationships is lacking. Even for fundamental statistical parity metrics which measure group advantage, it remains unclear whether metrics measure the same phenomena, or when one metric may produce different results than another. To address these open questions, we formulate a conceptual framework for analytical comparison of metrics. We prove that under reasonable assumptions, popular metrics in the literature exhibit the same behavior and that optimizing for one optimizes for all. However, our analysis also shows that the metrics vary in the degree of unfairness measured, in particular when one group has a strong majority. Based on this analysis, we design a practical statistical test to identify whether observed data is likely to exhibit predictable group bias. We provide a set of recommendations for practitioners to guide the choice of an appropriate fairness metric.",fair ranking; fairness metrics; statistical parity; group advantage,978-1-4503-8473-5,,2021,2023-11-06 01:29:59,2023-11-06 01:29:59,674–682,AIES '21,Association for Computing Machinery,ZTVKDRF4,0.0053191489361702,0.0,0.0,0.0048141431120154
486,486,Fairness Implications of Encoding Protected Categorical Attributes,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604657,"Mougan, Carlos; Alvarez, Jose Manuel; Ruggieri, Salvatore; Staab, Steffen",2023.0,https://doi.org/10.1145/3600211.3604657,conferencePaper,"Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g. support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: one-hot encoding and target encoding. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, irreducible bias, is due to direct group category discrimination and the second type, reducible bias, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",Algorithmic Accountability; Bias; Categorical Features; Fairness,9798400702310,,2023,2023-11-06 01:29:50,2023-11-06 01:29:50,454–465,AIES '23,Association for Computing Machinery,HE4G4DQ6,0.005,0.0,0.0,0.0045849297573435
487,487,No Rage Against the Machines: Threat of Automation Does Not Change Policy Preferences,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534179,"Zhang, Baobao",2022.0,https://doi.org/10.1145/3514094.3534179,conferencePaper,"Labor-saving technology has already decreased employment opportunities for middle-skill workers. Experts anticipate that advances in AI and robotics will cause even more significant disruptions in the labor market over the next two decades. This paper presents three experimental studies that investigate how this profound economic change could affect mass politics. Recent observational studies suggest that workers' exposure to automation risk predicts their support not only for redistribution but also for right-wing populist policies and candidates. Other observational studies, including my own, find that workers underestimate the impact of automation on their job security. Misdirected blame towards immigrants and workers in foreign countries, rather than concerns about workplace automation, could be driving support for right-wing populism. To correct American workers' beliefs about the threats to their jobs, I conducted three survey experiments in which I informed workers about the existent and future impact of workplace automation. While these informational treatments convinced workers that automation threatens American jobs, they failed to change respondents' preferences on welfare, immigration, and trade policies. My research finds that raising awareness about workplace automation did not decrease opposition to globalization or increase support for policies that will prepare workers for future technological disruptions.",automation; artificial intelligence and the future of work; political economy; public opinion,978-1-4503-9247-1,,2022,2023-11-06 01:29:55,2023-11-06 01:29:55,856–866,AIES '22,Association for Computing Machinery,TUB8HGXM,0.0051020408163265,0.0,0.0,0.0045428269830099
488,488,Comparing Equity and Effectiveness of Different Algorithms in an Application for the Room Rental Market,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462600,"Solans, David; Fabbri, Francesco; Calsamiglia, Caterina; Castillo, Carlos; Bonchi, Francesco",2021.0,https://doi.org/10.1145/3461702.3462600,conferencePaper,"Machine Learning (ML) techniques have been increasingly adopted by the real estate market in the last few years. Applications include, among many others, predicting the market value of a property or an area, advanced systems for managing marketing and ads campaigns, and recommendation systems based on user preferences. While these techniques can provide important benefits to the business owners and the users of the platforms, algorithmic biases can result in inequalities and loss of opportunities for groups of people who are already disadvantaged in their access to housing. In this work, we present a comprehensive and independent algorithmic evaluation of a recommender system for the real estate market, designed specifically for finding shared apartments in metropolitan areas. We were granted full access to the internals of the platform, including details on algorithms and usage data during a period of 2 years.We analyze the performance of the various algorithms which are deployed for the recommender system and asses their effect across different population groups.Our analysis reveals that introducing a recommender system algorithm facilitates finding an appropriate tenant or a desirable room to rent, but at the same time, it strengthen performance inequalities between groups, further reducing opportunities of finding a rental for certain minorities.",fairness; demographics; performance; recommender system,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,978–988,AIES '21,Association for Computing Machinery,MIZ3QCY8,0.0049261083743842,0.0,0.0,0.0044173991435654
489,489,Fair Bayesian Optimization,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462629,"Perrone, Valerio; Donini, Michele; Zafar, Muhammad Bilal; Schmucker, Robin; Kenthapadi, Krishnaram; Archambeau, Cédric",2021.0,https://doi.org/10.1145/3461702.3462629,conferencePaper,"Given the increasing importance of machine learning (ML) in our lives, several algorithmic fairness techniques have been proposed to mitigate biases in the outcomes of the ML models. However, most of these techniques are specialized to cater to a single family of ML models and a specific definition of fairness, limiting their adaptibility in practice. We introduce a general constrained Bayesian optimization (BO) framework to optimize the performance of any ML model while enforcing one or multiple fairness constraints. BO is a model-agnostic optimization method that has been successfully applied to automatically tune the hyperparameters of ML models. We apply BO with fairness constraints to a range of popular models, including random forests, gradient boosting, and neural networks, showing that we can obtain accurate and fair solutions by acting solely on the hyperparameters. We also show empirically that our approach is competitive with specialized techniques that enforce model-specific fairness constraints, and outperforms preprocessing methods that learn fair representations of the input data. Moreover, our method can be used in synergy with such specialized fairness techniques to tune their hyperparameters. Finally, we study the relationship between fairness and the hyperparameters selected by BO. We observe a correlation between regularization and unbiased models, explaining why acting on the hyperparameters leads to ML models that generalize well and are fair.",fairness; bias; autoML; Bayesian optimization; hyperparameter tuning,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,854–863,AIES '21,Association for Computing Machinery,B3R69JDY,0.0046082949308755,0.0,0.0,0.0043346501276523
490,490,Risk Identification Questionnaire for Detecting Unintended Bias in the Machine Learning Development Lifecycle,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462572,"Lee, Michelle Seng Ah; Singh, Jatinder",2021.0,https://doi.org/10.1145/3461702.3462572,conferencePaper,"Unintended biases in machine learning (ML) models have the potential to introduce undue discrimination and exacerbate social inequalities. The research community has proposed various technical and qualitative methods intended to assist practitioners in assessing these biases. While frameworks for identifying the risks of harm due to unintended biases have been proposed, they have not yet been operationalised into practical tools to assist industry practitioners.In this paper, we link prior work on bias assessment methods to phases of a standard organisational risk management process (RMP), noting a gap in measures for helping practitioners identify bias- related risks. Targeting this gap, we introduce a bias identification methodology and questionnaire, illustrating its application through a real-world, practitioner-led use case. We validate the need and usefulness of the questionnaire through a survey of industry practitioners, which provides insights into their practical requirements and preferences. Our results indicate that such a questionnaire is helpful for proactively uncovering unexpected bias concerns, particularly where it is easy to integrate into existing processes, and facilitates communication with non-technical stakeholders. Ultimately, the effective end-to-end management of ML risks requires a more targeted identification of potential harm and its sources, so that appropriate mitigation strategies can be formulated. Towards this, our questionnaire provides a practical means to assist practitioners in identifying bias-related risks.",algorithmic bias; algorithmic fairness; fair ML; questionnaire; risk identification; risk management,978-1-4503-8473-5,,2021,2023-11-06 01:29:57,2023-11-06 01:29:57,704–714,AIES '21,Association for Computing Machinery,DW3AG3GW,0.0046948356807511,0.0,0.0,0.0041442068046136
492,492,Racial Disparities in the Enforcement of Marijuana Violations in the US,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534184,"Butcher, Bradley; Robinson, Chris; Zilka, Miri; Fogliato, Riccardo; Ashurst, Carolyn; Weller, Adrian",2022.0,https://doi.org/10.1145/3514094.3534184,conferencePaper,"Racial disparities in US drug arrest rates have been observed for decades, but their causes and policy implications are still contested. Some have argued that the disparities largely reflect differences in drug use between racial groups, while others have hypothesized that discriminatory enforcement policies and police practices play a significant role. In this work, we analyze racial disparities in the enforcement of marijuana violations in the US. Using data from the National Incident-Based Reporting System (NIBRS) and the National Survey on Drug Use and Health (NSDUH) programs, we investigate whether marijuana usage and purchasing behaviors can explain the racial composition of offenders in police records. We examine potential driving mechanisms behind these disparities and the extent to which county-level socioeconomic factors are associated with corresponding disparities. Our results indicate that the significant racial disparities in reported incidents and arrests cannot be explained by differences in marijuana days-of-use alone. Variations in the location where marijuana is purchased and in the frequency of these purchases partially explain the observed disparities. We observe an increase in racial disparities across most counties over the last decade, with the greatest increases in states that legalized the use of marijuana within this timeframe. Income, high school graduation rate, and rate of employment positively correlate with larger racial disparities, while the rate of incarceration is negatively correlated. We conclude with a discussion of the implications of the observed racial disparities in the context of algorithmic fairness.",law enforcement; marijuana; racial disparities,978-1-4503-9247-1,,2022,2023-11-06 01:29:50,2023-11-06 01:29:50,130–143,AIES '22,Association for Computing Machinery,VQHJWXQ7,0.00418410041841,0.0,0.0,0.0039104616543217
493,493,Towards Robust Off-Policy Evaluation via Human Inputs,"Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3514094.3534198,"Singh, Harvineet; Joshi, Shalmali; Doshi-Velez, Finale; Lakkaraju, Himabindu",2022.0,https://doi.org/10.1145/3514094.3534198,conferencePaper,"Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.",dataset shift; human-in-the-loop; policy evaluation; robust learning; adversarial machine learning,978-1-4503-9247-1,,2022,2023-11-06 01:29:56,2023-11-06 01:29:56,686–699,AIES '22,Association for Computing Machinery,U3NTY2DZ,0.0042372881355932,0.0,0.0,0.0038806379079039
494,494,Social Biases through the Text-to-Image Generation Lens,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604711,"Naik, Ranjita; Nushi, Besmira",2023.0,https://doi.org/10.1145/3600211.3604711,conferencePaper,"Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people from results for both models. Such biases can get mitigated by increasing the amount of specification in the prompt itself, although the prompting mitigation will not address discrepancies in image quality or other usages of the model or its representations in other scenarios. Further, we observe personality traits being associated with only a limited set of people at the intersection of race, gender, and age. Finally, an analysis of geographical location representations on everyday situations (e.g., park, food, weddings) shows that for most situations, images generated through default location-neutral prompts are closer and more similar to images generated for locations of United States and Germany.",social biases; representational fairness; text-to-image generation,9798400702310,,2023,2023-11-06 01:29:58,2023-11-06 01:29:58,786–808,AIES '23,Association for Computing Machinery,BICFF9IL,0.0040160642570281,0.0,0.0,0.0037505212105251
495,495,Measuring Model Fairness under Noisy Covariates: A Theoretical Perspective,"Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462603,"Prost, Flavien; Awasthi, Pranjal; Blumm, Nick; Kumthekar, Aditee; Potter, Trevor; Wei, Li; Wang, Xuezhi; Chi, Ed H.; Chen, Jilin; Beutel, Alex",2021.0,https://doi.org/10.1145/3461702.3462603,conferencePaper,"In this work we study the problem of measuring the fairness of a machine learning model under noisy information. Focusing on group fairness metrics, we investigate the particular but common situation when the evaluation requires controlling for the confounding effect of covariate variables. In a practical setting, we might not be able to jointly observe the covariate and group information, and a standard workaround is to then use proxies for one or more of these variables. Prior works have demonstrated the challenges with using a proxy for sensitive attributes, and strong independence assumptions are needed to provide guarantees on the accuracy of the noisy estimates. In contrast, in this work we study using a proxy for the covariate variable and present a theoretical analysis that aims to characterize weaker conditions under which accurate fairness evaluation is possible. Furthermore, our theory identifies potential sources of errors and decouples them into two interpretable parts y and E. The first part y depends solely on the performance of the proxy such as precision and recall, whereas the second part E captures correlations between all the variables of interest. We show that in many scenarios the error in the estimates is dominated by y via a linear dependence, whereas the dependence on the correlations E only constitutes a lower order term. As a result we expand the understanding of scenarios where measuring model fairness via proxies can be an effective approach. Finally, we compare, via simulations, the theoretical upper-bounds to the distribution of simulated estimation errors and show that assuming some structure on the data, even weak, is key to significantly improve both theoretical guarantees and empirical results.",ml fairness; noisy covariates; statistical parity,978-1-4503-8473-5,,2021,2023-11-06 01:29:56,2023-11-06 01:29:56,873–883,AIES '21,Association for Computing Machinery,FGENC8CU,0.0036496350364963,0.0,0.0,0.0034109826656887
496,496,How to Promote Equitable Sleep Care among People Experiencing Homelessness: An AI-Enabled Person-Centred Computer Vision-Based Solution,"Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3600211.3604736,"Taghibeyglou, Behrad",2023.0,https://doi.org/10.1145/3600211.3604736,conferencePaper,"Homelessness, defined as a lack of stable housing, affects over 435,000 people in Canada, most of whom live in shelters and experience barriers to healthcare [2]. Compared to the general population, people experiencing homelessness (PEH) sleep less, experience excessive fatigue, and are more likely to use substances to fall asleep at night (drugs) or stay awake during the day (cigarettes, alcohol) [10]. A recent study among PEH indicated that sleep quality was a key factor for their health and daily function [3]. A prevalent, yet under-recognized, cause of poor sleep is sleep apnea, which is characterized by recurrent interruptions in breathing during sleep [7]. Obstructive sleep apnea (OSA), a common form of sleep apnea, has a high prevalence, affecting approximately 30%-70% of individuals with chronic conditions such as hypertension and substance use [5]. Untreated OSA has significant medical consequences and societal costs. Clinical polysomnography is the gold standard for sleep apnea diagnosis, but it is inconvenient, expensive, and inaccessible, especially for PEH who are experiencing several barriers to healthcare, e.g. low trust in care providers, difficulty visiting medical facilities, stress of daily needs, and fear of losing child custody if found to be homeless. The most accessible alternatives to polysomnography are sleep questionnaires, but they have low specificity. Speech can be used as a non-invasive and accessible biomarker for monitoring physiological changes in the pharyngeal airway and cardio-respiratory system, including lung edema and pharyngeal airway narrowing [8]. Therefore, speech can be a potential tool for assessing the risk of OSA. Craniofacial photography has also revealed specific characteristics of upper airway structures and facial dimensions that are associated with OSA [4]. Therefore, developing an accurate and accessible risk assessment method for OSA using these technologies will improve health outcomes in PEH. Our ultimate goal is to develop a person-centred smartphone application to assess the risk of OSA during wakefulness in PEH living in shelters. Objectives: To achieve our primary goal, we will 1) identify the preference and concerns of PEH about a mobile application based on speech for assessing the risk of sleep apnea, 2) determine the prevalence of sleep apnea among shelter residents, and 3) develop a person-centred smartphone application to assess the risk of sleep apnea through speech and image analyses. Preliminary works: Over the past 3 years, our team has established an interdisciplinary stakeholder group comprising people with lived experience of homelessness and OSA, care providers for shelter residents, researchers, and clinicians. Based on our preliminary assessment, most shelter residents have access to cell phones, and wireless internet through shelters and public spaces such as libraries. For objective sleep assessment, 15 participants are recruited (aged 51.94 ± 14.4 years old, including 8 men) from three shelters for an overnight sleep study at the shelter using the protocol mentioned in the next section. The analysis showed that 85% of the participants (13 out of 15) had moderate-to-severe sleep apnea (apnea-hypopnea index [AHI] ≥ 10 events/hour), with a mean AHI of 27.1 and a standard deviation of 18.53. These preliminary results highlight the importance of developing accessible technologies for diagnosing sleep apnea in shelter residents. Method: To enhance our understanding regarding the usability of such technology among this population, our team has been conducting semi-structured one-on-one interviews with participants to gather their feedback and concerns about the proposed application. A research assistant with lived experience of homelessness and sleep apnea and I collected anthropometric data such as height, weight, neck circumference, and blood pressure, as well as questionnaires related to sleep status, such as STOP-BANG [1] and Epworth Sleepiness Scale [6]. Then, we set up residents with portable polysomnography (level II) for overnight data collection. Before being set up for polysomnography and going to sleep, participants will hold an audio recorder and stand in front of the camera while saying five vowels in a specific order (/i/ as in “see”, /u/ as in “soo”, /a/ as in “sahh”, /e/ as in “set”, /o/ as in “so”, /n/, and /m/). Our team has previously shown acoustic features of these vowels can reveal differences in upper airway dimensions in individuals with high risk of sleep apnea compared to healthy participants [9]. To assess the risk of OSA, I will expand our previous work [9], along with other state-of-the-art algorithms. Preprocessing techniques will be applied to eliminate speech noises, such as background noise, and image artifacts, such as motion, blurring, and illumination. Afterwards, facial landmarks will be extracted from the image frames. Extracted features will be used as the input of classical machine learning models, which will map them to gold standard indices obtained from PSG and other measures. Significance: The discussed study, the first of its kind, aims to fill a critical gap by investigating the barriers to delivering equitable access to sleep care to PEH, determining the prevalence of sleep apnea using objective assessment, and developing a customized person-centred smartphone application to provide equitable sleep care in shelters. The outcome of this study will provide more insights for policymakers to change the current flow of sleep care in shelters. The results of this study could help improve sleep care for other structurally marginalized populations, such as people with low socioeconomic status or those living in remote areas.",Computer Vision; Health Equity; Homelessness; Person-centred approach; Sleep apnea,9798400702310,,2023,2023-11-06 01:29:52,2023-11-06 01:29:52,983–984,AIES '23,Association for Computing Machinery,BYXICF8C,0.0034642032332563,0.0,0.0,0.0033380313464459
